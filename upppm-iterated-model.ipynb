{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy\nimport random\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig","metadata":{"_uuid":"1f2a6277-0fa8-4b3e-a3ae-80d5786c09d3","_cell_guid":"18d8d1f5-1a85-4282-bf31-75b43c84e2d0","collapsed":false,"executionInfo":{"elapsed":20123,"status":"ok","timestamp":1644920080956,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"35916341","outputId":"06fa0ab8-a380-4f54-a98d-b7015b79d9e2","papermill":{"duration":26.143536,"end_time":"2022-03-22T09:40:36.798853","exception":false,"start_time":"2022-03-22T09:40:10.655317","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-11-27T19:42:23.067343Z","iopub.execute_input":"2022-11-27T19:42:23.067640Z","iopub.status.idle":"2022-11-27T19:42:25.056689Z","shell.execute_reply.started":"2022-11-27T19:42:23.067607Z","shell.execute_reply":"2022-11-27T19:42:25.055965Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"CFG1 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp1/deberta-v3-large',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG2 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp2/deberta-v3-large',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG3 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp3/bert-for-patents',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG4 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp4/deberta-v3-large',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': True\n}\n\nCFG10 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp10/deberta-v3-large',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG11 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp11/bert-for-patents',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG16 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp16/deberta-v3-large',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 8, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG17 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp17/deberta-v3-large',\n    'max_len': 256, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG18 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-base/deberta-v3-base',\n    'path': '../input/upppm-exp18/deberta-v3-base',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 8, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG19 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-base/deberta-v3-base',\n    'path': '../input/upppm-exp19/deberta-v3-base',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 8, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG20 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp20/bert-for-patents',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG21 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppmexp21/deberta-v3-large',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG23 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/debertalarge',\n    'path': '../input/upppm-exp23/deberta-large',\n    'max_len': 500, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG26 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp26/deberta-v3-large',\n    'max_len': 512, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG27 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp27/bert-for-patents',\n    'max_len': 512, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG28 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp28/deberta-v3-large',\n    'max_len': 512, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG29 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp29/deberta-v3-large',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG31 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp31/deberta-v3-large',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': True\n}\n\nCFG32 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-xlarge',\n    'path': '../input/upppm-exp32-output/upppm-exp32/deberta-xlarge',\n    'max_len': 450, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG33 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp33/deberta-v3-large',\n    'max_len': 512, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG35 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp35/exp35/deberta-v3-large',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG36 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp36/exp36/bert-for-patents',\n    'max_len': 64, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG38 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp38/exp38/deberta-v3-large',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 8, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG39 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp39/exp39/bert-for-patents',\n    'max_len': 384, \n    'epochs': 5,\n    'train_bs': 16, \n    'valid_bs': 32,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 0,\n    'sigmoid': False\n}\n\nCFG40 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/debertalarge',\n    'path': '../input/upppm-exp40/exp40/deberta-large',\n    'max_len': 450, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG49 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp49-output/upppm-exp49/deberta-v3-large',\n    'max_len': 560, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG51 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp51-output/upppm-exp51/bert-for-patents',\n    'max_len': 512, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG61 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp61-output/upppm-exp61/deberta-v3-large',\n    'max_len': 250, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG62 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp62-output/upppm-exp62/bert-for-patents',\n    'max_len': 250, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG63 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/debertalarge',\n    'path': '../input/upppm-exp63-output/upppm-exp63/deberta-large',\n    'max_len': 280, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG64 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp64-output/upppm-exp64/deberta-v3-large',\n    'max_len': 320, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG69 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp69-output/upppm-exp69/deberta-v3-large',\n    'max_len': 250, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG70 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/bert-for-patents/bert-for-patents',\n    'path': '../input/upppm-exp70-output/upppm-exp70/bert-for-patents',\n    'max_len': 320, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG71 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp71-output/upppm-exp71/deberta-v3-large',\n    'max_len': 250, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG74 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp74-output/upppm-exp74/deberta-v3-large',\n    'max_len': 280, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}\n\nCFG80 = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': '../input/deberta-v3-large/deberta-v3-large',\n    'path': '../input/upppm-exp80-output/upppm-exp80/deberta-v3-large',\n    'max_len': 280, \n    'epochs': 5,\n    'train_bs': 4, \n    'valid_bs': 16,\n    'lr': 1e-5, \n    'num_workers': 2,\n    'weight_decay': 1e-2,\n    'sigmoid': False\n}","metadata":{"_uuid":"77c63d31-005c-4827-a3c2-974be5663325","_cell_guid":"5a73298c-84e0-4b92-91c6-8d3e3bd1cfd9","collapsed":false,"id":"48dd82bb","papermill":{"duration":0.033949,"end_time":"2022-03-22T09:40:01.634977","exception":false,"start_time":"2022-03-22T09:40:01.601028","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-11-27T19:42:27.281820Z","iopub.execute_input":"2022-11-27T19:42:27.282417Z","iopub.status.idle":"2022-11-27T19:42:27.326662Z","shell.execute_reply.started":"2022-11-27T19:42:27.282365Z","shell.execute_reply":"2022-11-27T19:42:27.325446Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(CFG1['seed'])\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"aae8ffd1-e67c-454b-9ff4-6359b46e808b","_cell_guid":"89f259b7-9817-464d-b0ee-ca79fed58b1c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-11-27T19:42:29.814923Z","iopub.execute_input":"2022-11-27T19:42:29.815182Z","iopub.status.idle":"2022-11-27T19:42:29.883748Z","shell.execute_reply.started":"2022-11-27T19:42:29.815156Z","shell.execute_reply":"2022-11-27T19:42:29.882972Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_cpc_texts():\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('../input/cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results\n\n\ncpc_texts = get_cpc_texts()\n\ntrain_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ntest_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\n\ntrain_df['flag'] = 0\ntest_df['flag'] = 1\ntest_df['score'] = -1\n\nall_df = pd.concat([test_df, train_df], 0)\n\nall_df['context_text'] = all_df['context'].map(cpc_texts).apply(lambda x:x.lower())\nall_df = all_df.join(all_df.groupby('anchor').target.agg(list).rename('ref'), on='anchor')\nall_df['ref2'] = all_df.apply(lambda x:[i for i in x['ref'] if i != x['target']], axis=1)\nall_df['ref2'] = all_df.ref2.apply(lambda x: ', '.join(sorted(list(set(x)), key=x.index)))\nall_df['ref'] = all_df.ref.apply(lambda x:', '.join(sorted(list(set(x)), key=x.index)))\n\nall_df = all_df.join(all_df.groupby(['anchor', 'context']).target.agg(list).rename('ref3'), on=['anchor', 'context'])\nall_df['ref3'] = all_df.apply(lambda x: ', '.join([i for i in x['ref3'] if i != x['target']]), axis=1)\n\nall_df = all_df.join(all_df.groupby('context').anchor.agg('unique').rename('anchor_list'), on='context')\nall_df['anchor_list'] = all_df.apply(lambda x:', '.join([i for i in x['anchor_list'] if i != x['anchor']]), axis=1)\n\nall_df['text1'] = all_df['anchor'] + '[SEP]' + all_df['target'] + '[SEP]'  + all_df['context_text']\nall_df['text2'] = all_df['anchor'] + '[SEP]' + all_df['target'] + '[SEP]'  + all_df['context_text'] + '[SEP]'  + all_df['ref']\nall_df['text3'] = all_df['anchor'] + '[SEP]' + all_df['target'] + '[SEP]'  + all_df['context_text'] + '[SEP]'  + all_df['ref2']\nall_df['text4'] = all_df['anchor'] + '[SEP]' + all_df['target'] + '[SEP]'  + all_df['context_text'] + '[SEP]'  + all_df['ref2'] + ', ' + all_df['anchor_list']\nall_df['text5'] = all_df['anchor'] + '[SEP]' + all_df['target'] + '[SEP]'  + all_df['context_text'] + '[SEP]'  + all_df['ref3']\nall_df['text6'] = 'The ontological and semantic similarity between anchor term ' + all_df['anchor'] + ' and target term ' + all_df['target'] + '. Context is ' + all_df['context_text'] + \\\n            '. Candidates are ' + all_df['ref3']\nall_df\n\n\ntrain['general_context'] = train['context'].apply(lambda x: table[x[0].upper()])\ntest['general_context'] = test['context'].apply(lambda x: table[x[0].upper()])\n\ntrain = pd.concat([train, pd.get_dummies(train['general_context'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['general_context'])], axis=1)\n\ncpc_texts = torch.load(f\"../input/cpc-texts/cpc_texts.pth\")\ntrain['context_text'] = train['context'].map(cpc_texts)\ntest['context_text'] = test['context'].map(cpc_texts)\n\ntrain['section'] = train['context'].astype(str).str[0]\ntrain['classes'] = train['context'].astype(str).str[1:]\ntest['section'] = test['context'].astype(str).str[0]\ntest['classes'] = test['context'].astype(str).str[1:]\n\ntrain['anchor_len'] = train['anchor'].str.split().str.len()\ntrain['target_len'] = train['target'].str.split().str.len()\n\ntest['anchor_len'] = test['anchor'].str.split().str.len()\ntest['target_len'] = test['target'].str.split().str.len()\n\ntrain['len_diff'] = np.abs(train['target_len'] - train['anchor_len'])\ntest['len_diff'] = np.abs(test['target_len'] - test['anchor_len'])\n\ntrain['num_anchor_stops'] = test['anchor'].str.count('|'.join(stopwords))\ntest['num_anchor_stops'] = test['anchor'].str.count('|'.join(stopwords))\ntrain['num_target_stops'] = test['target'].str.count('|'.join(stopwords))\ntest['num_target_stops'] = test['target'].str.count('|'.join(stopwords))\n\ntrain['anchor_in_target'] = train.apply(lambda x: x[\"anchor\"] in x[\"target\"], axis=1)\ntrain['target_in_anchor'] = train.apply(lambda x: x[\"target\"] in x[\"anchor\"], axis=1)\ntest['anchor_in_target'] = test.apply(lambda x: x[\"anchor\"] in x[\"target\"], axis=1)\ntest['target_in_anchor'] = test.apply(lambda x: x[\"target\"] in x[\"anchor\"], axis=1)\n\ntrain['anchor_nlp'] = train.anchor.apply(lambda series: nlp(series))\ntrain['target_nlp'] = train.target.apply(lambda series: nlp(series))\n\ntrain['anchor_VERB'] = train.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\ntrain['target_VERB'] = train.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\n\ntrain['anchor_NOUN'] = train.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\ntrain['target_NOUN'] = train.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\n\ntrain['anchor_DET'] = train.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\ntrain['target_DET'] = train.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\n\ntrain['anchor_ADJ'] = train.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\ntrain['target_ADJ'] = train.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\n\ntest['anchor_nlp'] = test.anchor.apply(lambda series: nlp(series))\ntest['target_nlp'] = test.target.apply(lambda series: nlp(series))\n\ntest['anchor_VERB'] = test.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\ntest['target_VERB'] = test.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\n\ntest['anchor_NOUN'] = test.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\ntest['target_NOUN'] = test.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\n\ntest['anchor_DET'] = test.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\ntest['target_DET'] = test.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\n\ntest['anchor_ADJ'] = test.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\ntest['target_ADJ'] = test.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\n\ntrain = train.drop(columns=['anchor_nlp', 'target_nlp'])\ntest = test.drop(columns=['anchor_nlp', 'target_nlp'])","metadata":{"_uuid":"a32c1246-1775-4af5-8967-98b083e133d7","_cell_guid":"6721cc59-b8a2-4b2c-aef7-9508e5559191","collapsed":false,"id":"d5c0ccc6","papermill":{"duration":0.21551,"end_time":"2022-03-22T09:40:37.116848","exception":false,"start_time":"2022-03-22T09:40:36.901338","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, add_ref=0):\n        self.df = dataframe\n        self.add_ref = add_ref\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if self.add_ref == 0:\n            text = str(self.df.text1.values[idx])\n        elif self.add_ref == 1:\n            text = str(self.df.text2.values[idx])\n        elif self.add_ref == 2:\n            text = str(self.df.text3.values[idx])\n        elif self.add_ref == 3:\n            text = str(self.df.text4.values[idx])\n        elif self.add_ref == 4:\n            text = str(self.df.text5.values[idx])\n        else:\n            text = str(self.df.text6.values[idx])\n        return text","metadata":{"_uuid":"d61d5452-5f93-4d54-9059-84ddc58cdd9a","_cell_guid":"faf2be77-79b7-4590-a16d-042b5f429fd3","collapsed":false,"id":"9f791a19","papermill":{"duration":0.055528,"end_time":"2022-03-22T09:40:52.072178","exception":false,"start_time":"2022-03-22T09:40:52.01665","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-11-27T19:42:55.307790Z","iopub.execute_input":"2022-11-27T19:42:55.308254Z","iopub.status.idle":"2022-11-27T19:42:55.316051Z","shell.execute_reply.started":"2022-11-27T19:42:55.308213Z","shell.execute_reply":"2022-11-27T19:42:55.315237Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"out = MyDataset(all_df, 4)\nout[0]","metadata":{"_uuid":"f3abbf63-7664-44c8-b0a4-671258ff6ad9","_cell_guid":"8accc1be-c85d-4365-966b-df93523dcbe0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-11-27T20:12:42.879155Z","iopub.execute_input":"2022-11-27T20:12:42.879422Z","iopub.status.idle":"2022-11-27T20:12:42.884067Z","shell.execute_reply.started":"2022-11-27T20:12:42.879391Z","shell.execute_reply":"2022-11-27T20:12:42.883139Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def collate_fn(data):\n    text = tokenizer(data, padding='max_length', truncation=True, max_length=CFG['max_len'], return_tensors='pt')\n    input_ids = text['input_ids']\n    attention_mask = text['attention_mask']\n    return input_ids, attention_mask\n\ndef collate_fn_fast(data):\n    text = tokenizer(data, padding='longest', truncation=True, max_length=CFG['max_len'], return_tensors='pt')\n    input_ids = text['input_ids']\n    attention_mask = text['attention_mask']\n    return input_ids, attention_mask","metadata":{"_uuid":"ae753614-1172-4f4d-84b8-f3fbfbdf355b","_cell_guid":"52e843a1-a6ec-4002-8737-eb0b44e0cd2b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-11-27T19:42:55.331039Z","iopub.execute_input":"2022-11-27T19:42:55.331439Z","iopub.status.idle":"2022-11-27T19:42:55.338184Z","shell.execute_reply.started":"2022-11-27T19:42:55.331407Z","shell.execute_reply":"2022-11-27T19:42:55.337560Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, CFG):\n        super(Model, self).__init__()\n        cfg = AutoConfig.from_pretrained(CFG['model'])\n        cfg.num_labels=1\n        self.bert = AutoModelForSequenceClassification.from_config(cfg)\n \n    def forward(self, input_ids, attention_mask):\n        y = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n        return y","metadata":{"_uuid":"97cfe97f-56f5-454d-a7b6-1a2f803ed5ad","_cell_guid":"ea05de39-dc69-46be-b61e-9d9681de5682","collapsed":false,"id":"4c5bab44","papermill":{"duration":0.066203,"end_time":"2022-03-22T09:40:52.37203","exception":false,"start_time":"2022-03-22T09:40:52.305827","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-13T03:48:28.57061Z","iopub.execute_input":"2022-05-13T03:48:28.571118Z","iopub.status.idle":"2022-05-13T03:48:28.584396Z","shell.execute_reply.started":"2022-05-13T03:48:28.571052Z","shell.execute_reply":"2022-05-13T03:48:28.583553Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model, val_loader, sigmoid=False):\n    model.eval()\n    y_pred = []\n    \n    with torch.no_grad():\n        tk = tqdm(val_loader, total=len(val_loader), position=0, leave=True)\n        for step, batch in enumerate(tk):\n            input_ids, attention_mask = [x.to(device) for x in batch]\n            \n            output = model(input_ids, attention_mask).squeeze(-1)\n            \n            if sigmoid:\n                output = output.sigmoid()\n            \n            y_pred.extend(output.cpu().numpy())\n\n    return np.array(y_pred)","metadata":{"_uuid":"655b3450-bce2-4088-877c-5740ede11f4e","_cell_guid":"26ae88d0-f33b-446b-ac29-27e18f9bee49","collapsed":false,"execution":{"iopub.status.busy":"2022-05-13T03:48:28.58584Z","iopub.execute_input":"2022-05-13T03:48:28.586418Z","iopub.status.idle":"2022-05-13T03:48:28.596649Z","shell.execute_reply.started":"2022-05-13T03:48:28.586383Z","shell.execute_reply":"2022-05-13T03:48:28.595911Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = all_df[all_df.flag==1]\n\nw = [0.01, 0.02, -0.03, 0.01, 0.05, 0.03, 0.05, 0.06, 0.12, 0.1, 0.07, -0.02, 0.1, 0.04, 0.03, 0.07, 0.1, 0.11, 0.05]\n\nensemble_predictions = []\n\nfor CFG in [CFG3, CFG4, CFG11, CFG17, CFG20, CFG32, CFG35, CFG36, CFG38, CFG39, CFG40, CFG49, CFG61, CFG62, CFG63, CFG71, CFG74, CFG80, CFG69]:\n    predictions = []\n    \n    tokenizer = AutoTokenizer.from_pretrained(CFG['model'])\n\n    ## sort df\n    input_lengths = []\n    if CFG in [CFG17, CFG19, CFG20, CFG29, CFG32]:\n        for text in test_df['text2'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)\n    elif CFG in [CFG38, CFG39, CFG40, CFG49]:\n        for text in test_df['text3'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)\n    elif CFG in [CFG61, CFG62, CFG63, CFG69, CFG71]:\n        for text in test_df['text5'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)   \n    elif CFG in [CFG74, CFG80]:\n        for text in test_df['text6'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)     \n    else:\n        for text in test_df['text1'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)\n    test_df['input_lengths'] = input_lengths\n    length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n    sort_df = test_df.iloc[length_sorted_idx]\n    \n\n    if CFG in [CFG17, CFG19, CFG20, CFG29, CFG32]:\n        test_set = MyDataset(sort_df, 1)\n    elif CFG in [CFG38, CFG39, CFG40, CFG49]:\n        test_set = MyDataset(sort_df, 2)\n    elif CFG in [CFG61, CFG62, CFG63, CFG69, CFG71]:\n        test_set = MyDataset(sort_df, 4)\n    elif CFG in [CFG74, CFG80]:\n        test_set = MyDataset(sort_df, 5)\n    else:\n        test_set = MyDataset(sort_df, 0)\n    \n    test_loader = DataLoader(test_set, batch_size=CFG['valid_bs'], shuffle=False, collate_fn=collate_fn_fast, num_workers=CFG['num_workers'])\n\n    model = Model(CFG).to(device)\n\n    for fold in range(CFG['fold_num']):\n        model.load_state_dict(torch.load('{}_fold_{}.pt'.format(CFG['path'], fold)))\n        prediction = test_model(model, test_loader, CFG['sigmoid'])\n        \n        prediction = prediction[np.argsort(length_sorted_idx)]\n            \n        predictions.append(prediction)\n        \n    predictions = np.mean(predictions, 0)\n    ensemble_predictions.append(predictions)\n\nstage1_predictions = np.sum([w[i]*ensemble_predictions[i] for i in range(len(w))], 0)","metadata":{"_uuid":"1ab15c76-a841-46db-ac87-396a75ef7f82","_cell_guid":"14dd209d-f91e-4e7f-87ae-cb7adb376034","collapsed":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-13T03:54:40.398185Z","iopub.execute_input":"2022-05-13T03:54:40.398479Z","iopub.status.idle":"2022-05-13T03:58:31.047941Z","shell.execute_reply.started":"2022-05-13T03:54:40.398446Z","shell.execute_reply":"2022-05-13T03:58:31.046139Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df.loc[all_df.flag==0, 'oof'] = np.load('../input/upppm-oof/oof_cv8697.npy')\nall_df.loc[all_df.flag==1, 'oof'] = ((stage1_predictions-stage1_predictions.min())/(stage1_predictions.max()-stage1_predictions.min())*100).round().astype('int')\n\nall_df['target_oof'] = all_df['target'] + ' ' + all_df['oof'].astype('str')\nall_df = all_df.join(all_df.groupby('anchor').target_oof.agg(list).rename('ref4'), on='anchor')\nall_df.ref4 = all_df.ref4.apply(lambda x:sorted(list(set(x)), key=x.index))\n\nall_df = all_df.join(all_df.groupby(['anchor', 'context']).target_oof.agg(list).rename('ref5'), on=['anchor', 'context'])\n\ntest_df = all_df[all_df.flag==1]\n\ntest_df['ref4'] = test_df.apply(lambda x:', '.join([i for i in x['ref4'] if i != x['target_oof']]), axis=1)\ntest_df['ref5'] = test_df.apply(lambda x:', '.join([i for i in x['ref5'] if i != x['target_oof']]), axis=1)\ntest_df['text2'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]' + test_df['context_text'] + '[SEP]' + test_df['ref4']\ntest_df['text4'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]' + test_df['context_text'] + '[SEP]' + test_df['ref5']","metadata":{"_uuid":"c3088bdc-30e1-4a0f-85d0-15c63d5101eb","_cell_guid":"f6849847-4e95-460f-a31b-f6eed1b31a2b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = [0.01, 0.02, -0.03, 0.01, 0.04, 0.03, 0.04, 0.04, 0.08, 0.06, 0.05, -0.03, 0.07, -0.01, 0.02, 0.04, 0.09, 0.09, 0.05, 0.01, 0.09, 0.12]\n\nfor CFG in [CFG26, CFG64, CFG70]:\n    predictions = []\n    \n    tokenizer = AutoTokenizer.from_pretrained(CFG['model'])\n\n    ## sort df\n    input_lengths = []\n    if CFG in [CFG26, CFG33]:\n        for text in test_df['text2'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)\n    else:\n        for text in test_df['text4'].values:\n            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n            if length > CFG['max_len']:\n                length = CFG['max_len'] \n            input_lengths.append(length)\n    test_df['input_lengths'] = input_lengths\n    length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n    sort_df = test_df.iloc[length_sorted_idx]\n    \n    if CFG in [CFG26, CFG33]:\n        test_set = MyDataset(sort_df, 1)\n    else:\n        test_set = MyDataset(sort_df, 4)\n    test_loader = DataLoader(test_set, batch_size=CFG['valid_bs'], shuffle=False, collate_fn=collate_fn_fast, num_workers=CFG['num_workers'])\n\n    model = Model(CFG).to(device)\n\n    for fold in range(CFG['fold_num']):\n        model.load_state_dict(torch.load('{}_fold_{}.pt'.format(CFG['path'], fold)))\n        prediction = test_model(model, test_loader, CFG['sigmoid'])\n        \n        prediction = prediction[np.argsort(length_sorted_idx)]\n            \n        predictions.append(prediction)\n        \n    predictions = np.mean(predictions, 0)\n    ensemble_predictions.append(predictions)\n\nfinal_predictions = np.sum([w[i]*ensemble_predictions[i] for i in range(len(w))], 0)","metadata":{"_uuid":"59101deb-0167-4b0d-aaf4-bb19320163bc","_cell_guid":"28b18875-1e28-4a33-8aa6-9280477ce591","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\nsubmission['score'] = final_predictions\nsubmission[['id', 'score']].to_csv('submission.csv', index=False)\nsubmission","metadata":{"_uuid":"8bb575d6-a6d1-42ef-be9d-887d7793c85a","_cell_guid":"dfe75101-38a3-42da-9edd-8a5f06179d67","collapsed":false,"execution":{"iopub.status.busy":"2022-05-13T03:58:31.050111Z","iopub.execute_input":"2022-05-13T03:58:31.050552Z","iopub.status.idle":"2022-05-13T03:58:31.110197Z","shell.execute_reply.started":"2022-05-13T03:58:31.0505Z","shell.execute_reply":"2022-05-13T03:58:31.109405Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}