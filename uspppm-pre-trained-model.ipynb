{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers rake_nltk scispacy","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:27:18.790715Z","iopub.execute_input":"2022-06-14T18:27:18.791078Z","iopub.status.idle":"2022-06-14T18:28:00.172473Z","shell.execute_reply.started":"2022-06-14T18:27:18.791044Z","shell.execute_reply":"2022-06-14T18:28:00.171378Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n\u001b[K     |████████████████████████████████| 79 kB 1.8 MB/s eta 0:00:011\n\u001b[?25hCollecting rake_nltk\n  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\nCollecting scispacy\n  Downloading scispacy-0.5.0-py3-none-any.whl (44 kB)\n\u001b[K     |████████████████████████████████| 44 kB 1.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n\u001b[K     |████████████████████████████████| 4.2 MB 4.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.62.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.1+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.8.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.19.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.23.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.0.17)\nCollecting nltk\n  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 40.1 MB/s eta 0:00:01\n\u001b[?25hCollecting nmslib>=1.7.3.6\n  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n\u001b[K     |████████████████████████████████| 13.5 MB 32.0 MB/s eta 0:00:01\n\u001b[?25hCollecting spacy<3.3.0,>=3.2.0\n  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n\u001b[K     |████████████████████████████████| 6.0 MB 42.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scispacy) (2.25.1)\nCollecting pysbd\n  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n\u001b[K     |████████████████████████████████| 71 kB 5.5 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from scispacy) (1.0.1)\nCollecting conllu\n  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.1)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (2021.8.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from nmslib>=1.7.3.6->scispacy) (5.8.0)\nCollecting pybind11<2.6.2\n  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n\u001b[K     |████████████████████████████████| 188 kB 54.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2021.5.30)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.26.6)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.10)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (57.4.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (1.0.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (0.7.4)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (0.8.2)\nCollecting typer<0.5.0,>=0.3.0\n  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\nCollecting langcodes<4.0.0,>=3.2.0\n  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n\u001b[K     |████████████████████████████████| 181 kB 39.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (3.0.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (21.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (1.8.2)\nCollecting thinc<8.1.0,>=8.0.12\n  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n\u001b[K     |████████████████████████████████| 660 kB 42.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (3.7.4.3)\nCollecting catalogue<2.1.0,>=2.0.6\n  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\nCollecting pathy>=0.3.5\n  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 861 kB/s  eta 0:00:01\n\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n\u001b[K     |████████████████████████████████| 457 kB 46.9 MB/s eta 0:00:01\n\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\nCollecting spacy-legacy<3.1.0,>=3.0.8\n  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (3.0.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->scispacy) (2.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->scispacy) (3.5.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->sentence-transformers) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->scispacy) (2.4.7)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->scispacy) (5.2.1)\nCollecting huggingface-hub\n  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n\u001b[K     |████████████████████████████████| 86 kB 3.1 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[K     |████████████████████████████████| 6.6 MB 39.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->scispacy) (2.0.1)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.2.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120748 sha256=ac91563679448fcab864a7d2dfaecbb703b01dcf4b2b7c587b202643d03aa4f3\n  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\nSuccessfully built sentence-transformers\nInstalling collected packages: catalogue, typer, srsly, tokenizers, thinc, spacy-loggers, spacy-legacy, pybind11, pathy, langcodes, huggingface-hub, transformers, spacy, pysbd, nmslib, nltk, conllu, sentence-transformers, scispacy, rake-nltk\n  Attempting uninstall: catalogue\n    Found existing installation: catalogue 1.0.0\n    Uninstalling catalogue-1.0.0:\n      Successfully uninstalled catalogue-1.0.0\n  Attempting uninstall: srsly\n    Found existing installation: srsly 1.0.5\n    Uninstalling srsly-1.0.5:\n      Successfully uninstalled srsly-1.0.5\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.10.3\n    Uninstalling tokenizers-0.10.3:\n      Successfully uninstalled tokenizers-0.10.3\n  Attempting uninstall: thinc\n    Found existing installation: thinc 7.4.5\n    Uninstalling thinc-7.4.5:\n      Successfully uninstalled thinc-7.4.5\n  Attempting uninstall: pybind11\n    Found existing installation: pybind11 2.7.1\n    Uninstalling pybind11-2.7.1:\n      Successfully uninstalled pybind11-2.7.1\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.0.17\n    Uninstalling huggingface-hub-0.0.17:\n      Successfully uninstalled huggingface-hub-0.0.17\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.5.1\n    Uninstalling transformers-4.5.1:\n      Successfully uninstalled transformers-4.5.1\n  Attempting uninstall: spacy\n    Found existing installation: spacy 2.3.7\n    Uninstalling spacy-2.3.7:\n      Successfully uninstalled spacy-2.3.7\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.7 which is incompatible.\nfastai 2.2.7 requires spacy<3, but you have spacy 3.2.4 which is incompatible.\nen-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.2.4 which is incompatible.\nen-core-web-lg 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.2.4 which is incompatible.\ndatasets 1.12.1 requires huggingface-hub<0.1.0,>=0.0.14, but you have huggingface-hub 0.7.0 which is incompatible.\nallennlp 2.7.0 requires spacy<3.2,>=2.1.0, but you have spacy 3.2.4 which is incompatible.\nallennlp 2.7.0 requires transformers<4.10,>=4.1, but you have transformers 4.19.4 which is incompatible.\u001b[0m\nSuccessfully installed catalogue-2.0.7 conllu-4.4.2 huggingface-hub-0.7.0 langcodes-3.3.0 nltk-3.7 nmslib-2.1.1 pathy-0.6.1 pybind11-2.7.1 pysbd-0.3.4 rake-nltk-1.0.6 scispacy-0.5.0 sentence-transformers-2.2.0 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 tokenizers-0.12.1 transformers-4.19.4 typer-0.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m spacy validate","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:30:06.377174Z","iopub.execute_input":"2022-06-14T18:30:06.377493Z","iopub.status.idle":"2022-06-14T18:31:07.865779Z","shell.execute_reply.started":"2022-06-14T18:30:06.377459Z","shell.execute_reply":"2022-06-14T18:31:07.864398Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"2022-06-14 18:30:07.979342: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-06-14 18:30:07.979402: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nCollecting en-core-web-lg==3.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n\u001b[K     |████████████████████████████████| 777.4 MB 3.7 kB/s  eta 0:00:01     |███████▉                        | 190.5 MB 35.4 MB/s eta 0:00:17     |████████████▍                   | 300.2 MB 50.9 MB/s eta 0:00:10     |███████████████████▋            | 476.9 MB 43.9 MB/s eta 0:00:07��█▊            | 478.2 MB 43.9 MB/s eta 0:00:07██████████████▏           | 490.9 MB 43.9 MB/s eta 0:00:07\n\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-lg==3.2.0) (3.2.4)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.5)\nRequirement already satisfied: click<8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (21.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.19.5)\nRequirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.17)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.1)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.7.4.3)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.7)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.8.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.62.1)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (57.4.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.25.1)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.9)\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.5.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.7)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.1)\n\u001b[33mWARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/pybind11-2.7.1.dist-info/METADATA'\u001b[0m\nInstalling collected packages: en-core-web-lg\n  Attempting uninstall: en-core-web-lg\n    Found existing installation: en-core-web-lg 2.3.1\n    Uninstalling en-core-web-lg-2.3.1:\n      Successfully uninstalled en-core-web-lg-2.3.1\nSuccessfully installed en-core-web-lg-3.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_lg')\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport sys\nimport re\nimport random\nimport json\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport copy\nimport math\nimport shutil\nimport string\nimport pickle\nimport joblib\nimport asyncio\nimport scipy as sp\nimport networkx as nx\nfrom joblib import Parallel, delayed\nfrom pathlib import Path\nimport dask.dataframe as dd\nfrom dask.diagnostics import ProgressBar\n\nimport cv2\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport torch\nimport transformers\nimport tokenizers\nprint(f\"torch.__version__: {torch.__version__}\")\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom tqdm import tqdm\nfrom torch.optim import Adam, SGD, AdamW\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom torch.utils.data import DataLoader, Dataset\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nimport itertools\nimport unicodedata\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nfrom itertools import groupby\nfrom sentence_transformers import SentenceTransformer\nimport spacy\nfrom rake_nltk import Rake\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS, ALPHA, ALPHA_LOWER, ALPHA_UPPER\nfrom spacy.util import compile_infix_regex\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom spacy.pipeline import EntityRecognizer\n\nnlp = spacy.load('en_core_web_lg')\n#nlp.add_pipe(\"abbreviation_detector\")\n\nre_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:31:26.739612Z","iopub.execute_input":"2022-06-14T18:31:26.740033Z","iopub.status.idle":"2022-06-14T18:31:29.491775Z","shell.execute_reply.started":"2022-06-14T18:31:26.739988Z","shell.execute_reply":"2022-06-14T18:31:29.490829Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"torch.__version__: 1.7.1+cpu\ntokenizers.__version__: 0.10.3\ntransformers.__version__: 4.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.set_option('display.precision', 4)\n\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 42\nCUSTOM_BATCH = 24\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Stopwords and infixes\nADDITIONAL_STOPWORDS = ['one or more', 'a', 'needn', 'a', 'not', 'able', 'never', 'about', 'needn’t', 'accordance', 'now', 'often', 'above', 'no', 'according', 'of', 'mentioned', 'others', 'after', 'nor', 'all', 'on', 'accordingly', 'otherwise', 'again', 'not', 'also', 'onto', 'across', 'overall', 'against', 'now', 'an', 'or', 'along', 'rather', 'ain', 'o', 'and', 'other', 'already', 'remarkably', 'all', 'of', 'another', 'particularly', 'alternatively', 'significantly', 'am', 'off', 'are', 'preferably', 'always', 'simply', 'an', 'on', 'as', 'preferred', 'among', 'sometimes', 'and', 'once', 'at', 'present', 'and/or', 'specifically', 'any', 'only', 'be', 'provide', 'anything', 'straight', 'are', 'or', 'because', 'provided', 'anywhere', 'forward', 'aren', 'other', 'been', 'provides', 'better', 'substantially', 'aren’t', 'our', 'being', 'relatively', 'disclosure', 'thereafter', 'as', 'ours', 'by', 'respectively', 'due', 'therebetween', 'at', 'ourselves', 'claim', 'said', 'easily', 'therefor', 'be', 'out', 'comprises', 'comprising', 'should', 'easy', 'therefrom', 'because', 'over', 'since', 'e.g', 'therein', 'been', 'own', 'could', 'some', 'either', 'thereinto', 'before', 're', 'described', 'such', 'elsewhere', 'thereon', 'being', 's', 'desired', 'suitable', 'enough', 'therethrough', 'below', 'same', 'do', 'than', 'especially', 'therewith', 'between', 'shan', 'does', 'that', 'essentially', 'together', 'both', 'shan’t', 'each', 'the', 'et', 'al', 'toward', 'but', 'she', 'embodiment', 'their', 'etc', 'towards', 'by', 'she’s', 'fig', 'then', 'eventually', 'typical', 'can', 'should', 'figs', 'there', 'excellent', 'upon', 'couldn', 'should’ve', 'for', 'thereby', 'finally', 'via', 'couldn’t', 'shouldn', 'from', 'therefore', 'furthermore', 'vice', 'versa', 'd', 'shouldn’t', 'further', 'thereof', 'good', 'whatever', 'did', 'so', 'generally', 'thereto', 'hence', 'whereas', 'didn', 'some', 'had', 'these', 'he/she', 'whereat', 'didn’t', 'such', 'has', 'they', 'him/her', 'wherever', 'do', 't', 'have', 'this', 'his/her', 'whether', 'does', 'than', 'having', 'those', 'ie', 'whose', 'doesn', 'that', 'herein', 'thus', 'ii', 'within', 'doesn’t', 'that’ll', 'however', 'to', 'iii', 'without', 'doing', 'the', 'if', 'use', 'instead', 'yet', 'don', 'their', 'in', 'various', 'later', 'don’t', 'theirs', 'into', 'was', 'like', 'down', 'them', 'invention', 'were', 'little', 'during', 'themselves', 'is', 'what', 'many', 'each', 'there', 'it', 'when', 'may', 'few', 'these', 'its', 'where', 'meanwhile', 'for', 'they', 'means', 'whereby', 'might', 'from', 'this', 'wherein', 'moreover', 'further', 'those', 'which', 'much', 'had', 'through', 'while', 'must', 'hadn', 'to', 'who', 'hadn’t', 'too', 'will', 'has', 'under', 'with', 'hasn', 'until', 'Would', 'hasn’t', 'up', 'have', 've', 'haven', 'very', 'haven’t', 'was', 'having', 'wasn', 'he', 'wasn’t', 'her', 'we', 'here', 'were', 'hers', 'weren', 'herself', 'weren’t', 'him', 'what', 'himself', 'when', 'his', 'where', 'how', 'which', 'i', 'while', 'if', 'who', 'in', 'whom', 'into', 'why', 'is', 'will', 'isn', 'with', 'isn’t', 'won', 'it', 'won’t', 'it’s', 'wouldn', 'its', 'wouldn’t', 'itself', 'y', 'just', 'you', 'll', 'you’d', 'm', 'you’ll', 'ma', 'you’re', 'me', 'you’ve', 'mightn', 'your', 'mightn’t', 'yours', 'more', 'yourself', 'most', 'yourselves', 'mustn', 'mustn’t', 'my', 'myself']\n\npuncts = ['\\u200d','?', '....','..','...','', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\',\n    '•', '~', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█',\n    '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓',\n    '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾',\n    'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n    '¹', '≤', '‡', '√', '!','🅰','🅱']\n\ninfixes = (\n    LIST_ELLIPSES\n    + LIST_ICONS\n    + [\n        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n        ),\n        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n        # ✅ Commented out regex that splits on hyphens between letters:\n        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n        r'''[-~]'''\n    ]\n)\n\ntable = {\n'A': 'Human Necessities',\n'B': 'Operations and Transport',\n'C': 'Chemistry and Metallurgy',\n'D': 'Textiles',\n'E': 'Fixed Constructions',\n'F': 'Mechanical Engineering',\n'G': 'Physics',\n'H': 'Electricity',\n'Y': 'Emerging Cross-Sectional Technologies'\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:31:41.860631Z","iopub.execute_input":"2022-06-14T18:31:41.860931Z","iopub.status.idle":"2022-06-14T18:31:41.898241Z","shell.execute_reply.started":"2022-06-14T18:31:41.860903Z","shell.execute_reply":"2022-06-14T18:31:41.897262Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:31:45.022052Z","iopub.execute_input":"2022-06-14T18:31:45.022625Z","iopub.status.idle":"2022-06-14T18:31:45.026569Z","shell.execute_reply.started":"2022-06-14T18:31:45.022588Z","shell.execute_reply":"2022-06-14T18:31:45.025759Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\ndef inference_fn(test_loader, model, device, is_sigmoid=True):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n\n        with torch.no_grad():\n            output = model(inputs)\n\n        if is_sigmoid == True:\n            preds.append(output.sigmoid().to('cpu').numpy())\n        else:\n            preds.append(output.to('cpu').numpy())\n\n    return np.concatenate(preds)\n\n\ndef upd_outputs(data, is_trim=False, is_minmax=False, is_reshape=False):\n    min_max_scaler = MinMaxScaler()\n\n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n\n    if is_reshape == True:\n        data = data.reshape(-1)\n\n    return data\n\n\ndef parse_sections(out):\n    title = out.split(patent)[1].split(' - ')[1].split('\\n')[0].strip()\n    out = out.split(title)[3].split('Abstract')[1].split(' US1')[0]\n    out = out.replace('.\\n', '. ').replace('\\n', ' ').replace('\\r', '')\n    out = 'ABSTRACT ' + ' '.join(out.split()).strip()\n\n    abstract = out.split('ABSTRACT ')[1].split('BACKGROUND')[0].strip()\n\n    if 'BRIEF DESCRIPTION' in out:\n        background = out.split('BACKGROUND ')[1].split('BRIEF DESCRIPTION')[0].strip()\n        brief_description = out.split('BRIEF DESCRIPTION ')[1].split('DESCRIPTION OF DRAWINGS')[0].strip()\n    else:\n        background = out.split('BACKGROUND ')[1].split('SUMMARY ')[0].strip()\n        brief_description = out.split('SUMMARY ')[1].split('DESCRIPTION OF DRAWINGS')[0].strip()\n\n    if 'DESCRIPTION OF EMBODIMENTS' in out:\n        desc = 'DESCRIPTION OF EMBODIMENTS '\n    else:\n        desc = 'DETAILED DESCRIPTION '\n\n    if 'The invention claimed is:' in out:\n        detailed_description = out.split(desc)[1].split('The invention claimed is:')[0].strip()\n        claims = out.split('The invention claimed is: ')[1].split(' US1')[0]\n    elif 'What is claimed is:' in out:\n        detailed_description = out.split(desc)[1].split('What is claimed is:')[0].strip()\n        claims = out.split('What is claimed is: ')[1].split(' US1')[0]\n    else:\n        detailed_description = out.split(desc)[1].split('Claims')[0].strip()\n        claims = out.split('Claims')[1].split(' US1')[0]\n\n    return abstract, background, brief_description, detailed_description, claims\n\ndef flatten(l):\n    \"\"\"\n    Flatten list of lists.\n    \"\"\"\n    import collections\n\n    for el in l:\n        if isinstance(\n                el, collections.Iterable) and not isinstance(\n                el, (str, bytes)):\n            for ell in flatten(el):\n                yield ell\n        else:\n            yield el\n\ndef split_claims(claims):\n    nums = list(map(str, re.findall(r'\\d+\\.\\s', claims)))\n    claims_dict = {}\n    total_claims = len(nums)\n    for ix, num in enumerate(nums):\n         if ix == total_claims - 1:\n             claims_dict[f'Claim {num}'.replace('. ', '')] = claims.split(num)[1]\n             break\n         claims_dict[f'Claim {num}'.replace('. ', '')] = claims.split(num)[1].split(nums[ix+1])[0]\n    return claims_dict\n\ndef tokenize(sentence, infixes):\n    all_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n    infix_re = compile_infix_regex(infixes)\n\n    def customize_tokenizer(nlp):\n        # Adds support to use `-` as the delimiter for tokenization\n        return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n                         suffix_search=suffix_re.search,\n                         infix_finditer=infix_re.finditer,\n                         token_match=None\n                        )\n\n    nlp.tokenizer = customize_tokenizer(nlp)\n    return [token.text for token in sentence if not token.is_stop]\n\ndef remove_from_list(x, stuff_to_remove) -> list:\n    for item in stuff_to_remove:\n        # Making sure to iterate through the entire token\n        for i,token in enumerate(x):\n            if item == token:\n                del x[i]\n    return x\n\ndef Remove_Duplicates(text_in):\n    return re.sub(r\"\\b(\\w+)(?:\\W\\1\\b)+\", r\"\\1\", text_in, flags=re.IGNORECASE)\n\n\ndef remove_consecutive_nums(text):\n    # Remove any chunks of consecutive numbers\n    number_strings = re.findall(r'\\d+[ \\t]\\d+', text)\n    ind_num_strings = []\n    for j in number_strings:\n        x = [int(i) for i in j.split()]\n        ind_num_strings.append(x)\n\n    flat_num_list = [item for sublist in ind_num_strings for item in sublist]\n\n    for i in flat_num_list:\n        j=re.sub(r'\\d+','',str(i))\n        text = text.replace(str(i),j)\n    return text\n\n\ndef basic_clean(text_list, infixes, stopwords):\n    \"\"\"\n    A simple function to clean up the data. All the words that\n    are not designated as a stop word is then lemmatized after\n    encoding and basic regex parsing are performed.\n    \"\"\"\n\n    text_list_clean = []\n    for text in text_list:\n        text = re.sub(r'[\\)\\(\\.\\,\\;\\\\\\?\\&\\%\\!\\+\\-]', '', re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6§\\\\\\£\\Â*_<>\"\"⎫•{}Γ~]', ' ', str(' '.join(re.split('\\s*-\\s*', text)))))\n        if len(text.split(\"  \")) > 1000:\n            text = \" \".join([\"\".join(w.split(\" \")) if len(w.split(' '))>1 else w for w in text.split(\"  \")])\n        text_list_clean.append([i for i in remove_from_list(re.sub('\\s+', ' ', re.sub('\\s\\s+', ' ', re.sub('\\s+\\s+', ' ', Remove_Duplicates(re.sub(r\"\\b(?=[mdclxvii])m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})([ii]x|[ii]v|v?[ii]{0,3})\\b\\.?\", '', (unicodedata.normalize('NFKD', re.sub(' +', ' ', re.sub(r\"\\s+\\s+\",\" \", re.sub(r\"\\\\,\",\",\", re.sub(r\" \\,\",\",\", re.sub(r\"\\\\.\",\".\", re.sub(r\" \\.\",\".\", re.sub(r\"\\(\\s+\\)\",\"\", re.sub(r\"\\(\\)\",\"\", re.sub(r\" \\)\",\"\", re.sub(r\"\\( \",\"\", remove_consecutive_nums(re.sub(r\"\\s+\",\" \", re.sub(r\"([A-z])\\- ([A-z])\", r\"\\1\\2\", re.sub(r'\\s', ' ', text)).replace('\\'','').replace('. .', '.').replace('\\'',''))))))))))))).lower())\n        .encode('ascii', 'ignore')\n        .decode('utf-8', 'ignore')\n        .lower())))))).split(), puncts) if not i.isdigit() or i in stopwords])\n        del text\n\n    return '. '.join(x.strip().capitalize() for x in '. '.join(' '.join([word for word in sent]) for sent in text_list_clean).split('.')) + '.'\n\n\ndef get_cpc_texts():\n    \"\"\"\n    Function taken from Y Nakama's notebook:\n    https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\n    \"\"\"\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:31:47.135205Z","iopub.execute_input":"2022-06-14T18:31:47.135498Z","iopub.status.idle":"2022-06-14T18:31:47.195140Z","shell.execute_reply.started":"2022-06-14T18:31:47.135465Z","shell.execute_reply":"2022-06-14T18:31:47.194417Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv(f\"{INPUT_DIR}train.csv\")\n# test = pd.read_csv(f\"{INPUT_DIR}test.csv\")\n# submission = pd.read_csv(f\"{INPUT_DIR}sample_submission.csv\")\n# print(f\"train.shape: {train.shape}\")\n# print(f\"test.shape: {test.shape}\")\n# print(f\"submission.shape: {submission.shape}\")\n# train.head()\n# test.head()\n# submission.head()\n\n# train['general_context'] = train['context'].apply(lambda x: table[x[0].upper()])\n# test['general_context'] = test['context'].apply(lambda x: table[x[0].upper()])\n\n# train = pd.concat([train, pd.get_dummies(train['general_context'])], axis=1)\n# test = pd.concat([test, pd.get_dummies(test['general_context'])], axis=1)\n\n# cpc_texts = get_cpc_texts()\n# torch.save(cpc_texts, f\"{OUTPUT_DIR}cpc_texts.pth\")\n# train['context_text'] = train['context'].map(cpc_texts)\n# test['context_text'] = test['context'].map(cpc_texts)\n\n\n# train['section'] = train['context'].astype(str).str[0]\n# train['classes'] = train['context'].astype(str).str[1:]\n\n# train['num_anchor'] = train['anchor'].str.contains('[0-9]', na=False)\n# train['anchor_len'] = train['anchor'].str.split().str.len()\n# train['num_target'] = train['target'].str.contains('[0-9]', na=False)\n# train['target_len'] = train['target'].str.split().str.len()\n# train.target_len.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:28:02.939474Z","iopub.status.idle":"2022-06-14T18:28:02.940207Z","shell.execute_reply.started":"2022-06-14T18:28:02.939859Z","shell.execute_reply":"2022-06-14T18:28:02.939889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"seed_everything(CUSTOM_SEED)\n\nLOGGER = get_logger()\n\nclass CFG:\n    competition='PPPM'\n    debug=False\n    apex=True\n    print_freq=100\n    num_workers=4\n    #input_path = '../input/traintestcleaned'\n    input_path = './'\n    model=\"microsoft/deberta-v3-large\"\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    batch_size=19\n    fc_dropout=0.2\n    target_size=1\n    max_len=512\n    weight_decay=0.01\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f\"{CFG.input_path}train_clean.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:31:55.661315Z","iopub.execute_input":"2022-06-14T18:31:55.662089Z","iopub.status.idle":"2022-06-14T18:31:56.525415Z","shell.execute_reply.started":"2022-06-14T18:31:55.662052Z","shell.execute_reply":"2022-06-14T18:31:56.524599Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0]\n\ntrain['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\nFold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ntrain.groupby('fold').size()\n\nif CFG.debug:\n    train.groupby('fold').size()\n    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n    train.groupby('fold').size()\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.model)\ntokenizer.save_pretrained(OUTPUT_DIR + 'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:32:23.642601Z","iopub.execute_input":"2022-06-14T18:32:23.643323Z","iopub.status.idle":"2022-06-14T18:32:27.041429Z","shell.execute_reply.started":"2022-06-14T18:32:23.643282Z","shell.execute_reply":"2022-06-14T18:32:27.040376Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7291c92e04014db88b93b178186d7aca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a3580069b34c6c8ab22d2eb89cf956"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb935b3903c341398ecccbe0af20a36d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Token Lengths","metadata":{}},{"cell_type":"code","source":"#cpc_texts = torch.load('../input/cpc-texts/cpc_texts.pth')\ncpc_texts = torch.load('cpc_texts.pth')\nlengths_dict = {}\n\nlengths = []\ntk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nlengths_dict['context_text'] = lengths\n\nfor text_col in ['anchor', 'target']:\n    lengths = []\n    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    lengths_dict[text_col] = lengths\n\nCFG.max_len = max(lengths_dict['anchor']) + max(lengths_dict['target']) + max(lengths_dict['context_text']) + 4\nLOGGER.info(f\"max_len: {CFG.max_len}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:32:31.924363Z","iopub.execute_input":"2022-06-14T18:32:31.924665Z","iopub.status.idle":"2022-06-14T18:32:38.738703Z","shell.execute_reply.started":"2022-06-14T18:32:31.924626Z","shell.execute_reply":"2022-06-14T18:32:38.737879Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 136/136 [00:00<00:00, 5583.65it/s]\n100%|██████████| 36473/36473 [00:03<00:00, 11089.32it/s]\n100%|██████████| 36473/36473 [00:03<00:00, 10554.05it/s]\nmax_len: 133\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.labels = df['score'].values\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        label = torch.tensor(self.labels[item], dtype=torch.float)\n        return inputs, label","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:32:53.373349Z","iopub.execute_input":"2022-06-14T18:32:53.373680Z","iopub.status.idle":"2022-06-14T18:32:53.380929Z","shell.execute_reply.started":"2022-06-14T18:32:53.373647Z","shell.execute_reply":"2022-06-14T18:32:53.380010Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        return torch.sum(weights * last_hidden_states, dim=1)\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        return self.fc(self.fc_dropout(feature))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:32:58.841032Z","iopub.execute_input":"2022-06-14T18:32:58.841338Z","iopub.status.idle":"2022-06-14T18:32:58.859674Z","shell.execute_reply.started":"2022-06-14T18:32:58.841306Z","shell.execute_reply":"2022-06-14T18:32:58.858630Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, labels) in enumerate(train_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.cuda.amp.autocast(enabled=CFG.apex):\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step, len(train_loader),\n                          remain=timeSince(start, float(step+1)/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, labels) in enumerate(valid_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, np.concatenate(predictions)\n\n\ndef train_loop(folds, fold):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    valid_labels = valid_folds['score'].values\n\n    train_dataset = TrainDataset(CFG, train_folds)\n    valid_dataset = TrainDataset(CFG, valid_folds)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, config_path=None, pretrained=True)\n    torch.save(model.config, OUTPUT_DIR + 'config.pth')\n    model.to(device)\n\n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr,\n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n\n    # ====================================================\n    # scheduler\n    # ====================================================\n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif cfg.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n            )\n        return scheduler\n\n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    best_score = 0.\n\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n\n        # scoring\n        score = get_score(valid_labels, predictions)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if best_score < score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n\n    predictions = torch.load(OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                             map_location=torch.device('cpu'))['predictions']\n    valid_folds['pred'] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return valid_folds","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:33:07.921515Z","iopub.execute_input":"2022-06-14T18:33:07.922298Z","iopub.status.idle":"2022-06-14T18:33:08.465323Z","shell.execute_reply.started":"2022-06-14T18:33:07.922253Z","shell.execute_reply":"2022-06-14T18:33:08.464558Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_result(oof_df):\n    labels = oof_df['score'].values\n    preds = oof_df['pred'].values\n    score = get_score(labels, preds)\n    LOGGER.info(f'Score: {score:<.4f}')\n\nif CFG.train:\n    oof_df = pd.DataFrame()\n    for fold in range(CFG.n_fold):\n        if fold in CFG.trn_fold:\n            _oof_df = train_loop(train, fold)\n            oof_df = pd.concat([oof_df, _oof_df])\n            LOGGER.info(f\"========== fold: {fold} result ==========\")\n            get_result(_oof_df)\n    oof_df = oof_df.reset_index(drop=True)\n    LOGGER.info(f\"========== CV ==========\")\n    get_result(oof_df)\n    oof_df.to_pickle(OUTPUT_DIR + 'oof_df.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T18:38:25.351345Z","iopub.execute_input":"2022-06-14T18:38:25.351631Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"========== fold: 0 training ==========\n/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py:114: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}