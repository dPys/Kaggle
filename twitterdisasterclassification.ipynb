{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import string\nimport statistics\nimport nltk\nimport spacy\nimport matplotlib\nimport os\nimport string\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom joblib import parallel_backend\nfrom sklearn import linear_model, decomposition\nfrom collections import OrderedDict\nfrom operator import itemgetter\nfrom sklearn import metrics\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\nfrom sklearn.feature_selection import f_classif, VarianceThreshold, SelectFwe\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone, RegressorMixin\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\n\ntry:\n    from sklearn.utils._testing import ignore_warnings\nexcept:\n    from sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning","metadata":{"execution":{"iopub.status.busy":"2022-03-26T06:08:00.320638Z","iopub.execute_input":"2022-03-26T06:08:00.321336Z","iopub.status.idle":"2022-03-26T06:08:12.582033Z","shell.execute_reply.started":"2022-03-26T06:08:00.321295Z","shell.execute_reply":"2022-03-26T06:08:12.581037Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def slice_by_corr(X, r_min=0):\n    # Create correlation matrix\n    corr_matrix = X.corr().abs()\n\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find features with correlation greater than r_min\n    return X[[column for column in upper.columns if any(upper[column] > r_min)]]\n\ndef variance_inflation_factor(X, exog_idx):\n    clf = LinearRegression(fit_intercept=True)\n    sub_X = np.delete(np.nan_to_num(X), exog_idx, axis=1)\n    sub_y = X[:, exog_idx][np.newaxis].T\n    sub_clf = clf.fit(sub_X, sub_y)\n    return 1 / (1 - r2_score(sub_y, sub_clf.predict(sub_X)))\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n\n    def __init__(self, thresh=10.0, nthreads=4, r_min=0, obs=250):\n        self.thresh = thresh\n        self.nthreads = nthreads\n        self.r_min = r_min\n        self.obs = obs\n        \n    def fit(self, X):\n        self.X = X\n        return self\n\n    def transform(self, X):\n        return ReduceVIF.calculate_vif(X, self.thresh, \n                                       self.nthreads, \n                                       self.r_min, \n                                       self.obs)\n\n    @staticmethod\n    def calculate_vif(X, thresh=10.0, nthreads=16, r_min=0, obs=250):        \n        dropped = True\n        vif_cols = []\n        X_vif_candidates = slice_by_corr(X, r_min)\n        X_vif_candidates = X_vif_candidates.sample(n=obs)\n        while dropped:\n            variables = X_vif_candidates.columns\n            dropped = False\n            with Parallel(n_jobs=nthreads, backend='threading') as parallel:\n                vif = parallel(\n                    delayed(variance_inflation_factor)(\n                        np.asarray(X_vif_candidates[variables].values),\n                        X_vif_candidates.columns.get_loc(var)) for var in \n                    X_vif_candidates.columns)\n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X_vif_candidates.columns[maxloc]} with vif={max_vif}')\n                vif_cols.append(X_vif_candidates.columns.tolist()[maxloc])\n                X_vif_candidates = X_vif_candidates.drop(\n                    [X_vif_candidates.columns.tolist()[maxloc]], axis=1)\n                dropped = True\n        \n        if len(vif_cols) > 0:\n            return X.drop(columns=vif_cols), vif_cols\n        else:\n            return X, vif_cols\n\n    \ndef preprocess_x_y(X, nodrop_columns=[],\n                   var_thr=0.80, remove_multi=True,\n                   standardize=True,\n                   std_dev=3, vif_thr=5, missingness_thr=0.50,\n                   zero_thr=0.50, nthreads=16):\n    from colorama import Fore, Style\n\n    # Replace all near-zero with zeros\n    # Drop excessively sparse columns with >zero_thr zeros\n    if zero_thr > 0:\n        X = X.apply(lambda x: np.where(np.abs(x) < 0.000001, 0, x))\n        X_tmp = X.T.loc[(X == 0).sum() < (float(zero_thr)) * X.shape[0]].T\n\n        if len(nodrop_columns) > 0:\n            X = pd.concat([X_tmp, X[[i for i in X.columns if i in\n                                     nodrop_columns and i not in\n                                     X_tmp.columns]]], axis=1)\n        else:\n            X = X_tmp\n        del X_tmp\n\n        if X.empty or len(X.columns) < 5:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (Zero Columns): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    # Remove columns with excessive missing values\n    X = X.dropna(thresh=len(X) * (1 - missingness_thr), axis=1)\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (missingness): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n    # Apply a simple imputer (note that this assumes extreme cases of\n    # missingness have already been addressed). The SimpleImputer is better\n    # for smaller datasets, whereas the IterativeImputer performs best on\n    # larger sets.\n\n    # from sklearn.experimental import enable_iterative_imputer\n    # from sklearn.impute import IterativeImputer\n    # imp = IterativeImputer(random_state=0, sample_posterior=True)\n    # X = pd.DataFrame(imp.fit_transform(X, y), columns=X.columns)\n    imp1 = SimpleImputer()\n    X = pd.DataFrame(imp1.fit_transform(X.astype('float32')),\n                     columns=X.columns)\n\n    # Standardize X\n    if standardize is True:\n        scaler = StandardScaler()\n        X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n    # Remove low-variance columns\n    sel = VarianceThreshold(threshold=(var_thr * (1 - var_thr)))\n    sel.fit(X)\n    if len(nodrop_columns) > 0:\n        good_var_cols = X.columns[np.concatenate(\n            [sel.get_support(indices=True), np.array([X.columns.get_loc(c)\n                                                      for c in\n                                                      nodrop_columns if\n                                                      c in X])])]\n    else:\n        good_var_cols = X.columns[sel.get_support(indices=True)]\n    low_var_cols = [i for i in X.columns if i not in list(good_var_cols)]\n    if len(low_var_cols) > 0:\n        print(f\"Dropping {low_var_cols} for low variance...\")\n    X = X[good_var_cols]\n\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (low-variance): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n    # Remove multicollinear columns\n    if remove_multi is True:\n        try:\n            rvif = ReduceVIF(thresh=vif_thr, nthreads=nthreads)\n            X = rvif.fit_transform(X)[0]\n            if X.empty or len(X.columns) < 5:\n                print(f\"\\n\\n{Fore.RED}Empty feature-space \"\n                      f\"(multicollinearity): \"\n                      f\"{X}{Style.RESET_ALL}\\n\\n\")\n                return X\n        except:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (multicollinearity): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    print(f\"\\nX: {X}\\n\")\n    print(f\"Features: {list(X.columns)}\\n\")\n    return X\n\n\nclass Razors(object):\n    \"\"\"\n    Razors is a callable refit option for `GridSearchCV` whose aim is to\n    balance model complexity and cross-validated score in the spirit of the\n    \"one standard error\" rule of Breiman et al. (1984), which showed that\n    the tuning hyperparameter associated with the best performing model may be\n    prone to overfit. To help mitigate this risk, we can instead instruct\n    gridsearch to refit the highest performing 'parsimonious' model, as defined\n    using simple statistical rules (e.g. standard error (`sigma`),\n    percentile (`eta`), or significance level (`alpha`)) to compare\n    distributions of model performance across folds. Importantly, this\n    strategy assumes that the grid of multiple cross-validated models\n    can be principly ordered from simplest to most complex with respect to some\n    target hyperparameter of interest. To use the razors suite, supply\n    the `simplify` function partial of the `Razors` class as a callable\n    directly to the `refit` argument of `GridSearchCV`.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`.\n    scoring : str\n        Refit scoring metric.\n    param : str\n        Parameter whose complexity will be optimized.\n    rule : str\n        Rule for balancing model complexity with performance.\n        Options are 'se', 'percentile', and 'ranksum'. Default is 'se'.\n    sigma : int\n        Number of standard errors tolerance in the case that a standard error\n        threshold is used to filter outlying scores across folds. Required if\n        `rule`=='se'. Default is 1.\n    eta : float\n        Percentile tolerance in the case that a percentile threshold\n        is used to filter outlier scores across folds. Required if\n        `rule`=='percentile'. Default is 0.68.\n    alpha : float\n        An alpha significance level in the case that wilcoxon rank sum\n        hypothesis testing is used to filter outlying scores across folds.\n        Required if `rule`=='ranksum'. Default is 0.05.\n\n    References\n    ----------\n    Breiman, Friedman, Olshen, and Stone. (1984) Classification and Regression\n    Trees. Wadsworth.\n\n    Notes\n    -----\n    Here, 'simplest' is defined by the complexity of the model as influenced by\n    some user-defined target parameter (e.g. number of components, number of\n    estimators, polynomial degree, cost, scale, number hidden units, weight\n    decay, number of nearest neighbors, L1/L2 penalty, etc.).\n\n    The callable API accordingly assumes that the `params` attribute of\n    `cv_results_` 1) contains the indicated hyperparameter (`param`) of\n    interest, and 2) contains a sequence of values (numeric, boolean, or\n    categorical) that are ordered from least to most complex.\n    \"\"\"\n    __slots__ = ('cv_results', 'param', 'param_complexity', 'scoring',\n                 'rule', 'greater_is_better',\n                 '_scoring_funcs', '_scoring_dict',\n                 '_n_folds', '_splits', '_score_grid',\n                 '_cv_means', '_sigma', '_eta', '_alpha')\n\n    def __init__(\n            self,\n            cv_results_,\n            param,\n            scoring,\n            rule,\n            sigma=1,\n            eta=0.95,\n            alpha=0.01,\n    ):\n        import sklearn.metrics\n\n        self.cv_results = cv_results_\n        self.param = param\n        self.scoring = scoring\n        self.rule = rule\n        self._scoring_funcs = [\n            met\n            for met in sklearn.metrics.__all__\n            if (met.endswith(\"_score\")) or (met.endswith(\"_error\"))\n        ]\n        # Set _score metrics to True and _error metrics to False\n        self._scoring_dict = dict(\n            zip(\n                self._scoring_funcs,\n                [met.endswith(\"_score\") for met in self._scoring_funcs],\n            )\n        )\n        self.greater_is_better = self._check_scorer()\n        self._n_folds = len(list(set([i.split('_')[0] for i in\n                                     list(self.cv_results.keys()) if\n                                     i.startswith('split')])))\n        # Extract subgrid corresponding to the scoring metric of interest\n        self._splits = [i for i in list(self.cv_results.keys()) if\n                        i.endswith(f\"test_{self.scoring}\") and\n                        i.startswith('split')]\n        self._score_grid = np.vstack([self.cv_results[cv] for cv in\n                                      self._splits]).T\n        self._cv_means = np.array(np.nanmean(self._score_grid, axis=1))\n        self._sigma = sigma\n        self._eta = eta\n        self._alpha = alpha\n\n    def _check_scorer(self):\n        \"\"\"\n        Check whether the target refit scorer is negated. If so, adjust\n        greater_is_better accordingly.\n        \"\"\"\n\n        if (\n                self.scoring not in self._scoring_dict.keys()\n                and f\"{self.scoring}_score\" not in self._scoring_dict.keys()\n        ):\n            if self.scoring.startswith(\"neg_\"):\n                self.greater_is_better = True\n            else:\n                raise NotImplementedError(f\"Scoring metric {self.scoring} not \"\n                                          f\"recognized.\")\n        else:\n            self.greater_is_better = [\n                value for key, value in self._scoring_dict.items() if\n                self.scoring in key][0]\n        return self.greater_is_better\n\n    def _best_low_complexity(self):\n        \"\"\"\n        Balance model complexity with cross-validated score.\n\n        Return\n        ------\n        int\n            Index of a model that has the lowest complexity but its test score\n            is the highest on average across folds as compared to other models\n            that are equally likely to occur.\n        \"\"\"\n\n        # Check parameter(s) whose complexity we seek to restrict\n        if not any(self.param in x for x in\n                   self.cv_results[\"params\"][0].keys()):\n            raise KeyError(f\"Parameter {self.param} not found in cv grid.\")\n        else:\n            hyperparam = [\n                i for i in self.cv_results[\"params\"][0].keys() if\n                i.endswith(self.param)][0]\n\n        # Select low complexity threshold based on specified evaluation rule\n        if self.rule == \"se\":\n            if not self._sigma:\n                raise ValueError(\n                    \"For `se` rule, the tolerance \"\n                    \"(i.e. `_sigma`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_standard_error()\n        elif self.rule == \"percentile\":\n            if not self._eta:\n                raise ValueError(\n                    \"For `percentile` rule, the tolerance \"\n                    \"(i.e. `_eta`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_percentile()\n        elif self.rule == \"ranksum\":\n            if not self._alpha:\n                raise ValueError(\n                    \"For `ranksum` rule, the alpha-level \"\n                    \"(i.e. `_alpha`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_rank_sum_test()\n        else:\n            raise NotImplementedError(f\"{self.rule} is not a valid \"\n                                      f\"rule of RazorCV.\")\n\n        self.cv_results[f\"param_{hyperparam}\"].mask = np.where(\n            (self._cv_means >= float(l_cutoff)) &\n            (self._cv_means <= float(h_cutoff)),\n            True, False)\n\n        if np.sum(self.cv_results[f\"param_{hyperparam}\"].mask) == 0:\n            print(f\"\\nLow: {l_cutoff}\")\n            print(f\"High: {h_cutoff}\")\n            print(f\"{self._cv_means}\")\n            print(f\"hyperparam: {hyperparam}\\n\")\n            raise ValueError(\"No valid grid columns remain within the \"\n                             \"boundaries of the specified razor\")\n\n        highest_surviving_rank = np.nanmin(\n            self.cv_results[f\"rank_test_{self.scoring}\"][\n                self.cv_results[f\"param_{hyperparam}\"].mask])\n\n        # print(f\"Highest surviving rank: {highest_surviving_rank}\\n\")\n\n        return np.flatnonzero(np.isin(\n            self.cv_results[f\"rank_test_{self.scoring}\"],\n            highest_surviving_rank))[0]\n\n    def call_standard_error(self):\n        \"\"\"\n        Returns the simplest model whose performance is within `sigma`\n        standard errors of the average highest performing model.\n        \"\"\"\n\n        # Estimate the standard error across folds for each column of the grid\n        cv_se = np.array(np.nanstd(self._score_grid, axis=1) /\n                         np.sqrt(self._n_folds))\n\n        # Determine confidence interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    def call_rank_sum_test(self):\n        \"\"\"\n        Returns the simplest model whose paired performance across folds is\n        insignificantly different from the average highest performing,\n        at a predefined `alpha` level of significance.\n        \"\"\"\n\n        from scipy.stats import wilcoxon\n        import itertools\n\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n\n        # Perform signed Wilcoxon rank sum test for each pair combination of\n        # columns against the best average score column\n        tests = [pair for pair in list(itertools.combinations(range(\n            self._score_grid.shape[0]), 2)) if best_score_idx in pair]\n\n        p_dict = {}\n        for i, test in enumerate(tests):\n            p_dict[i] = wilcoxon(self._score_grid[test[0], :],\n                                 self._score_grid[test[1], :])[1]\n\n        # Sort and prune away significant tests\n        p_dict = {k: v for k, v in sorted(p_dict.items(),\n                                          key=lambda item: item[1]) if\n                  v > self._alpha}\n\n        # Flatten list of tuples, remove best score index, and take the\n        # lowest and highest remaining bounds\n        tests = [j for j in list(set(list(sum([tests[i] for i in\n                                               list(p_dict.keys())],\n                                              ())))) if j != best_score_idx]\n        if self.greater_is_better:\n            h_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n        else:\n            h_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n\n        return l_cutoff, h_cutoff\n\n\n    def call_percentile(self):\n        \"\"\"\n        Returns the simplest model whose performance is within the `eta`\n        percentile of the average highest performing model.\n        \"\"\"\n\n        # Estimate the indicated percentile, and its inverse, across folds for\n        # each column of the grid\n        perc_cutoff = np.nanpercentile(self._score_grid,\n                                       [100 * self._eta,\n                                        100 - 100 * self._eta], axis=1)\n\n        # Determine bounds of the percentile interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    @staticmethod\n    def simplify(param, scoring, rule='se', sigma=1, eta=0.68, alpha=0.01):\n        \"\"\"\n        Callable to be run as `refit` argument of `GridsearchCV`.\n\n        Parameters\n        ----------\n        param : str\n            Parameter with the largest influence on model complexity.\n        scoring : str\n            Refit scoring metric.\n        sigma : int\n            Number of standard errors tolerance in the case that a standard\n            error threshold is used to filter outlying scores across folds.\n            Only applicable if `rule`=='se'. Default is 1.\n        eta : float\n            Acceptable percent tolerance in the case that a percentile\n            threshold is used. Only applicable if `rule`=='percentile'.\n            Default is 0.68.\n        alpha : float\n            Alpha-level to use for signed wilcoxon rank sum testing.\n            Only applicable if `rule`=='ranksum'. Default is 0.01.\n        \"\"\"\n        from functools import partial\n\n        def razor_pass(\n                cv_results_, param, scoring, rule, sigma, alpha, eta\n        ):\n            rcv = Razors(cv_results_, param, scoring, rule=rule,\n                         sigma=sigma, alpha=alpha, eta=eta)\n            return rcv._best_low_complexity()\n\n        return partial(\n            razor_pass,\n            param=param,\n            scoring=scoring,\n            rule=rule,\n            sigma=sigma,\n            alpha=alpha,\n            eta=eta,\n        )\n\ndef divide_df(df_all,train_len):\n    return df_all.loc[:train_len-1], df_all.loc[train_len:].drop('target',axis=1)\n\ndef concat_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T06:08:58.464844Z","iopub.execute_input":"2022-03-26T06:08:58.465107Z","iopub.status.idle":"2022-03-26T06:08:58.522542Z","shell.execute_reply.started":"2022-03-26T06:08:58.465079Z","shell.execute_reply":"2022-03-26T06:08:58.521151Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"#working_dir = '/kaggle/input/nlp-getting-started/'\nworking_dir = '/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/'\ndf_train_all = pd.read_csv(f\"{working_dir}train.csv\")\ndf_test_all = pd.read_csv(f\"{working_dir}test.csv\")\nprint(f\"Rows in train.csv = {len(df_train_all)}\")\nprint(f\"Rows in test.csv = {len(df_test_all)}\")\npd.set_option('display.max_colwidth', None)\ndf_train_all.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T06:08:12.646418Z","iopub.execute_input":"2022-03-26T06:08:12.647175Z","iopub.status.idle":"2022-03-26T06:08:12.747099Z","shell.execute_reply.started":"2022-03-26T06:08:12.647084Z","shell.execute_reply":"2022-03-26T06:08:12.746247Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Rows in train.csv = 7613\nRows in test.csv = 3263\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   id keyword location  \\\n0   1     NaN      NaN   \n1   4     NaN      NaN   \n2   5     NaN      NaN   \n3   6     NaN      NaN   \n4   7     NaN      NaN   \n\n                                                                                                                                    text  \\\n0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n1                                                                                                 Forest fire near La Ronge Sask. Canada   \n2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n3                                                                      13,000 people receive #wildfires evacuation orders in California    \n4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation orders in California</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df_train_pos = df_train_all[df_train_all.target == 1]\ndf_train_neg = df_train_all[df_train_all.target == 0]\nprint(f\"No. of positive training examples = {len(df_train_pos)}\")\nprint(f\"No. of negative training examples = {len(df_train_neg)}\")\ntrain_keywords_unique = df_train_all.keyword.unique()\nprint(f\"No. of unique keywords = {len(train_keywords_unique)}\")\ndf_train_notnull_keywords = df_train_all[~df_train_all.keyword.isnull()]\nprint(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T06:08:12.748464Z","iopub.execute_input":"2022-03-26T06:08:12.749654Z","iopub.status.idle":"2022-03-26T06:08:12.768430Z","shell.execute_reply.started":"2022-03-26T06:08:12.749590Z","shell.execute_reply":"2022-03-26T06:08:12.767833Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"No. of positive training examples = 3271\nNo. of negative training examples = 4342\nNo. of unique keywords = 222\nNo of train examples with keyword not null = 7552\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature Engineering\n## NLP libraries and functions","metadata":{}},{"cell_type":"code","source":"# import re\n# import nltk\n# import gensim\n# import sys\n# import spacy\n# import emoji\n# import warnings\n# import en_core_web_sm\n# from spacy import displacy\n# from spacy.tokenizer import Tokenizer\n# from tqdm import tqdm\n# import seaborn as sns\n# import string\n# from collections import defaultdict\n# from collections import Counter\n# from nltk.corpus import stopwords\n# from nltk.corpus import stopwords\n# from nltk.corpus import wordnet\n# from nltk.stem.snowball import SnowballStemmer\n# import gensim.corpora as corpora\n# from gensim.utils import simple_preprocess\n# from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n# from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n# from spacy.util import compile_infix_regex\n# from emoji import demojize\n# from bs4 import BeautifulSoup\n# from html import unescape\n# import torch\n# from transformers import AutoModel, AutoTokenizer \n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# unicode = str\n\n# special = string.punctuation \n# warnings.filterwarnings(\"ignore\")\n# nltk.download('stopwords')\n# nltk.download('wordnet')\n# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n# stemmer = SnowballStemmer(\"english\")\n# nlp = spacy.load('en_core_web_lg')\n\n# # Tweet symbols\n# # Retrieve the default token-matching regex pattern\n# re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n# # Add #hashtag pattern\n# re_token_match = f\"({re_token_match}|#\\\\w+)\"\n# nlp.tokenizer.token_match = re.compile(re_token_match).match\n\n# # Punctuations I want to remove, including the empty token\n# puncts = ['\\u200d','?', '....','..','...','', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\', \n#     '‚Ä¢', '~', '¬£', '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',  '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', \n#     '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù', '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ', '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', \n#     '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº', '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤', '√®', '¬∏', '¬æ', \n#     '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª', 'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò', \n#     '¬π', '‚â§', '‚Ä°', '‚àö', '!','üÖ∞','üÖ±']\n\n# disaster_words = ['injur',\n#  'crust',\n#  'battle',\n#  'terrorism',\n#  'impact',\n#  'bush%20fires',\n#  'inundated',\n#  'rogue',\n#  'hostages',\n#  'theft',\n#  'heat',\n#  'oil%20spill',\n#  'dust%20storm',\n#  'burned',\n#  'wreck',\n#  'siren',\n#  'rockfall',\n#  'boat',\n#  'nuclear',\n#  'infestation',\n#  'lightning',\n#  'burning%20buildings',\n#  'rubble',\n#  'life',\n#  'rioting',\n#  'killing',\n#  'traumatised',\n#  'emergency%20plan',\n#  'bombing',\n#  'bermuda',\n#  'thunderstorm',\n#  'drowning',\n#  'fatal',\n#  'harm',\n#  'outbreak',\n#  'collide',\n#  'disease',\n#  'catastrophe',\n#  'disaster',\n#  'structural%20failure',\n#  'riverine',\n#  'threat',\n#  'seiche',\n#  'apocalypse',\n#  'shooting',\n#  'hurricane',\n#  'obliterate',\n#  'weather',\n#  'fog',\n#  'buildings%20burning',\n#  'violent',\n#  'sea',\n#  'smoke',\n#  'drought',\n#  'cyclone',\n#  'blazing',\n#  'hazard',\n#  'pyroclastic',\n#  'deluge',\n#  'hijacking',\n#  'conditions',\n#  'blew%20up',\n#  'poverty',\n#  'armageddon',\n#  'ruin',\n#  'rape',\n#  'panicking',\n#  'flood',\n#  'sandstorm',\n#  'cold',\n#  'violent%20storm',\n#  'blown%20up',\n#  'panic',\n#  'destroy',\n#  'wrecked',\n#  'cloud',\n#  'volcano',\n#  'ocean',\n#  'deluged',\n#  'crushed',\n#  'bombed',\n#  'tragedy',\n#  'emergency',\n#  'ice',\n#  'snow',\n#  'damage',\n#  'curfew',\n#  'floods',\n#  'mass%20murderer',\n#  'demolish',\n#  'surge',\n#  'detonate',\n#  'hijack',\n#  'suicide%20bomb',\n#  'winter',\n#  'collapsed',\n#  'wild%20fires',\n#  'injuries',\n#  'blaze',\n#  'lava',\n#  'rescuers',\n#  'electrocute',\n#  'hail',\n#  'suicide%20bombing',\n#  'death',\n#  'richter',\n#  'crashed',\n#  'whirlwind',\n#  'weapons',\n#  'obliterated',\n#  'volcanic',\n#  'wind',\n#  'hot',\n#  'frost',\n#  'flooding',\n#  'sinkhole',\n#  'drown',\n#  'sea-level',\n#  'stretcher',\n#  'collided',\n#  'avalanche',\n#  'trauma',\n#  'grasshopper',\n#  'mudflow',\n#  'demolished',\n#  'weapon',\n#  'loud%20bang',\n#  'ambulance',\n#  'explosion',\n#  'flames',\n#  'collision',\n#  'dead',\n#  'severe',\n#  'spill',\n#  'chilly',\n#  'freeze',\n#  'robbery',\n#  'fire%20truck',\n#  'inundation',\n#  'suicide%20bomber',\n#  'rain',\n#  'dust',\n#  'wounded',\n#  'mayhem',\n#  'attacked',\n#  'epidemic',\n#  'locust',\n#  'radiation%20emergency',\n#  'annihilated',\n#  'survived',\n#  'body%20bag',\n#  'landslide',\n#  'hostage',\n#  'collapse',\n#  'pandemic',\n#  'storm',\n#  'evacuate',\n#  'injury',\n#  'desolation',\n#  'quarantined',\n#  'stuck',\n#  'danger',\n#  'dam',\n#  'meltdown',\n#  'screams',\n#  'annihilation',\n#  'obliteration',\n#  'refugees',\n#  'forest%20fires',\n#  'coastal',\n#  'brush',\n#  'arsonist',\n#  'terrorist',\n#  'nimbus',\n#  'monster',\n#  'casualty',\n#  'famine',\n#  'emergency%20services',\n#  'sirens',\n#  'derailment',\n#  'destruction',\n#  'body%20bags',\n#  'permafrost',\n#  'pandemonium',\n#  'massacre',\n#  'explode',\n#  'bioterror',\n#  'blight',\n#  'evacuated',\n#  'derailed',\n#  'thunder',\n#  'sink',\n#  'tremor',\n#  'police',\n#  'viral',\n#  'casualties',\n#  'rescue',\n#  'attack',\n#  'airplane%20accident',\n#  'war%20zone',\n#  'hijacker',\n#  'military',\n#  'drowned',\n#  'crash',\n#  'heat%20wave',\n#  'typhoon',\n#  'blow',\n#  'cliff',\n#  'burning',\n#  'shelter',\n#  'iceberg',\n#  'bloody',\n#  'ash',\n#  'engulfed',\n#  'tsunami',\n#  'whirlpool',\n#  'debris',\n#  'desolate',\n#  'destroyed',\n#  'airburst',\n#  'hellfire',\n#  'bush',\n#  'bacterial',\n#  'chemical%20emergency',\n#  'detonat',\n#  'detonation',\n#  'aftershock',\n#  'bleeding',\n#  'bioterrorism',\n#  'first%20responders',\n#  'earthquake',\n#  'razed',\n#  'mudslide',\n#  'glacial',\n#  'deaths',\n#  'pasture',\n#  'rescued',\n#  'cumulonimbus',\n#  'rainstorm',\n#  'virus',\n#  'quarantine',\n#  'blood',\n#  'bridge%20collapse',\n#  'nuclear%20disaster',\n#  'seismic',\n#  'forest',\n#  'crush',\n#  'eyewitness',\n#  'derail',\n#  'sinking',\n#  'flash',\n#  'snowstorm',\n#  'exploded',\n#  'demolition',\n#  'fungal',\n#  'wave',\n#  'buildings%20on%20fire',\n#  'wreckage',\n#  'trapped',\n#  'injured',\n#  'natural%20disaster',\n#  'lahar',\n#  'screaming',\n#  'wounds',\n#  'mountain',\n#  'geyser',\n#  'forest%20fire',\n#  '911',\n#  'prion',\n#  'erosion',\n#  'screamed',\n#  'bomb',\n#  'temperature',\n#  'epicentre',\n#  'windstorm',\n#  'blackout',\n#  'trouble',\n#  'fatality',\n#  'shockwave',\n#  'wildfire',\n#  'survive',\n#  'riot',\n#  'gale',\n#  'cliff%20fall',\n#  'nuclear%20reactor',\n#  'arson',\n#  'ablaze',\n#  'fear',\n#  'electrocuted',\n#  'twister',\n#  'body%20bagging',\n#  'displaced',\n#  'flattened',\n#  'army',\n#  'hailstorm',\n#  'gust',\n#  'fatalities',\n#  'upheaval',\n#  'sunk',\n#  'blizzard',\n#  'hazardous',\n#  'fire',\n#  'accident',\n#  'parasitic',\n#  'catastrophic',\n#  'stabbing',\n#  'devastation',\n#  'fault',\n#  'devastated',\n#  'tornado',\n#  'derecho',\n#  'geomagnetic',\n#  'mass%20murder',\n#  'magma',\n#  'evacuation',\n#  'sand',\n#  'survivors']\n\n# chat_words = {\n#     \"AFAIK\": \"As Far As I Know\",\n#     \"AFK\": \"Away From Keyboard\",\n#     \"ASAP\": \"As Soon As Possible\",\n#     \"ATK\": \"At The Keyboard\",\n#     \"ATM\": \"At The Moment\",\n#     \"A3\": \"Anytime, Anywhere, Anyplace\",\n#     \"BAK\": \"Back At Keyboard\",\n#     \"BBL\": \"Be Back Later\",\n#     \"BBS\": \"Be Back Soon\",\n#     \"BFN\": \"Bye For Now\",\n#     \"B4N\": \"Bye For Now\",\n#     \"BRB\": \"Be Right Back\",\n#     \"BRT\": \"Be Right There\",\n#     \"BTW\": \"By The Way\",\n#     \"B4\": \"Before\",\n#     \"B4N\": \"Bye For Now\",\n#     \"CU\": \"See You\",\n#     \"CUL8R\": \"See You Later\",\n#     \"CYA\": \"See You\",\n#     \"FAQ\": \"Frequently Asked Questions\",\n#     \"FC\": \"Fingers Crossed\",\n#     \"FWIW\": \"For What It's Worth\",\n#     \"FYI\": \"For Your Information\",\n#     \"GAL\": \"Get A Life\",\n#     \"GG\": \"Good Game\",\n#     \"GN\": \"Good Night\",\n#     \"GMTA\": \"Great Minds Think Alike\",\n#     \"GR8\": \"Great!\",\n#     \"G9\": \"Genius\",\n#     \"IC\": \"I See\",\n#     \"ICQ\": \"I Seek you (also a chat program)\",\n#     \"ILU\": \"ILU: I Love You\",\n#     \"IMHO\": \"In My Honest/Humble Opinion\",\n#     \"IMO\": \"In My Opinion\",\n#     \"IOW\": \"In Other Words\",\n#     \"IRL\": \"In Real Life\",\n#     \"KISS\": \"Keep It Simple, Stupid\",\n#     \"LDR\": \"Long Distance Relationship\",\n#     \"LMAO\": \"Laugh My A.. Off\",\n#     \"LOL\": \"Laughing Out Loud\",\n#     \"LTNS\": \"Long Time No See\",\n#     \"L8R\": \"Later\",\n#     \"MTE\": \"My Thoughts Exactly\",\n#     \"M8\": \"Mate\",\n#     \"NRN\": \"No Reply Necessary\",\n#     \"OIC\": \"Oh I See\",\n#     \"PITA\": \"Pain In The A..\",\n#     \"PRT\": \"Party\",\n#     \"PRW\": \"Parents Are Watching\",\n#     \"ROFL\": \"Rolling On The Floor Laughing\",\n#     \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n#     \"ROTFLMAO\": \"Rolling On The Floor Laughing My Ass Off\",\n#     \"SK8\": \"Skate\",\n#     \"STATS\": \"Your sex and age\",\n#     \"ASL\": \"Age, Sex, Location\",\n#     \"THX\": \"Thank You\",\n#     \"TTFN\": \"Ta-Ta For Now!\",\n#     \"TTYL\": \"Talk To You Later\",\n#     \"U\": \"You\",\n#     \"U2\": \"You Too\",\n#     \"U4E\": \"Yours For Ever\",\n#     \"WB\": \"Welcome Back\",\n#     \"WTF\": \"What The F...\",\n#     \"WTG\": \"Way To Go!\",\n#     \"WUF\": \"Where Are You From?\",\n#     \"W8\": \"Wait\",\n#     \"IMMA\": \"I am going to\",\n#     \"2NITE\": \"tonight\",\n#     \"DMED\": \"mesaged\",\n#     'DM': \"message\",\n#     \"SMH\": \"I am dissapointed\"\n# }\n\n# # Thanks to https://stackoverflow.com/a/43023503/3971619\n# contractions = {\n#     \"ain't\": \"are not\",\n#     \"aren't\": \"are not\",\n#     \"can't\": \"cannot\",\n#     \"can't've\": \"cannot have\",\n#     \"'cause\": \"because\",\n#     \"could've\": \"could have\",\n#     \"couldn't\": \"could not\",\n#     \"couldn't've\": \"could not have\",\n#     \"didn't\": \"did not\",\n#     \"doesn't\": \"does not\",\n#     \"don't\": \"do not\",\n#     \"hadn't\": \"had not\",\n#     \"hadn't've\": \"had not have\",\n#     \"hasn't\": \"has not\",\n#     \"haven't\": \"have not\",\n#     \"he'd\": \"he would\",\n#     \"he'd've\": \"he would have\",\n#     \"he'll\": \"he will\",\n#     \"he'll've\": \"he shall have / he will have\",\n#     \"he's\": \"he is\",\n#     \"how'd\": \"how did\",\n#     \"how'd'y\": \"how do you\",\n#     \"how'll\": \"how will\",\n#     \"how's\": \"how is\",\n#     \"i'd\": \"I would\",\n#     \"i'd've\": \"I would have\",\n#     \"i'll\": \"I will\",\n#     \"i'll've\": \"I will have\",\n#     \"i'm\": \"I am\",\n#     \"i've\": \"I have\",\n#     \"isn't\": \"is not\",\n#     \"it'd\": \"it would\",\n#     \"it'd've\": \"it would have\",\n#     \"it'll\": \"it will\",\n#     \"it'll've\": \"it will have\",\n#     \"it's\": \"it is\",\n#     \"let's\": \"let us\",\n#     \"ma'am\": \"madam\",\n#     \"mayn't\": \"may not\",\n#     \"might've\": \"might have\",\n#     \"mightn't\": \"might not\",\n#     \"mightn't've\": \"might not have\",\n#     \"must've\": \"must have\",\n#     \"mustn't\": \"must not\",\n#     \"mustn't've\": \"must not have\",\n#     \"needn't\": \"need not\",\n#     \"needn't've\": \"need not have\",\n#     \"o'clock\": \"of the clock\",\n#     \"oughtn't\": \"ought not\",\n#     \"oughtn't've\": \"ought not have\",\n#     \"shan't\": \"shall not\",\n#     \"sha'n't\": \"shall not\",\n#     \"shan't've\": \"shall not have\",\n#     \"she'd\": \"she would\",\n#     \"she'd've\": \"she would have\",\n#     \"she'll\": \"she will\",\n#     \"she'll've\": \"she will have\",\n#     \"she's\": \"she is\",\n#     \"should've\": \"should have\",\n#     \"shouldn't\": \"should not\",\n#     \"shouldn't've\": \"should not have\",\n#     \"so've\": \"so have\",\n#     \"so's\": \"so is\",\n#     \"that'd\": \"that had\",\n#     \"that'd've\": \"that would have\",\n#     \"that's\": \"that is\",\n#     \"there'd\": \"there would\",\n#     \"there'd've\": \"there would have\",\n#     \"there's\": \"there is\",\n#     \"they'd\": \"they would\",\n#     \"they'd've\": \"they would have\",\n#     \"they'll\": \"they will\",\n#     \"they're\": \"they are\",\n#     \"they've\": \"they have\",\n#     \"to've\": \"to have\",\n#     \"wasn't\": \"was not\",\n#     \"we'd\": \"we would\",\n#     \"we'd've\": \"we would have\",\n#     \"we'll\": \"we will\",\n#     \"we'll've\": \"we will have\",\n#     \"we're\": \"we are\",\n#     \"we've\": \"we have\",\n#     \"weren't\": \"were not\",\n#     \"what'll\": \"what will\",\n#     \"what're\": \"what are\",\n#     \"what's\": \"what is\",\n#     \"what've\": \"what have\",\n#     \"when's\": \"when is\",\n#     \"when've\": \"when have\",\n#     \"where'd\": \"where did\",\n#     \"where's\": \"where is\",\n#     \"where've\": \"where have\",\n#     \"who'll\": \"who will\",\n#     \"who's\": \"who is\",\n#     \"who've\": \"who have\",\n#     \"why's\": \"why is\",\n#     \"why've\": \"why have\",\n#     \"will've\": \"will have\",\n#     \"won't\": \"will not\",\n#     \"won't've\": \"will not have\",\n#     \"would've\": \"would have\",\n#     \"wouldn't\": \"would not\",\n#     \"wouldn't've\": \"would not have\",\n#     \"y'all\": \"you all\",\n#     \"y'all'd\": \"you all would\",\n#     \"y'all'd've\": \"you all would have\",\n#     \"y'all're\": \"you all are\",\n#     \"y'all've\": \"you all have\",\n#     \"you'll\": \"you will\",\n#     \"you're\": \"you are\",\n#     \"you've\": \"you have\",\n# }\n\n# EMOTICONS = {\n#     u\":‚Äë\\)\":\"Happy face or smiley\",\n#     u\":\\)\":\"Happy face or smiley\",\n#     u\":-\\]\":\"Happy face or smiley\",\n#     u\":\\]\":\"Happy face or smiley\",\n#     u\":-3\":\"Happy face smiley\",\n#     u\":3\":\"Happy face smiley\",\n#     u\":->\":\"Happy face smiley\",\n#     u\":>\":\"Happy face smiley\",\n#     u\"8-\\)\":\"Happy face smiley\",\n#     u\":o\\)\":\"Happy face smiley\",\n#     u\":-\\}\":\"Happy face smiley\",\n#     u\":\\}\":\"Happy face smiley\",\n#     u\":-\\)\":\"Happy face smiley\",\n#     u\":c\\)\":\"Happy face smiley\",\n#     u\":\\^\\)\":\"Happy face smiley\",\n#     u\"=\\]\":\"Happy face smiley\",\n#     u\"=\\)\":\"Happy face smiley\",\n#     u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n#     u\":D\":\"Laughing, big grin or laugh with glasses\",\n#     u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n#     u\"8D\":\"Laughing, big grin or laugh with glasses\",\n#     u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n#     u\"XD\":\"Laughing, big grin or laugh with glasses\",\n#     u\"=D\":\"Laughing, big grin or laugh with glasses\",\n#     u\"=3\":\"Laughing, big grin or laugh with glasses\",\n#     u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n#     u\":-\\)\\)\":\"Very happy\",\n#     u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n#     u\":-\\(\":\"Frown, sad, andry or pouting\",\n#     u\":\\(\":\"Frown, sad, andry or pouting\",\n#     u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n#     u\":c\":\"Frown, sad, andry or pouting\",\n#     u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n#     u\":<\":\"Frown, sad, andry or pouting\",\n#     u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n#     u\":\\[\":\"Frown, sad, andry or pouting\",\n#     u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n#     u\">:\\[\":\"Frown, sad, andry or pouting\",\n#     u\":\\{\":\"Frown, sad, andry or pouting\",\n#     u\":@\":\"Frown, sad, andry or pouting\",\n#     u\">:\\(\":\"Frown, sad, andry or pouting\",\n#     u\":'‚Äë\\(\":\"Crying\",\n#     u\":'\\(\":\"Crying\",\n#     u\":'‚Äë\\)\":\"Tears of happiness\",\n#     u\":'\\)\":\"Tears of happiness\",\n#     u\"D‚Äë':\":\"Horror\",\n#     u\"D:<\":\"Disgust\",\n#     u\"D:\":\"Sadness\",\n#     u\"D8\":\"Great dismay\",\n#     u\"D;\":\"Great dismay\",\n#     u\"D=\":\"Great dismay\",\n#     u\"DX\":\"Great dismay\",\n#     u\":‚ÄëO\":\"Surprise\",\n#     u\":O\":\"Surprise\",\n#     u\":‚Äëo\":\"Surprise\",\n#     u\":o\":\"Surprise\",\n#     u\":-0\":\"Shock\",\n#     u\"8‚Äë0\":\"Yawn\",\n#     u\">:O\":\"Yawn\",\n#     u\":-\\*\":\"Kiss\",\n#     u\":\\*\":\"Kiss\",\n#     u\":X\":\"Kiss\",\n#     u\";‚Äë\\)\":\"Wink or smirk\",\n#     u\";\\)\":\"Wink or smirk\",\n#     u\"\\*-\\)\":\"Wink or smirk\",\n#     u\"\\*\\)\":\"Wink or smirk\",\n#     u\";‚Äë\\]\":\"Wink or smirk\",\n#     u\";\\]\":\"Wink or smirk\",\n#     u\";\\^\\)\":\"Wink or smirk\",\n#     u\":‚Äë,\":\"Wink or smirk\",\n#     u\";D\":\"Wink or smirk\",\n#     u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n#     u\":‚Äë\\|\":\"Straight face\",\n#     u\":\\|\":\"Straight face\",\n#     u\":$\":\"Embarrassed or blushing\",\n#     u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n#     u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n#     u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n#     u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n#     u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n#     u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n#     u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n#     u\"O:\\)\":\"Angel, saint or innocent\",\n#     u\"0:‚Äë3\":\"Angel, saint or innocent\",\n#     u\"0:3\":\"Angel, saint or innocent\",\n#     u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n#     u\"0:\\)\":\"Angel, saint or innocent\",\n#     u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n#     u\"0;\\^\\)\":\"Angel, saint or innocent\",\n#     u\">:‚Äë\\)\":\"Evil or devilish\",\n#     u\">:\\)\":\"Evil or devilish\",\n#     u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n#     u\"\\}:\\)\":\"Evil or devilish\",\n#     u\"3:‚Äë\\)\":\"Evil or devilish\",\n#     u\"3:\\)\":\"Evil or devilish\",\n#     u\">;\\)\":\"Evil or devilish\",\n#     u\"\\|;‚Äë\\)\":\"Cool\",\n#     u\"\\|‚ÄëO\":\"Bored\",\n#     u\":‚ÄëJ\":\"Tongue-in-cheek\",\n#     u\"#‚Äë\\)\":\"Party all night\",\n#     u\"%‚Äë\\)\":\"Drunk or confused\",\n#     u\"%\\)\":\"Drunk or confused\",\n#     u\":-###..\":\"Being sick\",\n#     u\":###..\":\"Being sick\",\n#     u\"<:‚Äë\\|\":\"Dump\",\n#     u\"\\(>_<\\)\":\"Troubled\",\n#     u\"\\(>_<\\)>\":\"Troubled\",\n#     u\"\\(';'\\)\":\"Baby\",\n#     u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n#     u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n#     u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n#     u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n#     u\"\\(-_-\\)zzz\":\"Sleeping\",\n#     u\"\\(\\^_-\\)\":\"Wink\",\n#     u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n#     u\"\\(\\+o\\+\\)\":\"Confused\",\n#     u\"\\(o\\|o\\)\":\"Ultraman\",\n#     u\"\\^_\\^\":\"Joyful\",\n#     u\"\\(\\^_\\^\\)/\":\"Joyful\",\n#     u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n#     u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n#     u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n#     u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n#     u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n#     u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n#     u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n#     u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n#     u\"\\('_'\\)\":\"Sad or Crying\",\n#     u\"\\(/_;\\)\":\"Sad or Crying\",\n#     u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n#     u\"\\(;_;\":\"Sad of Crying\",\n#     u\"\\(;_:\\)\":\"Sad or Crying\",\n#     u\"\\(;O;\\)\":\"Sad or Crying\",\n#     u\"\\(:_;\\)\":\"Sad or Crying\",\n#     u\"\\(ToT\\)\":\"Sad or Crying\",\n#     u\";_;\":\"Sad or Crying\",\n#     u\";-;\":\"Sad or Crying\",\n#     u\";n;\":\"Sad or Crying\",\n#     u\";;\":\"Sad or Crying\",\n#     u\"Q\\.Q\":\"Sad or Crying\",\n#     u\"T\\.T\":\"Sad or Crying\",\n#     u\"QQ\":\"Sad or Crying\",\n#     u\"Q_Q\":\"Sad or Crying\",\n#     u\"\\(-\\.-\\)\":\"Shame\",\n#     u\"\\(-_-\\)\":\"Shame\",\n#     u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n#     u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n#     u\"\\(=_=\\)\":\"Tired\",\n#     u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n#     u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n#     u\"=_\\^=\t\":\"cat\",\n#     u\"\\(\\.\\.\\)\":\"Looking down\",\n#     u\"\\(\\._\\.\\)\":\"Looking down\",\n#     u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n#     u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n#     u\"\\(?_?\\)\":\"Confusion\",\n#     u\">\\^_\\^<\":\"Normal Laugh\",\n#     u\"<\\^!\\^>\":\"Normal Laugh\",\n#     u\"\\^/\\^\":\"Normal Laugh\",\n#     u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n#     u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n#     u\"\\(^\\^\\)\":\"Normal Laugh\",\n#     u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n#     u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n#     u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n#     u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n#     u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n#     u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n#     u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n#     u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n#     u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n#     u\"\\(;_;\\)/~~~\":\"Waving\",\n#     u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n#     u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n#     u\"\\(T_T\\)/~~~\":\"Waving\",\n#     u\"\\(ToT\\)/~~~\":\"Waving\",\n#     u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n#     u\"\\(\\*_\\*\\)\":\"Amazed\",\n#     u\"\\(\\*_\\*;\":\"Amazed\",\n#     u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n#     u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n#     u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n#     u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n#     u'\\(-\"-\\)':\"Worried\",\n#     u\"\\(„Éº„Éº;\\)\":\"Worried\",\n#     u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n#     u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n#     u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n#     u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n#     u\"\\(\\^O\\^\\)\":\"Happy\",\n#     u\"\\(\\^o\\^\\)\":\"Happy\",\n#     u\"\\)\\^o\\^\\(\":\"Happy\",\n#     u\":O o_O\":\"Surprised\",\n#     u\"o_0\":\"Surprised\",\n#     u\"o\\.O\":\"Surpised\",\n#     u\"\\(o\\.o\\)\":\"Surprised\",\n#     u\"oO\":\"Surprised\",\n#     u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n#     u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n# }\n \n# # function to print sentiments\n# # of the sentence.\n# def sentiment_scores(sentence):\n \n#     # Create a SentimentIntensityAnalyzer object.\n#     sid_obj = SentimentIntensityAnalyzer()\n \n#     # polarity_scores method of SentimentIntensityAnalyzer\n#     # object gives a sentiment dictionary.\n#     # which contains pos, neg, neu, and compound scores.\n#     sentiment_dict = sid_obj.polarity_scores(sentence)\n    \n#     return sentiment_dict['neg'], sentiment_dict['neu'], sentiment_dict['pos']\n        \n# # Initializing the lemmatizer\n# lemmatizer = nltk.stem.WordNetLemmatizer()\n\n# try:\n#     # nlp = spacy.load('en_core_web_sm')\n#     nlp = spacy.load('en_core_web_lg') \n#     language_detector = LanguageDetector()\n#     nlp.add_pipe(language_detector)\n# except BaseException:\n#     pass\n    \n# token_dict = {}\n\n# def normalizeToken(token):\n#     lowercased_token = token.lower()\n#     if token.startswith(\"@\"):\n#         return \"@USER\"\n#     elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n#         return \"HTTPURL\"\n#     elif len(token) == 1:\n#         return demojize(token)\n#     else:\n#         if token == \"‚Äô\":\n#             return \"'\"\n#         elif token == \"‚Ä¶\":\n#             return \"...\"\n#         else:\n#             return token\n\n# def seed_everything(seed):\n#     random.seed(seed)\n#     np.random.seed(seed)\n    \n# def List_of_words(df): \n#     words = [word for tweet in tqdm(df['text']) for word in tweet.split()]\n#     return words\n\n# def List_of_tweets(df):\n#     tweets = [tweet for tweet in tqdm(df['text']) ]\n#     return tweets\n\n# def mislabeled_tweets(df) : # function that returns mislabeled labeled tweets\n#     df = df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n#     df = df[df['target'] > 1]['target']\n#     return (df.index.tolist()) \n\n# # Correct mislabeled tweets\n# def correcting_labels(df) : \n#     df['target_relabeled'] = df['target'].copy() \n#     df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n#     df.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and don¬â√õ¬™t let your desires control you! #Afterlife', 'target_relabeled'] = 0\n#     df.loc[df['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n#     df.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n#     df.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n#     df.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n#     df.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n#     df.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n#     df.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n#     df.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"Hellfire! We don¬â√õ¬™t even want to think about it or mention it so let¬â√õ¬™s not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n#     df.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n#     df.loc[df['text'] == \"that horrible sinking feeling when you¬â√õ¬™ve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0\n#     df.drop('target', axis= 1, inplace=True) \n#     df.columns = ['id', 'keyword', 'location', 'text', 'target']\n#     return df\n\n# def location_binging(df):\n#     df['location'].replace({'United States':'USA',\n#                             'New York':'USA',\n#                               \"London\":'UK',\n#                               \"Los Angeles, CA\":'USA',\n#                               \"Washington, D.C.\":'USA',\n#                               \"California\":'USA',\n#                               \"Chicago, IL\":'USA',\n#                               \"Chicago\":'USA',\n#                               \"New York, NY\":'USA',\n#                               \"California, USA\":'USA',\n#                               \"FLorida\":'USA',\n#                               \"Nigeria\":'Africa',\n#                               \"Kenya\":'Africa',\n#                               \"Everywhere\":'Worldwide',\n#                               \"San Francisco\":'USA',\n#                               \"Florida\":'USA',\n#                               \"United Kingdom\":'UK',\n#                               \"Los Angeles\":'USA',\n#                               \"Toronto\":'Canada',\n#                               \"San Francisco, CA\":'USA',\n#                               \"NYC\":'USA',\n#                               \"Seattle\":'USA',\n#                               \"Earth\":'Worldwide',\n#                               \"Ireland\":'UK',\n#                               \"London, England\":'UK',\n#                               \"New York City\":'USA',\n#                               \"Texas\":'USA',\n#                               \"London, UK\":'UK',\n#                               \"Atlanta, GA\":'USA',\n#                               \"Mumbai\":\"India\"},inplace=True)\n#     return df\n\n# UNICODE_EMO = emoji.EMOJI_UNICODE_ENGLISH\n\n# emo_vals = [\"_\".join(i.replace(\",\",\"\").replace(\":\",\"\").split()) for i in UNICODE_EMO] + [\"_\".join(i.replace(\",\",\"\").replace(\":\",\"\").split()) for i in UNICODE_EMO]\n# emo_vals.sort(key=lambda el: (len(el), el), reverse=True)\n# disaster_words.sort(key=lambda el: (len(el), el), reverse=True)\n\n# def convert_emoticons(text):\n#     for emot in EMOTICONS:\n#         text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n#     return text\n\n# def convert_emojis(text):\n#     for emot in UNICODE_EMO:\n#         text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n#     return text\n\n# def clean_text(text):\n#     text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6¬ß\\\\\\¬£\\√Ç*_<>\"\"‚é´‚Ä¢{}Œì~]', ' ', str(text))\n    \n#     text = BeautifulSoup(unescape(text), 'lxml').text\n    \n#      # Remove html\n#     html = re.compile(r'<.*?>')\n#     text = html.sub(r'',text)\n    \n#     # Convert emoticon\n#     text = convert_emoticons(text)\n\n#     text = re.sub(r'[\\)\\(\\.\\,\\;\\\\\\?\\&\\%\\!\\+\\-]', '', text)\n#     if len(text.split(\"  \")) > 1000:\n#         text = \" \".join([\"\".join(w.split(\" \")) if len(w.split(' '))>1 else w for w in text.split(\"  \")])\n#     text = re.sub(r'\\s', ' ', text)\n#     text = re.sub(r\"([A-z])\\- ([A-z])\", r\"\\1\\2\", text)\n#     text = text.replace('\\'','')\n#     text = text.replace('. .', '.')\n#     text = text.replace('\\'','')\n#     text = re.sub(r\"\\s+\",\" \", text)\n    \n#     # Convert emoji\n#     text = convert_emojis(text)\n    \n#     # Remove any chunks of consecutive numbers\n#     number_strings = re.findall(r'\\d+[ \\t]\\d+', text)\n#     ind_num_strings = []\n#     for j in number_strings:\n#         x = [int(i) for i in j.split()]\n#         ind_num_strings.append(x)\n    \n#     flat_num_list = [item for sublist in ind_num_strings for item in sublist]\n    \n#     for i in flat_num_list:\n#         j=re.sub(r'\\d+','',str(i))\n#         text = text.replace(str(i),j)\n\n#     texter_filt = re.sub(r\"\\( \",\"\", text)\n#     texter_filt = re.sub(r\" \\)\",\"\", texter_filt)\n#     texter_filt = re.sub(r\"\\(\\)\",\"\", texter_filt)\n#     texter_filt = re.sub(r\"\\(\\s+\\)\",\"\", texter_filt)\n#     texter_filt = re.sub(r\" \\.\",\".\", texter_filt)\n#     texter_filt = re.sub(r\"\\\\.\",\".\", texter_filt)\n#     texter_filt = re.sub(r\" \\,\",\",\", texter_filt)\n#     texter_filt = re.sub(r\"\\\\,\",\",\", texter_filt)\n#     texter_filt = re.sub(r\"\\s+\\s+\",\" \", texter_filt)    \n#     texter_filt = re.sub(' +', ' ', texter_filt)\n#     return texter_filt.lower()\n\n# def word_count(df_all) : \n#     df_all['word_count'] = df_all['text'].apply(lambda x: len(str(x).split()))\n#     return df_all\n\n# def unique_word_count(df_all) : \n#     df_all['unique_word_count'] = df_all['text'].apply(lambda x: len(set(str(x).split())))  \n#     return df_all\n\n# def stop_word_count(df_all) : \n#     df_all['stop_word_count'] = df_all['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n#     return df_all\n\n# def url_count(df_all) : \n#     df_all['url_count'] = df_all['text'].apply(lambda x: ('http' in str(x).lower() or 'https' in str(x).lower() or 'www' in str(x).lower()))\n#     return df_all\n\n# def mean_word_length(df_all) : \n#     df_all['mean_word_length'] = df_all['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n#     return df_all\n\n# def char_count(df_all) : \n#     df_all['char_count'] = df_all['text'].apply(lambda x: len(str(x).lower()))\n#     return df_all\n\n# def punctuation_count(df_all) : \n#     df_all['punctuation_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#     return df_all\n\n# def hashtag_count(df_all) : \n#     df_all['hashtag_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n#     return df_all\n\n# def mention_count(df_all) : \n#     df_all['mention_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n#     return df_all\n\n# def emo_count(df_all) : \n#     df_all['emo_count'] = df_all['text'].apply(lambda x: any(c in str(x).lower() for c in emo_vals))\n#     return df_all\n\n# def disaster_count(df_all) : \n#     df_all['dis_count'] = df_all['text'].apply(lambda x: any(c in str(x).lower() for c in disaster_words))\n#     return df_all\n\n# def stemming(text) : \n#     stemmer = nltk.stem.PorterStemmer()\n#     return(\" \".join(stemmer.stem(word) for word in text.split()))\n\n# def flatten(l):\n#     \"\"\"\n#     Flatten list of lists.\n#     \"\"\"\n#     import collections\n\n#     for el in l:\n#         if isinstance(\n#                 el, collections.Iterable) and not isinstance(\n#                 el, (str, bytes)):\n#             for ell in flatten(el):\n#                 yield ell\n#         else:\n#             yield el\n\n# infixes = (\n#     LIST_ELLIPSES\n#     + LIST_ICONS\n#     + [\n#         r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n#         r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n#             al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n#         ),\n#         r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n#         # ‚úÖ Commented out regex that splits on hyphens between letters:\n#         # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n#         r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n#         r'''[-~]'''\n#     ]\n# )\n\n# def tokenize(sentence, infixes):\n#     all_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n#     prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n#     suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n#     infix_re = compile_infix_regex(infixes)\n    \n#     def customize_tokenizer(nlp):\n#         # Adds support to use `-` as the delimiter for tokenization\n#         return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n#                          suffix_search=suffix_re.search,\n#                          infix_finditer=infix_re.finditer,\n#                          token_match=None\n#                         )\n\n#     nlp.tokenizer = customize_tokenizer(nlp)\n#     return [token.text for token in sentence if not token.is_stop]    \n\n# def replace_from_dict(x, dic):\n#     replaced_counter = 0\n#     for item in dic.items():\n#         for i, e in enumerate(x):\n#             if e == item[0]:\n#                 replaced_counter += 1\n#                 del x[i]\n#                 for ix, token in enumerate(item[1].split()):\n#                     x.insert(i + ix,token)\n#     return x\n\n# def lemmatize_list(x):\n#     x = \" \".join(x)\n#     # Returning a list again\n#     return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]\n\n# def remove_from_list(x, stuff_to_remove) -> list:\n#     for item in stuff_to_remove:\n#         # Making sure to iterate through the entire token\n#         for i,token in enumerate(x):\n#             if item == token:\n#                 del x[i]\n#     return x\n\n# def get_wordnet_pos(word):\n#     tag = nltk.pos_tag([word])[0][1][0].upper()\n#     tag_dict = {\"J\": wordnet.ADJ,\n#                     \"N\": wordnet.NOUN,\n#                     \"V\": wordnet.VERB,\n#                     \"R\": wordnet.ADV}\n#     return tag_dict.get(tag, wordnet.NOUN)\n\n# def normalizeTweet(tweet):\n#     tokens = tweet.replace(\"‚Äô\", \"'\").replace(\"‚Ä¶\", \"...\").split(' ')\n#     normTweet = \" \".join([normalizeToken(token) for token in tokens])\n\n#     normTweet = (\n#         normTweet.replace(\"cannot \", \"can not \")\n#         .replace(\"n't \", \" n't \")\n#         .replace(\"n 't \", \" n't \")\n#         .replace(\"ca n't\", \"can't\")\n#         .replace(\"ai n't\", \"ain't\")\n#     )\n#     normTweet = (\n#         normTweet.replace(\"'m \", \" 'm \")\n#         .replace(\"'re \", \" 're \")\n#         .replace(\"'s \", \" 's \")\n#         .replace(\"'ll \", \" 'll \")\n#         .replace(\"'d \", \" 'd \")\n#         .replace(\"'ve \", \" 've \")\n#     )\n#     normTweet = (\n#         normTweet.replace(\" p . m .\", \"  p.m.\")\n#         .replace(\" p . m \", \" p.m \")\n#         .replace(\" a . m .\", \" a.m.\")\n#         .replace(\" a . m \", \" a.m \")\n#     )\n\n#     return \" \".join(normTweet.split())\n\n# def sent_to_topics(text):\n\n#     # text = ' '.join(list(train['text'].values))\n#     texter = nlp(text)\n#     sentences = list(texter.sents)\n\n#     def sent_to_words(sentences):\n#         for sentence in sentences:\n#             yield([word for word in tokenize(sentence, infixes)])\n    \n#     # Tokenize\n#     topics = list(flatten(list(sent_to_words(sentences))))\n    \n#     # Map contractions\n#     topics = replace_from_dict(topics, contractions)\n\n#     # Map chat_words\n#     topics = replace_from_dict(topics, chat_words)\n    \n#     # Lemmatize\n#     topics = lemmatize_list(topics)\n    \n#     # Remove punctuation\n#     topics = remove_from_list(topics, puncts)\n    \n#     return topics\n\n# def misspelled_correction(val):\n#     for x in val.split(): \n#         if x in miss_corr.keys(): \n#             val = val.replace(x, miss_corr[x]) \n#     return val","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.573967Z","iopub.execute_input":"2022-03-12T07:35:07.574265Z","iopub.status.idle":"2022-03-12T07:35:07.601391Z","shell.execute_reply.started":"2022-03-12T07:35:07.574241Z","shell.execute_reply":"2022-03-12T07:35:07.600787Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NLP Munging","metadata":{}},{"cell_type":"code","source":"# mislabeled_tweets(df_train_all)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.602508Z","iopub.execute_input":"2022-03-12T07:35:07.602698Z","iopub.status.idle":"2022-03-12T07:35:07.613754Z","shell.execute_reply.started":"2022-03-12T07:35:07.602673Z","shell.execute_reply":"2022-03-12T07:35:07.613067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = correcting_labels(df_train_all)\n# df_all = concat_df(df_train_all, df_test_all)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.614919Z","iopub.execute_input":"2022-03-12T07:35:07.615135Z","iopub.status.idle":"2022-03-12T07:35:07.622542Z","shell.execute_reply.started":"2022-03-12T07:35:07.61511Z","shell.execute_reply":"2022-03-12T07:35:07.62178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_all = punctuation_count(url_count(location_binging(df_all)))\n\n# misspell_data = pd.read_csv(\"../input/twitterdisasterclassoutput/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n# misspell_data.misspell = misspell_data.misspell.str.strip()\n# misspell_data.misspell = misspell_data.misspell.str.split(\" \")\n# misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\n# misspell_data.drop_duplicates(\"misspell\",inplace=True)\n# miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n\n# df_all['text'] = df_all['text'].apply(lambda x : clean_text(x))\n# df_all['text'] = df_all['text'].apply(lambda x : misspelled_correction(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.624233Z","iopub.execute_input":"2022-03-12T07:35:07.62534Z","iopub.status.idle":"2022-03-12T07:35:07.631979Z","shell.execute_reply.started":"2022-03-12T07:35:07.625307Z","shell.execute_reply":"2022-03-12T07:35:07.631508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_prep = df_all.copy()\n# df_all = emo_count(unique_word_count(mention_count(stop_word_count(hashtag_count(df_all)))))\n# df_all['text'] = df_all['text'].apply(lambda x : normalizeTweet(x))\n# df_all['text'] = df_all['text'].apply(lambda x : sent_to_topics(x))\n# df_all['text'] = df_all['text'].apply(lambda x : ' '.join(x).replace('# ', '#').replace('@ ', '@'))\n# df_all[['neg', 'neut', 'pos']] = pd.DataFrame(df_all['text'].apply(lambda x : sentiment_scores(x)).values.tolist(), index=df_all.index)\n# df_all = disaster_count(char_count(mean_word_length(word_count(df_all))))\n\n# df_all.to_csv('df_nlp.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all = pd.read_csv(f\"../input/twitterdisasterclassoutput/df_nlp.csv\")\ntrain, test = divide_df(df_all, df_train_all.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T06:09:08.047230Z","iopub.execute_input":"2022-03-26T06:09:08.047720Z","iopub.status.idle":"2022-03-26T06:09:08.092565Z","shell.execute_reply.started":"2022-03-26T06:09:08.047690Z","shell.execute_reply":"2022-03-26T06:09:08.091969Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Feature embedding","metadata":{}},{"cell_type":"markdown","source":"## Naive strategy","metadata":{}},{"cell_type":"code","source":"# train_tweet_vectors = None\n# test_tweet_vectors = None\n# with nlp.disable_pipes():\n#     train_tweet_vectors = np.array([nlp(str(row.text)).vector for id, row in pd.DataFrame(train[['id', 'text']]).reset_index(drop=True).iterrows()])\n#     test_tweet_vectors = np.array([nlp(str(row.text)).vector for id, row in pd.DataFrame(test[['id', 'text']]).reset_index(drop=True).iterrows()])","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.643063Z","iopub.execute_input":"2022-03-12T07:35:07.643593Z","iopub.status.idle":"2022-03-12T07:35:07.649007Z","shell.execute_reply.started":"2022-03-12T07:35:07.643554Z","shell.execute_reply":"2022-03-12T07:35:07.648558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Bert Tweet transformer ","metadata":{}},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n# bertweet = AutoModel.from_pretrained(\"vinai/bertweet-large\")\n\n# def embed_twitter_feats(tokenized):\n#     with torch.no_grad():\n#         last_hidden_states = bertweet(torch.tensor([tokenized]))\n#     return last_hidden_states[0][:,0,:].numpy()\n\n# train = train.dropna(subset=['text'])\n# test = test.dropna(subset=['text'])\n# train_tokenized = train['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, padding=True)).values.tolist()\n# test_tokenized = test['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, padding=True)).values.tolist()\n\n# train_tweet_vectors = []\n# for t in train_tokenized:\n#     train_tweet_vectors.append(embed_twitter_feats(t))\n# train_tweet_vectors = np.array(np.vstack(train_tweet_vectors))\n\n# test_tweet_vectors = []\n# for t in test_tokenized:\n#     test_tweet_vectors.append(embed_twitter_feats(t))\n# test_tweet_vectors = np.array(np.vstack(test_tweet_vectors))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.649763Z","iopub.execute_input":"2022-03-12T07:35:07.650054Z","iopub.status.idle":"2022-03-12T07:35:07.660074Z","shell.execute_reply.started":"2022-03-12T07:35:07.650032Z","shell.execute_reply":"2022-03-12T07:35:07.659546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom sentence_transformers import SentenceTransformer\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\nnlp = spacy.load('en_core_web_lg')\nmodel = SentenceTransformer('vinai/bertweet-base')\n\ntrain = train.dropna(subset=['text'])\ntest = test.dropna(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\ntrain['text'] = train['text'].apply(lambda x: model.encode(x).tolist())\ntest['text'] = test['text'].apply(lambda x: model.encode(x).tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install simpletransformers\n# import gc\n# import torch\n# from transformers import pipeline\n# from scipy.special import softmax\n# import sklearn\n# from simpletransformers.classification import ClassificationModel, ClassificationArgs\n# gc.collect()\n\n# # Create a ClassificationModel\n# model_args = ClassificationArgs(num_train_epochs=2, \n#                                 overwrite_output_dir=True)\n# model_args.manual_seed = 42\n# model_args.normalization = True #this enables the built-in Bertweet custom tokenizer\n\n# model_args.reprocess_input_data = True\n# #odel_args.evaluate_during_training = True\n# #model_args.evaluate_during_training_verbose = True\n# model_args.train_batch_size = 80\n# model_args.eval_batch_size = 80\n# model_args.early_stopping_metric = \"mcc\"\n# model_args.early_stopping_metric_minimize = False\n# model_args.use_early_stopping = True\n# model_args.early_stopping_consider_epochs = True\n# model_args.early_stopping_patience = 1\n\n# model = ClassificationModel(model_type='bertweet', \n#                             model_name='vinai/bertweet-base', \n#                             args = model_args, \n#                             num_labels = 2,\n#                             use_cuda = False)\n\n# training_df = train[[\"text\", \"target\"]]\n# training_df.columns = [\"text\", \"labels\"]\n\n# shuffled_training = training_df.sample(frac=1.0).reset_index(drop=True)\n\n# model.train_model(shuffled_training,\n#                   acc=sklearn.metrics.accuracy_score, \n#                   f1=sklearn.metrics.f1_score)\n\n# result, model_outputs, wrong_predictions = model.eval_model(shuffled_training, \n#                                                             acc=sklearn.metrics.accuracy_score,\n#                                                             f1=sklearn.metrics.f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare target label vectors","metadata":{}},{"cell_type":"code","source":"other_columns = ['keyword', 'location', 'url_count', 'hashtag_count', 'punctuation_count', 'stop_word_count', 'word_count', 'unique_word_count', 'mean_word_length', 'char_count', 'mention_count', 'neg', 'pos', 'neut', 'emo_count', 'dis_count']\ntrain_other = train[other_columns].reset_index(drop=True)\nX_train_all = pd.concat([pd.DataFrame(train['text'].values.tolist()), train_other[other_columns]], axis=1)\ntest_other = test[other_columns].reset_index(drop=True)\nX_test_all = pd.concat([pd.DataFrame(test['text'].values.tolist()), test_other], axis=1)\nX_all = pd.concat([X_train_all, pd.DataFrame(df_train_all['target'].values, columns=['target'])], axis=1)\nX_all = X_all.dropna(thresh=80, axis=0)\nX_all = concat_df(X_all, X_test_all)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T07:35:07.661093Z","iopub.execute_input":"2022-03-12T07:35:07.661839Z","iopub.status.idle":"2022-03-12T07:35:07.668439Z","shell.execute_reply.started":"2022-03-12T07:35:07.661803Z","shell.execute_reply":"2022-03-12T07:35:07.667755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geopy\nimport pycountry\nimport math\nimport swifter \nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nfrom dateutil.parser import parse\n    \ngeolocator = Nominatim(user_agent=\"kaggle\")\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3)\n\ndef is_date(string, fuzzy=True):\n    \"\"\"\n    Return whether the string can be interpreted as a date.\n    \n    :param string: str, string to check for date\n    :param fuzzy: bool, ignore unknown tokens in string if True\n    \"\"\"\n    try:\n        parse(string, fuzzy=fuzzy)\n        return True\n    except:\n        return False\n\ndef get_location(region):\n    if region == '':\n        return \"no_country\"\n    try:\n        region = ' '.join([str(i) for i in region])\n        return geocode(region, language='en')[0].split(\",\")[-1].strip()\n    except:\n        return \"no_country\"\n\nX_all['extracted_location'] = X_all['location'].apply(lambda x: list(nlp(str(x)).ents) if len(list(nlp(str(x)).ents))>0 else '') #return na value if list is empty\nX_all['extracted_location'] = X_all['extracted_location'].apply(lambda x: [i for i in x if is_date(str(i[0])) is False] if x!='' else '') #return na value if list is empty\nX_all[\"country\"] = X_all['extracted_location'].swifter.progress_bar(enable=True).apply(get_location)\nnan_values = {\"keyword\": \"no_keyword\", \"country\": \"no_location\"}\n\nX_all[\"country\"] = X_all[\"country\"].fillna(value=nan_values, inplace=True)\nX_all['country'] = X_all['country'].astype('category')\nX_all = X_all.drop(columns=['extracted_location', 'location'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_all.to_csv('../input/twitterdisasterclassoutput/X_all_raw.csv', index=False)\n# X_all = pd.read_csv('../input/twitterdisasterclassoutput/X_all_raw.csv')\nX_all.to_csv('/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/X_all_raw.csv', index=False)\nX_all = pd.read_csv('/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/X_all_raw.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:30:56.369377Z","iopub.execute_input":"2022-03-12T10:30:56.37Z","iopub.status.idle":"2022-03-12T10:30:56.992678Z","shell.execute_reply.started":"2022-03-12T10:30:56.369965Z","shell.execute_reply":"2022-03-12T10:30:56.99204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_all = X_all.dropna(thresh=80, axis=0)\nX_train_all = X_all.dropna(subset=['target'], axis=0).drop(columns=['target'])\nX_test_all = X_all[X_all['target'].isna()].drop(columns=['target'])\ntrain_targets = X_all.dropna(subset=['target'], axis=0)['target'].values\nX_all = X_all.drop(columns=['target'])\nX_all[['emo_count', 'dis_count', 'url_count']] = X_all[['emo_count', 'dis_count', 'url_count']].astype('int')\n\nX_all = pd.concat([X_all, pd.get_dummies(X_all['keyword'])], axis=1).drop(columns=['keyword'])\n# X_all = X_all.drop(columns=['location', 'keyword'])\n# 'location' 'keyword'\n# encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n# country_encoded = encoder.fit_transform(X_all['country'].values.reshape(1, -1))\n# feature_names = encoder.get_feature_names_out(input_features=['country'])\n# country_encoded = pd.DataFrame(country_encoded, columns=feature_names)\n# X_all['country']=country_encoded","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:30:58.28535Z","iopub.execute_input":"2022-03-12T10:30:58.285668Z","iopub.status.idle":"2022-03-12T10:30:58.338425Z","shell.execute_reply.started":"2022-03-12T10:30:58.285635Z","shell.execute_reply":"2022-03-12T10:30:58.337494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"preprocess = FunctionTransformer(preprocess_x_y)\ncleaned_reg = preprocess.fit_transform(X=X_all[X_all.columns.values.tolist()[769:]])\n\npreprocess = FunctionTransformer(preprocess_x_y, kw_args={'remove_multi': False})\ncleaned_bert = preprocess.fit_transform(X=X_all[X_all.columns.values.tolist()[0:768]])\n        \ncleaned = pd.concat([cleaned_bert, cleaned_reg], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:00.887251Z","iopub.execute_input":"2022-03-12T10:31:00.887586Z","iopub.status.idle":"2022-03-12T10:31:40.503276Z","shell.execute_reply.started":"2022-03-12T10:31:00.887534Z","shell.execute_reply":"2022-03-12T10:31:40.498347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_clean = cleaned.head(X_train_all.shape[0])\nX_test_clean = cleaned.tail(X_test_all.shape[0])\n\nX_train_clean = X_train_clean.reset_index(drop=True)\nX_test_clean = X_test_clean.reset_index(drop=True)\n\n# X_train_clean.to_csv('/kaggle/working/df_train_preprocessed2.csv', index=False)\n# X_test_clean.to_csv('/kaggle/working/df_test_preprocessed2.csv', index=False)\n\n# X_train_clean.to_csv('/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/df_train_preprocessed2.csv', index=False)\n# X_test_clean.to_csv('/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/df_test_preprocessed2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:44.838578Z","iopub.execute_input":"2022-03-12T10:31:44.83892Z","iopub.status.idle":"2022-03-12T10:31:53.762672Z","shell.execute_reply.started":"2022-03-12T10:31:44.838872Z","shell.execute_reply":"2022-03-12T10:31:53.762103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seed=42\n# X_train_clean = pd.read_csv('/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/df_train_preprocessed2.csv')\n# X_test_clean = pd.read_csv('/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/df_test_preprocessed2.csv')\n\n# # X_train_clean = pd.read_csv('/kaggle/working/df_train_preprocessed2.csv')\n# # X_test_clean = pd.read_csv('/kaggle/working/df_test_preprocessed2.csv')\n\n# # deriv_features = ['url_count', 'punctuation_count', 'stop_word_count', 'word_count', 'unique_word_count', 'mean_word_length', 'char_count']\n# # X_train_deriv, X_test_deriv, y_train_deriv, y_test_deriv = train_test_split(X_train_clean[deriv_features], train_targets, random_state=seed)\n\n# # X_train, X_test, y_train, y_test = train_test_split(X_train_clean.drop(columns=deriv_features), train_targets, random_state=seed)\n\n# X_train, X_test, y_train, y_test = train_test_split(X_train_clean, train_targets, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:32:23.584824Z","iopub.execute_input":"2022-03-12T10:32:23.58541Z","iopub.status.idle":"2022-03-12T10:32:24.228177Z","shell.execute_reply.started":"2022-03-12T10:32:23.585374Z","shell.execute_reply":"2022-03-12T10:32:24.227536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nmodels = [\n            'SVC',\n            'EN'\n         ]\n                \nestimators = [\n        SVC(random_state=seed, kernel='rbf', class_weight='balanced', max_iter=10000), \n        LogisticRegression(penalty='elasticnet', fit_intercept=True,\n                                                  solver='saga',\n                                                  class_weight='auto',\n                                                  random_state=seed, max_iter=10000)\n]\n    \nparams = {\n            models[0]: {'C':[1.25, 1, 0.75], 'tol': [0.01, 0.005, 0.001]},\n            models[1]: {'l1_ratio': [0.75, 0.8, 0.85, 0.9], 'C':[1, 0.1, 0.05, 0.01], 'tol': [0.05, 0.01, 0.005]}\n         }\n    \ndef get_pca_range(X):\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components = 0.95)\n    pca.fit(X)\n    min_comps = pca.components_.shape[0]\n\n    pca = PCA(n_components = 0.99)\n    pca.fit(X)\n    max_comps = pca.components_.shape[0]\n    return np.arange(start=np.round(min_comps, -1), stop=np.round(max_comps, -1), step=10) ","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:03:18.378947Z","iopub.execute_input":"2022-03-13T08:03:18.379324Z","iopub.status.idle":"2022-03-13T08:03:19.525543Z","shell.execute_reply.started":"2022-03-13T08:03:18.379251Z","shell.execute_reply":"2022-03-13T08:03:19.524463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection with Grid Search ","metadata":{}},{"cell_type":"markdown","source":"## Take subset of data to avoid overcomputation","metadata":{}},{"cell_type":"code","source":"seed=42\nX_train = X_train.reset_index(drop=True)\ny_train = pd.DataFrame(y_train).reset_index(drop=True)\n\nX_train = X_train.head(750)\ny_train = y_train.head(750)\n\nX_test = X_test.reset_index(drop=True)\ny_test = pd.DataFrame(y_test).reset_index(drop=True)\n\nX_test = X_test.head(750)\ny_test = y_test.head(750)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:03:19.52669Z","iopub.status.idle":"2022-03-13T08:03:19.527888Z","shell.execute_reply.started":"2022-03-13T08:03:19.527578Z","shell.execute_reply":"2022-03-13T08:03:19.527613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nn_comps_range = list(get_pca_range(X_train))\nn_comps_range","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_factory = {}\n\ninner_scoring = \"f1\"\n\nfeature_select = FeatureUnion([('pca', decomposition.PCA(random_state=seed)), \n                               (\"anova\", SelectFwe(f_classif, alpha=0.01))])\n            \nfor name, estimator in zip(models, estimators):\n    print(name)\n    model_factory[name] = {}\n    \n    # Pipeline feature selection (PCA) with model fitting\n    pipe = Pipeline(\n        [\n             ('feature_select', feature_select),\n             (name, estimator),\n        ]\n    )\n    model_params = {}\n    for hyperparam in params[name].keys():\n        model_params[f\"{name}__{hyperparam}\"] = params[name][hyperparam]\n    model_params[f\"feature_select__pca__n_components\"] = n_comps_range\n    pipe_grid_cv = GridSearchCV(pipe, model_params, scoring=[inner_scoring], \n                       refit=Razors.simplify(param='feature_select__pca__n_components', \n                                             scoring=inner_scoring, rule=\"se\", sigma=1), \n                       cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=seed), n_jobs=-1)\n    pipe_grid_cv.fit(X_train, y_train.values.ravel())\n    model_factory[name]['oos_score'] = cross_val_score(pipe_grid_cv, X_test, y_test.values.ravel(), \n                                                       scoring='accuracy', \n                                                       cv=StratifiedKFold(n_splits=10, \n                                                                          shuffle=True, \n                                                                          random_state=seed + 1))\n    model_factory[name]['best_params'] = pipe_grid_cv.best_params_\n    model_factory[name]['best_estimator'] = pipe_grid_cv.best_estimator_\n\nleaderboard = {}\nfor mod in model_factory.keys():\n    leaderboard[mod] = np.mean(model_factory[mod]['oos_score'])\n\nbest_estimator_name = max(leaderboard, key=leaderboard.get)\n\nbest_estimator = model_factory[best_estimator_name]['best_estimator']\n\nmodel_factory\n\nbest_estimator.fit(X_train_clean, train_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outer_best = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n\n# scores = cross_val_score(best_estimator, X_train_clean, train_targets, scoring='accuracy', cv=outer_best, n_jobs=-1, error_score='raise')\n# scores\n# final_est = best_estimator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit best model to full training data","metadata":{}},{"cell_type":"code","source":"# from sklearn import ensemble\n# from sklearn.naive_bayes import GaussianNB\n# meta_clf = GaussianNB()\n\n# base_models = [('SVC', model_factory['SVC']['best_estimator']), ('EN', model_factory['EN']['best_estimator'])]\n\n# outer_stacked = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n\n# ec = ensemble.StackingClassifier(estimators=base_models, final_estimator=meta_clf, passthrough=False, cv=outer_stacked)\n                                               \n# ec.fit(X_train_clean, train_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores = cross_val_score(ec, X_train_clean, train_targets, scoring='accuracy', cv=outer_stacked, n_jobs=-1, error_score='raise')\n# scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# y_test_pred = final_est.predict(X_test_all)\n# np.save('../input/twitterdisasterclassoutput/y_predicted.npy', y_test_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred = np.load('../input/twitterdisasterclassoutput/y_predicted.npy')\ndf_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndf_submission[\"target\"] = y_test_pred.astype(int)\ndf_submission.to_csv('submission.csv',index=False)\ndf_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}