{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/derekpisner/twitterdisasterclassification?scriptVersionId=89215140\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import string\nimport statistics\nimport nltk\nimport spacy\nimport matplotlib\nimport os\nimport string\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom joblib import parallel_backend\nfrom sklearn import linear_model, decomposition\nfrom collections import OrderedDict\nfrom operator import itemgetter\nfrom sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.feature_selection import f_classif, VarianceThreshold, SelectFwe\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone, RegressorMixin\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.preprocessing import FunctionTransformer\n\ntry:\n    from sklearn.utils._testing import ignore_warnings\nexcept:\n    from sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:02:12.318615Z","iopub.execute_input":"2022-03-03T04:02:12.318977Z","iopub.status.idle":"2022-03-03T04:02:24.173032Z","shell.execute_reply.started":"2022-03-03T04:02:12.318866Z","shell.execute_reply":"2022-03-03T04:02:24.171932Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class ReduceVIF(BaseEstimator, TransformerMixin):\n\n    def __init__(self, thresh=10.0):\n        self.thresh = thresh\n\n    def fit(self, X, y=None):\n        self.X = X\n        self.y = y\n        return self\n\n    def transform(self, X):\n        return ReduceVIF.calculate_vif(X, self.thresh)\n\n    @staticmethod\n    def calculate_vif(X, thresh=10.0):\n        from statsmodels.stats.outliers_influence import \\\n            variance_inflation_factor\n        dropped = True\n        vif_cols = []\n        while dropped:\n            # Loop repeatedly until we find that all columns within our dataset\n            # have a VIF value less than the threshold\n            variables = X.columns\n            dropped = False\n            vif = []\n            new_vif = 0\n            for var in X.columns:\n                new_vif = variance_inflation_factor(X[variables].values,\n                                                    X.columns.get_loc(var))\n                vif.append(new_vif)\n                if np.isinf(new_vif):\n                    break\n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n                vif_cols.append(X.columns.tolist()[maxloc])\n                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n                dropped = True\n        return X, vif_cols\n\ndef preprocess_x_y(X, nuisance_cols=[], nodrop_columns=[],\n                   var_thr=0.80, remove_multi=True,\n                   standardize=True,\n                   std_dev=3, vif_thr=5, missingness_thr=0.50,\n                   zero_thr=0.50):\n    from colorama import Fore, Style\n\n    # Replace all near-zero with zeros\n    # Drop excessively sparse columns with >zero_thr zeros\n    if zero_thr > 0:\n        X = X.apply(lambda x: np.where(np.abs(x) < 0.000001, 0, x))\n        X_tmp = X.T.loc[(X == 0).sum() < (float(zero_thr)) * X.shape[0]].T\n\n        if len(nodrop_columns) > 0:\n            X = pd.concat([X_tmp, X[[i for i in X.columns if i in\n                                     nodrop_columns and i not in\n                                     X_tmp.columns]]], axis=1)\n        else:\n            X = X_tmp\n        del X_tmp\n\n        if X.empty or len(X.columns) < 5:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (Zero Columns): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    # Remove columns with excessive missing values\n    X = X.dropna(thresh=len(X) * (1 - missingness_thr), axis=1)\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (missingness): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n    # Apply a simple imputer (note that this assumes extreme cases of\n    # missingness have already been addressed). The SimpleImputer is better\n    # for smaller datasets, whereas the IterativeImputer performs best on\n    # larger sets.\n\n    # from sklearn.experimental import enable_iterative_imputer\n    # from sklearn.impute import IterativeImputer\n    # imp = IterativeImputer(random_state=0, sample_posterior=True)\n    # X = pd.DataFrame(imp.fit_transform(X, y), columns=X.columns)\n    imp1 = SimpleImputer()\n    X = pd.DataFrame(imp1.fit_transform(X.astype('float32')),\n                     columns=X.columns)\n\n    # Standardize X\n    if standardize is True:\n        scaler = StandardScaler()\n        X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n    # Remove low-variance columns\n    sel = VarianceThreshold(threshold=(var_thr * (1 - var_thr)))\n    sel.fit(X)\n    if len(nodrop_columns) > 0:\n        good_var_cols = X.columns[np.concatenate(\n            [sel.get_support(indices=True), np.array([X.columns.get_loc(c)\n                                                      for c in\n                                                      nodrop_columns if\n                                                      c in X])])]\n    else:\n        good_var_cols = X.columns[sel.get_support(indices=True)]\n\n    low_var_cols = [i for i in X.columns if i not in list(good_var_cols)]\n    if len(low_var_cols) > 0:\n        print(f\"Dropping {low_var_cols} for low variance...\")\n    X = X[good_var_cols]\n\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (low-variance): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n    # Remove multicollinear columns\n    if remove_multi is True:\n        try:\n            rvif = ReduceVIF(thresh=vif_thr)\n            X = rvif.fit_transform(X)[0]\n            if X.empty or len(X.columns) < 5:\n                print(f\"\\n\\n{Fore.RED}Empty feature-space \"\n                      f\"(multicollinearity): \"\n                      f\"{X}{Style.RESET_ALL}\\n\\n\")\n                return X\n        except:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (multicollinearity): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    print(f\"\\nX: {X}\\n\")\n    print(f\"Features: {list(X.columns)}\\n\")\n    return X\n\n\nclass Razors(object):\n    \"\"\"\n    Razors is a callable refit option for `GridSearchCV` whose aim is to\n    balance model complexity and cross-validated score in the spirit of the\n    \"one standard error\" rule of Breiman et al. (1984), which showed that\n    the tuning hyperparameter associated with the best performing model may be\n    prone to overfit. To help mitigate this risk, we can instead instruct\n    gridsearch to refit the highest performing 'parsimonious' model, as defined\n    using simple statistical rules (e.g. standard error (`sigma`),\n    percentile (`eta`), or significance level (`alpha`)) to compare\n    distributions of model performance across folds. Importantly, this\n    strategy assumes that the grid of multiple cross-validated models\n    can be principly ordered from simplest to most complex with respect to some\n    target hyperparameter of interest. To use the razors suite, supply\n    the `simplify` function partial of the `Razors` class as a callable\n    directly to the `refit` argument of `GridSearchCV`.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`.\n    scoring : str\n        Refit scoring metric.\n    param : str\n        Parameter whose complexity will be optimized.\n    rule : str\n        Rule for balancing model complexity with performance.\n        Options are 'se', 'percentile', and 'ranksum'. Default is 'se'.\n    sigma : int\n        Number of standard errors tolerance in the case that a standard error\n        threshold is used to filter outlying scores across folds. Required if\n        `rule`=='se'. Default is 1.\n    eta : float\n        Percentile tolerance in the case that a percentile threshold\n        is used to filter outlier scores across folds. Required if\n        `rule`=='percentile'. Default is 0.68.\n    alpha : float\n        An alpha significance level in the case that wilcoxon rank sum\n        hypothesis testing is used to filter outlying scores across folds.\n        Required if `rule`=='ranksum'. Default is 0.05.\n\n    References\n    ----------\n    Breiman, Friedman, Olshen, and Stone. (1984) Classification and Regression\n    Trees. Wadsworth.\n\n    Notes\n    -----\n    Here, 'simplest' is defined by the complexity of the model as influenced by\n    some user-defined target parameter (e.g. number of components, number of\n    estimators, polynomial degree, cost, scale, number hidden units, weight\n    decay, number of nearest neighbors, L1/L2 penalty, etc.).\n\n    The callable API accordingly assumes that the `params` attribute of\n    `cv_results_` 1) contains the indicated hyperparameter (`param`) of\n    interest, and 2) contains a sequence of values (numeric, boolean, or\n    categorical) that are ordered from least to most complex.\n    \"\"\"\n    __slots__ = ('cv_results', 'param', 'param_complexity', 'scoring',\n                 'rule', 'greater_is_better',\n                 '_scoring_funcs', '_scoring_dict',\n                 '_n_folds', '_splits', '_score_grid',\n                 '_cv_means', '_sigma', '_eta', '_alpha')\n\n    def __init__(\n            self,\n            cv_results_,\n            param,\n            scoring,\n            rule,\n            sigma=1,\n            eta=0.95,\n            alpha=0.01,\n    ):\n        import sklearn.metrics\n\n        self.cv_results = cv_results_\n        self.param = param\n        self.scoring = scoring\n        self.rule = rule\n        self._scoring_funcs = [\n            met\n            for met in sklearn.metrics.__all__\n            if (met.endswith(\"_score\")) or (met.endswith(\"_error\"))\n        ]\n        # Set _score metrics to True and _error metrics to False\n        self._scoring_dict = dict(\n            zip(\n                self._scoring_funcs,\n                [met.endswith(\"_score\") for met in self._scoring_funcs],\n            )\n        )\n        self.greater_is_better = self._check_scorer()\n        self._n_folds = len(list(set([i.split('_')[0] for i in\n                                     list(self.cv_results.keys()) if\n                                     i.startswith('split')])))\n        # Extract subgrid corresponding to the scoring metric of interest\n        self._splits = [i for i in list(self.cv_results.keys()) if\n                        i.endswith(f\"test_{self.scoring}\") and\n                        i.startswith('split')]\n        self._score_grid = np.vstack([self.cv_results[cv] for cv in\n                                      self._splits]).T\n        self._cv_means = np.array(np.nanmean(self._score_grid, axis=1))\n        self._sigma = sigma\n        self._eta = eta\n        self._alpha = alpha\n\n    def _check_scorer(self):\n        \"\"\"\n        Check whether the target refit scorer is negated. If so, adjust\n        greater_is_better accordingly.\n        \"\"\"\n\n        if (\n                self.scoring not in self._scoring_dict.keys()\n                and f\"{self.scoring}_score\" not in self._scoring_dict.keys()\n        ):\n            if self.scoring.startswith(\"neg_\"):\n                self.greater_is_better = True\n            else:\n                raise NotImplementedError(f\"Scoring metric {self.scoring} not \"\n                                          f\"recognized.\")\n        else:\n            self.greater_is_better = [\n                value for key, value in self._scoring_dict.items() if\n                self.scoring in key][0]\n        return self.greater_is_better\n\n    def _best_low_complexity(self):\n        \"\"\"\n        Balance model complexity with cross-validated score.\n\n        Return\n        ------\n        int\n            Index of a model that has the lowest complexity but its test score\n            is the highest on average across folds as compared to other models\n            that are equally likely to occur.\n        \"\"\"\n\n        # Check parameter(s) whose complexity we seek to restrict\n        if not any(self.param in x for x in\n                   self.cv_results[\"params\"][0].keys()):\n            raise KeyError(f\"Parameter {self.param} not found in cv grid.\")\n        else:\n            hyperparam = [\n                i for i in self.cv_results[\"params\"][0].keys() if\n                i.endswith(self.param)][0]\n\n        # Select low complexity threshold based on specified evaluation rule\n        if self.rule == \"se\":\n            if not self._sigma:\n                raise ValueError(\n                    \"For `se` rule, the tolerance \"\n                    \"(i.e. `_sigma`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_standard_error()\n        elif self.rule == \"percentile\":\n            if not self._eta:\n                raise ValueError(\n                    \"For `percentile` rule, the tolerance \"\n                    \"(i.e. `_eta`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_percentile()\n        elif self.rule == \"ranksum\":\n            if not self._alpha:\n                raise ValueError(\n                    \"For `ranksum` rule, the alpha-level \"\n                    \"(i.e. `_alpha`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_rank_sum_test()\n        else:\n            raise NotImplementedError(f\"{self.rule} is not a valid \"\n                                      f\"rule of RazorCV.\")\n\n        self.cv_results[f\"param_{hyperparam}\"].mask = np.where(\n            (self._cv_means >= float(l_cutoff)) &\n            (self._cv_means <= float(h_cutoff)),\n            True, False)\n\n        if np.sum(self.cv_results[f\"param_{hyperparam}\"].mask) == 0:\n            print(f\"\\nLow: {l_cutoff}\")\n            print(f\"High: {h_cutoff}\")\n            print(f\"{self._cv_means}\")\n            print(f\"hyperparam: {hyperparam}\\n\")\n            raise ValueError(\"No valid grid columns remain within the \"\n                             \"boundaries of the specified razor\")\n\n        highest_surviving_rank = np.nanmin(\n            self.cv_results[f\"rank_test_{self.scoring}\"][\n                self.cv_results[f\"param_{hyperparam}\"].mask])\n\n        # print(f\"Highest surviving rank: {highest_surviving_rank}\\n\")\n\n        return np.flatnonzero(np.isin(\n            self.cv_results[f\"rank_test_{self.scoring}\"],\n            highest_surviving_rank))[0]\n\n    def call_standard_error(self):\n        \"\"\"\n        Returns the simplest model whose performance is within `sigma`\n        standard errors of the average highest performing model.\n        \"\"\"\n\n        # Estimate the standard error across folds for each column of the grid\n        cv_se = np.array(np.nanstd(self._score_grid, axis=1) /\n                         np.sqrt(self._n_folds))\n\n        # Determine confidence interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    def call_rank_sum_test(self):\n        \"\"\"\n        Returns the simplest model whose paired performance across folds is\n        insignificantly different from the average highest performing,\n        at a predefined `alpha` level of significance.\n        \"\"\"\n\n        from scipy.stats import wilcoxon\n        import itertools\n\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n\n        # Perform signed Wilcoxon rank sum test for each pair combination of\n        # columns against the best average score column\n        tests = [pair for pair in list(itertools.combinations(range(\n            self._score_grid.shape[0]), 2)) if best_score_idx in pair]\n\n        p_dict = {}\n        for i, test in enumerate(tests):\n            p_dict[i] = wilcoxon(self._score_grid[test[0], :],\n                                 self._score_grid[test[1], :])[1]\n\n        # Sort and prune away significant tests\n        p_dict = {k: v for k, v in sorted(p_dict.items(),\n                                          key=lambda item: item[1]) if\n                  v > self._alpha}\n\n        # Flatten list of tuples, remove best score index, and take the\n        # lowest and highest remaining bounds\n        tests = [j for j in list(set(list(sum([tests[i] for i in\n                                               list(p_dict.keys())],\n                                              ())))) if j != best_score_idx]\n        if self.greater_is_better:\n            h_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n        else:\n            h_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n\n        return l_cutoff, h_cutoff\n\n\n    def call_percentile(self):\n        \"\"\"\n        Returns the simplest model whose performance is within the `eta`\n        percentile of the average highest performing model.\n        \"\"\"\n\n        # Estimate the indicated percentile, and its inverse, across folds for\n        # each column of the grid\n        perc_cutoff = np.nanpercentile(self._score_grid,\n                                       [100 * self._eta,\n                                        100 - 100 * self._eta], axis=1)\n\n        # Determine bounds of the percentile interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    @staticmethod\n    def simplify(param, scoring, rule='se', sigma=1, eta=0.68, alpha=0.01):\n        \"\"\"\n        Callable to be run as `refit` argument of `GridsearchCV`.\n\n        Parameters\n        ----------\n        param : str\n            Parameter with the largest influence on model complexity.\n        scoring : str\n            Refit scoring metric.\n        sigma : int\n            Number of standard errors tolerance in the case that a standard\n            error threshold is used to filter outlying scores across folds.\n            Only applicable if `rule`=='se'. Default is 1.\n        eta : float\n            Acceptable percent tolerance in the case that a percentile\n            threshold is used. Only applicable if `rule`=='percentile'.\n            Default is 0.68.\n        alpha : float\n            Alpha-level to use for signed wilcoxon rank sum testing.\n            Only applicable if `rule`=='ranksum'. Default is 0.01.\n        \"\"\"\n        from functools import partial\n\n        def razor_pass(\n                cv_results_, param, scoring, rule, sigma, alpha, eta\n        ):\n            rcv = Razors(cv_results_, param, scoring, rule=rule,\n                         sigma=sigma, alpha=alpha, eta=eta)\n            return rcv._best_low_complexity()\n\n        return partial(\n            razor_pass,\n            param=param,\n            scoring=scoring,\n            rule=rule,\n            sigma=sigma,\n            alpha=alpha,\n            eta=eta,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:02:24.176425Z","iopub.execute_input":"2022-03-03T04:02:24.176627Z","iopub.status.idle":"2022-03-03T04:02:24.225655Z","shell.execute_reply.started":"2022-03-03T04:02:24.176603Z","shell.execute_reply":"2022-03-03T04:02:24.225175Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"working_dir = '/kaggle/input/nlp-getting-started/'\n#working_dir = '/home/dpys/Documents/Kaggle_Competitions/nlp-getting-started/'\ndf_train = pd.read_csv(f\"{working_dir}train.csv\")\ndf_test = pd.read_csv(f\"{working_dir}test.csv\")\nprint(f\"Rows in train.csv = {len(df_train)}\")\nprint(f\"Rows in test.csv = {len(df_test)}\")\npd.set_option('display.max_colwidth', None)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:02:24.22658Z","iopub.execute_input":"2022-03-03T04:02:24.22681Z","iopub.status.idle":"2022-03-03T04:02:24.330871Z","shell.execute_reply.started":"2022-03-03T04:02:24.22679Z","shell.execute_reply":"2022-03-03T04:02:24.329856Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Rows in train.csv = 7613\nRows in test.csv = 3263\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id keyword location  \\\n0   1     NaN      NaN   \n1   4     NaN      NaN   \n2   5     NaN      NaN   \n3   6     NaN      NaN   \n4   7     NaN      NaN   \n\n                                                                                                                                    text  \\\n0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n1                                                                                                 Forest fire near La Ronge Sask. Canada   \n2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n3                                                                      13,000 people receive #wildfires evacuation orders in California    \n4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation orders in California</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df_train_pos = df_train[df_train.target == 1]\ndf_train_neg = df_train[df_train.target == 0]\nprint(f\"No. of positive training examples = {len(df_train_pos)}\")\nprint(f\"No. of negative training examples = {len(df_train_neg)}\")\ntrain_keywords_unique = df_train.keyword.unique()\nprint(f\"No. of unique keywords = {len(train_keywords_unique)}\")\ndf_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\nprint(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:02:29.389753Z","iopub.execute_input":"2022-03-03T04:02:29.39001Z","iopub.status.idle":"2022-03-03T04:02:29.411038Z","shell.execute_reply.started":"2022-03-03T04:02:29.389986Z","shell.execute_reply":"2022-03-03T04:02:29.40945Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"No. of positive training examples = 3271\nNo. of negative training examples = 4342\nNo. of unique keywords = 222\nNo of train examples with keyword not null = 7552\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature Engineering\n## NLP libraries and functions","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nimport gensim\nimport sys\nimport spacy\nimport emoji\nimport warnings\nimport en_core_web_sm\nfrom spacy import displacy\nfrom spacy.tokenizer import Tokenizer\nfrom tqdm import tqdm\nimport seaborn as sns\nimport string\nfrom collections import defaultdict\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem.snowball import SnowballStemmer\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\nfrom spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\nfrom spacy.util import compile_infix_regex\n\nunicode = str\n\nspecial = string.punctuation \nwarnings.filterwarnings(\"ignore\")\nnltk.download('stopwords')\nnltk.download('wordnet')\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nstemmer = SnowballStemmer(\"english\")\nnlp = spacy.load('en_core_web_lg')\n\n# Tweet symbols\n# Retrieve the default token-matching regex pattern\nre_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n# Add #hashtag pattern\nre_token_match = f\"({re_token_match}|#\\\\w+)\"\nnlp.tokenizer.token_match = re.compile(re_token_match).match\n\n# Punctuations I want to remove, including the empty token\npuncts = ['\\u200d','?', '....','..','...','','@','#', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\', \n    '‚Ä¢', '~', '¬£', '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',  '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', \n    '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù', '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ', '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', \n    '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº', '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤', '√®', '¬∏', '¬æ', \n    '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª', 'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò', \n    '¬π', '‚â§', '‚Ä°', '‚àö', '!','üÖ∞','üÖ±']\n\nEMOTICONS = {\n    u\":‚Äë\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'‚Äë\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'‚Äë\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D‚Äë':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":‚ÄëO\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":‚Äëo\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8‚Äë0\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";‚Äë\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";‚Äë\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":‚Äë,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‚Äë\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:‚Äë\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:‚Äë\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;‚Äë\\)\":\"Cool\",\n    u\"\\|‚ÄëO\":\"Bored\",\n    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n    u\"#‚Äë\\)\":\"Party all night\",\n    u\"%‚Äë\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:‚Äë\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^/\\^\":\"Normal Laugh\",\n    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n    u\"\\(;_;\\)/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n    u\"\\(T_T\\)/~~~\":\"Waving\",\n    u\"\\(ToT\\)/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n}\n\n# Found a dictionary of common contractions and colloquial language\ncontraction_colloq_dict = {\"btw\": \"by the way\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\"}\n\n# Initializing the lemmatizer\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ntry:\n    # nlp = spacy.load('en_core_web_sm')\n    nlp = spacy.load('en_core_web_lg') \n    language_detector = LanguageDetector()\n    nlp.add_pipe(language_detector)\nexcept BaseException:\n    pass\n    \ntoken_dict = {}\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    \ndef concat_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(df_all,train_len):\n    return df_all.loc[:train_len-1], df_all.loc[train_len:].drop('target',axis=1)\n\ndef List_of_words(df): \n    words = [word for tweet in tqdm(df['text']) for word in tweet.split()]\n    return words\n\ndef List_of_tweets(df):\n    tweets = [tweet for tweet in tqdm(df['text']) ]\n    return tweets\n\ndef mislabeled_tweets(df) : # function that returns mislabeled labeled tweets\n    df = df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n    df = df[df['target'] > 1]['target']\n    return (df.index.tolist()) \n\n# Correct mislabeled tweets\ndef correcting_labels(df) : \n    df['target_relabeled'] = df['target'].copy() \n    df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n    df.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and don¬â√õ¬™t let your desires control you! #Afterlife', 'target_relabeled'] = 0\n    df.loc[df['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n    df.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n    df.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n    df.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n    df.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n    df.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n    df.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n    df.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"Hellfire! We don¬â√õ¬™t even want to think about it or mention it so let¬â√õ¬™s not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n    df.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n    df.loc[df['text'] == \"that horrible sinking feeling when you¬â√õ¬™ve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0\n    df.drop('target', axis= 1, inplace=True) \n    df.columns = ['id', 'keyword', 'location', 'text', 'target']\n    return df\n\ndef location_binging(df):\n    df['location'].replace({'United States':'USA',\n                            'New York':'USA',\n                              \"London\":'UK',\n                              \"Los Angeles, CA\":'USA',\n                              \"Washington, D.C.\":'USA',\n                              \"California\":'USA',\n                              \"Chicago, IL\":'USA',\n                              \"Chicago\":'USA',\n                              \"New York, NY\":'USA',\n                              \"California, USA\":'USA',\n                              \"FLorida\":'USA',\n                              \"Nigeria\":'Africa',\n                              \"Kenya\":'Africa',\n                              \"Everywhere\":'Worldwide',\n                              \"San Francisco\":'USA',\n                              \"Florida\":'USA',\n                              \"United Kingdom\":'UK',\n                              \"Los Angeles\":'USA',\n                              \"Toronto\":'Canada',\n                              \"San Francisco, CA\":'USA',\n                              \"NYC\":'USA',\n                              \"Seattle\":'USA',\n                              \"Earth\":'Worldwide',\n                              \"Ireland\":'UK',\n                              \"London, England\":'UK',\n                              \"New York City\":'USA',\n                              \"Texas\":'USA',\n                              \"London, UK\":'UK',\n                              \"Atlanta, GA\":'USA',\n                              \"Mumbai\":\"India\"},inplace=True)\n    return df\n\ndef convert_emoticons(text):\n    for emot in EMOTICONS:\n        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n    return text\n\ndef convert_emojis(text):\n    UNICODE_EMO = emoji.EMOJI_UNICODE_ENGLISH\n    for emot in UNICODE_EMO:\n        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n    return text\n\ndef clean_text(text):\n    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6¬ß\\\\\\¬£\\√Ç*_<>\"\"‚é´‚Ä¢{}Œì~]', ' ', str(text))\n    text = re.sub(r'[^A-Za-z0-9\\\\n+^A-Za-z0-9\\)\\(\\.\\,\\;\\\\\\'\\/\\?\\&\\%\\@\\!\\+\\:\\-]', ' ', text)\n    if len(text.split(\"  \")) > 1000:\n        text = \" \".join([\"\".join(w.split(\" \")) if len(w.split(' '))>1 else w for w in text.split(\"  \")])\n    text = re.sub(r'\\s', ' ', text)\n    text = re.sub(r\"([A-z])\\- ([A-z])\", r\"\\1\\2\", text)\n    text = text.replace('\\'','')\n    text = text.replace('. .', '.')\n    text = text.replace('\\'','')\n    text = re.sub(r\"\\s+\",\" \", text)\n    \n    # Remove Emails\n    text = ''.join([re.sub('\\S*@\\S*\\s?', '', sent) for sent in text])\n    \n    # Remove URL's\n    text_nourl = re.sub(r'\\w+:\\/{2}[\\d+\\w-]+(\\.[\\d+\\w-]+)*(?:(?:\\/[^\\s+/]*))*', '', text)\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    text_nourl = url.sub(r'',text_nourl)\n    \n    # Remove html\n    html = re.compile(r'<.*?>')\n    text = html.sub(r'',text_nourl)\n\n    # Convert emoticons/emoji\n    text = convert_emoticons(text)\n    text = convert_emojis(text)\n\n    # Remove any chunks of consecutive numbers\n    number_strings = re.findall(r'\\d+[ \\t]\\d+', text)\n    ind_num_strings = []\n    for j in number_strings:\n        x = [int(i) for i in j.split()]\n        ind_num_strings.append(x)\n    \n    flat_num_list = [item for sublist in ind_num_strings for item in sublist]\n    \n    for i in flat_num_list:\n        j=re.sub(r'\\d+','',str(i))\n        text = text.replace(str(i),j)\n\n    texter_filt = re.sub(r\"\\( \",\"\", text)\n    texter_filt = re.sub(r\" \\)\",\"\", texter_filt)\n    texter_filt = re.sub(r\"\\(\\)\",\"\", texter_filt)\n    texter_filt = re.sub(r\"\\(\\s+\\)\",\"\", texter_filt)\n    texter_filt = re.sub(r\" \\.\",\".\", texter_filt)\n    texter_filt = re.sub(r\"\\\\.\",\".\", texter_filt)\n    texter_filt = re.sub(r\" \\,\",\",\", texter_filt)\n    texter_filt = re.sub(r\"\\\\,\",\",\", texter_filt)\n    texter_filt = re.sub(r\"\\s+\\s+\",\" \", texter_filt)    \n    texter_filt = re.sub(' +', ' ', texter_filt)\n    return texter_filt.lower()\n\ndef word_count(df_all) : \n    df_all['word_count'] = df_all['text'].apply(lambda x: len(str(x).split()))\n    return df_all\n\ndef unique_word_count(df_all) : \n    df_all['unique_word_count'] = df_all['text'].apply(lambda x: len(set(str(x).split())))  \n    return df_all\n\ndef stop_word_count(df_all) : \n    df_all['stop_word_count'] = df_all['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n    return df_all\n\ndef url_count(df_all) : \n    df_all['url_count'] = df_all['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n    return df_all\n\ndef mean_word_length(df_all) : \n    df_all['mean_word_length'] = df_all['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    return df_all\n\ndef char_count(df_all) : \n    df_all['char_count'] = df_all['text'].apply(lambda x: len(str(x)))\n    return df_all\n\ndef punctuation_count(df_all) : \n    df_all['punctuation_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    return df_all\n\ndef hashtag_count(df_all) : \n    df_all['hashtag_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n    return df_all\n\ndef mention_count(df_all) : \n    df_all['mention_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n    return df_all\n\ndef stemming(text) : \n    stemmer = nltk.stem.PorterStemmer()\n    return(\" \".join(stemmer.stem(word) for word in text.split()))\n\ndef flatten(l):\n    \"\"\"\n    Flatten list of lists.\n    \"\"\"\n    import collections\n\n    for el in l:\n        if isinstance(\n                el, collections.Iterable) and not isinstance(\n                el, (str, bytes)):\n            for ell in flatten(el):\n                yield ell\n        else:\n            yield el\n\ninfixes = (\n    LIST_ELLIPSES\n    + LIST_ICONS\n    + [\n        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n        ),\n        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n        # ‚úÖ Commented out regex that splits on hyphens between letters:\n        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n        r'''[-~]'''\n    ]\n)\n\ndef tokenize(sentence, infixes):\n    all_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n    infix_re = compile_infix_regex(infixes)\n    def customize_tokenizer(nlp):\n        # Adds support to use `-` as the delimiter for tokenization\n        return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n                         suffix_search=suffix_re.search,\n                         infix_finditer=infix_re.finditer,\n                         token_match=None\n                        )\n\n    nlp.tokenizer = customize_tokenizer(nlp)\n    return [token.text for token in sentence if not token.is_stop]    \n\ndef replace_from_dict(x, dic):\n    replaced_counter = 0\n    for item in dic.items():\n        for i, e in enumerate(x):\n            if e == item[0]:\n                replaced_counter += 1\n                del x[i]\n                for ix, token in enumerate(item[1].split()):\n                    x.insert(i + ix,token)\n    return x\n\ndef lemmatize_list(x):\n    x = \" \".join(x)\n    # Returning a list again\n    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]\n\ndef remove_from_list(x, stuff_to_remove) -> list:\n    for item in stuff_to_remove:\n        # Making sure to iterate through the entire token\n        for i,token in enumerate(x):\n            if item == token:\n                del x[i]\n    return x\n\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                    \"N\": wordnet.NOUN,\n                    \"V\": wordnet.VERB,\n                    \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef sent_to_topics(text):\n\n    # text = ' '.join(list(train['text'].values))\n    texter = nlp(text)\n    sentences = list(texter.sents)\n\n    def sent_to_words(sentences):\n        for sentence in sentences:\n            yield([word for word in gensim.utils.simple_preprocess(' '.join(tokenize(sentence, infixes)), deacc=True)])\n    \n    # Tokenize\n    topics = list(flatten(list(sent_to_words(sentences))))\n    \n    # Map contractions\n    topics = replace_from_dict(topics, contraction_colloq_dict)\n\n    # Lemmatize\n    topics = lemmatize_list(topics)\n    \n    # Remove punctuation\n    topics = remove_from_list(topics, puncts)\n    \n    return topics","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:02:34.408077Z","iopub.execute_input":"2022-03-03T04:02:34.408404Z","iopub.status.idle":"2022-03-03T04:02:41.531186Z","shell.execute_reply.started":"2022-03-03T04:02:34.408302Z","shell.execute_reply":"2022-03-03T04:02:41.529943Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## NLP Munging","metadata":{}},{"cell_type":"code","source":"# mislabeled_tweets(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T23:28:08.45167Z","iopub.execute_input":"2022-03-02T23:28:08.451997Z","iopub.status.idle":"2022-03-02T23:28:08.49336Z","shell.execute_reply.started":"2022-03-02T23:28:08.451953Z","shell.execute_reply":"2022-03-02T23:28:08.492362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = correcting_labels(df_train)\n# df_all = concat_df(df_train, df_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T00:58:31.088609Z","iopub.execute_input":"2022-03-03T00:58:31.088864Z","iopub.status.idle":"2022-03-03T00:58:31.125275Z","shell.execute_reply.started":"2022-03-03T00:58:31.088833Z","shell.execute_reply":"2022-03-03T00:58:31.124313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_all = stop_word_count(punctuation_count(hashtag_count(url_count(location_binging(df_all))))) \n\n# df_all['text'] = df_all['text'].apply(lambda x : clean_text(x))\n# df_all['text'] = df_all['text'].apply(lambda x : sent_to_topics(x))\n# df_all['text'] = df_all['text'].apply(lambda x : ' '.join(x))\n\n# df_all = mention_count(char_count(mean_word_length(unique_word_count(word_count(df_all)))))\n# df_all.to_csv('df_nlp.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_all = pd.read_csv(f\"../input/twitterdisasterclassoutput/df_nlp.csv\")\n# train, test = divide_df(df_all, df_train.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:02:42.350297Z","iopub.execute_input":"2022-03-03T04:02:42.351006Z","iopub.status.idle":"2022-03-03T04:02:42.419168Z","shell.execute_reply.started":"2022-03-03T04:02:42.350974Z","shell.execute_reply":"2022-03-03T04:02:42.418066Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Feature embedding","metadata":{}},{"cell_type":"code","source":"# train_tweet_vectors = None\n# test_tweet_vectors = None\n# with nlp.disable_pipes():\n#     train_tweet_vectors = np.array([nlp(str(row.text)).vector for id, row in pd.DataFrame(train[['id', 'text']]).reset_index(drop=True).iterrows()])\n#     test_tweet_vectors = np.array([nlp(str(row.text)).vector for id, row in pd.DataFrame(test[['id', 'text']]).reset_index(drop=True).iterrows()])","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:54:50.460288Z","iopub.execute_input":"2022-03-01T21:54:50.460659Z","iopub.status.idle":"2022-03-01T21:56:37.172886Z","shell.execute_reply.started":"2022-03-01T21:54:50.460619Z","shell.execute_reply":"2022-03-01T21:56:37.171568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare target label vectors","metadata":{}},{"cell_type":"code","source":"# train_targets = df_train[\"target\"].values\n# #print(train_tweet_vectors.mean(axis=0).shape, train_tweet_vectors.std(axis=0).shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:03:04.42434Z","iopub.execute_input":"2022-03-03T04:03:04.424746Z","iopub.status.idle":"2022-03-03T04:03:04.428724Z","shell.execute_reply.started":"2022-03-03T04:03:04.424716Z","shell.execute_reply":"2022-03-03T04:03:04.427925Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"# other_columns = ['url_count', 'hashtag_count', 'punctuation_count', 'stop_word_count', 'word_count', 'unique_word_count', 'mean_word_length', 'char_count', 'mention_count']\n# train_other = train[other_columns].reset_index(drop=True)\n# X_train=pd.concat([pd.DataFrame(train_tweet_vectors), train_other[other_columns]], axis=1)\n# test_other = test[other_columns].reset_index(drop=True)\n# X_test=pd.concat([pd.DataFrame(test_tweet_vectors), test_other], axis=1)\n# y_train=train_targets\n# X_all = concat_df(X_train, X_test)\n\n# preprocess = FunctionTransformer(preprocess_x_y)\n# cleaned = preprocess.fit_transform(X=X_all)\n\n# X_train_clean = cleaned.head(X_train.shape[0])\n# X_test_clean = cleaned.tail(X_test.shape[0])\n\n# X_train_clean = X_train_clean.reset_index(drop=True)\n# X_test_clean = X_test_clean.reset_index(drop=True)\n\n# X_train_clean.to_csv('/kaggle/working/df_train_preprocessed.csv', index=False)\n# X_test_clean.to_csv('/kaggle/working/df_test_preprocessed.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T23:18:16.958024Z","iopub.execute_input":"2022-03-01T23:18:16.95831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed=42\nX_train_clean = pd.read_csv('../input/twitterdisasterclassoutput/df_train_preprocessed.csv')\nX_test_clean = pd.read_csv('../input/twitterdisasterclassoutput/df_test_preprocessed.csv')\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_clean, train_targets, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T05:44:37.888656Z","iopub.execute_input":"2022-03-03T05:44:37.889622Z","iopub.status.idle":"2022-03-03T05:44:38.303244Z","shell.execute_reply.started":"2022-03-03T05:44:37.889559Z","shell.execute_reply":"2022-03-03T05:44:38.301906Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nmodels = [\n#             'GBC',\n            'SVC',\n            'EN'\n         ]\n                \nestimators = [\n#         GradientBoostingClassifier(max_features='sqrt', loss='deviance', criterion='friedman_mse', random_state=seed),\n        SVC(random_state=seed, kernel='rbf', class_weight='balanced'), \n        LogisticRegression(penalty='elasticnet', fit_intercept=True,\n                                                  solver='saga',\n                                                  class_weight='auto',\n                                                  random_state=seed,\n                                                  warm_start=True)\n]\n    \nparams = {\n#             models[0]:{'max_depth': [2, 3, 4], 'min_samples_leaf': [0.1, 0.2, 0.3], 'min_samples_split': [0.1, 0.2, 0.3], 'n_estimators': [500, 1000, 2000],  'learning_rate':[0.01, 0.05, 0.1, 0.15], 'subsample': [0.7, 0.8, 0.9]},\n            models[0]: {'C':[1, 0.5, 0.1], 'tol': [0.01, 0.001]},\n            models[1]: {'l1_ratio': [0.85, 0.9, 0.95], 'C':[1, 0.1], 'tol': [0.01, 0.001]}\n         }\n    \ndef get_pca_range(X):\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components = 0.95)\n    pca.fit(X)\n    min_comps = pca.components_.shape[0]\n\n    pca = PCA(n_components = 0.99)\n    pca.fit(X)\n    max_comps = pca.components_.shape[0]\n    return np.arange(start=np.round(min_comps, -1), stop=np.round(max_comps, -1), step=10) \n\nn_comps_range = list(get_pca_range(X_train_clean))\nn_comps_range","metadata":{"execution":{"iopub.status.busy":"2022-03-03T05:44:40.349222Z","iopub.execute_input":"2022-03-03T05:44:40.350481Z","iopub.status.idle":"2022-03-03T05:44:40.753949Z","shell.execute_reply.started":"2022-03-03T05:44:40.350423Z","shell.execute_reply":"2022-03-03T05:44:40.753394Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[200, 210, 220, 230, 240, 250]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Selection with Grid Search ","metadata":{}},{"cell_type":"markdown","source":"## Take subset of data to avoid overcomputation","metadata":{}},{"cell_type":"code","source":"X_train = X_train.reset_index(drop=True)\ny_train = pd.DataFrame(y_train).reset_index(drop=True)\n\nX_train = X_train.head(500)\ny_train = y_train.head(500)\n\nX_test = X_test.reset_index(drop=True)\ny_test = pd.DataFrame(y_test).reset_index(drop=True)\n\nX_test = X_test.head(500)\ny_test = y_test.head(500)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T05:44:43.698833Z","iopub.execute_input":"2022-03-03T05:44:43.69909Z","iopub.status.idle":"2022-03-03T05:44:43.709023Z","shell.execute_reply.started":"2022-03-03T05:44:43.699067Z","shell.execute_reply":"2022-03-03T05:44:43.708165Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model_factory = {}\n\ninner_scoring = \"f1\"\n\nfeature_select = FeatureUnion([('pca', decomposition.PCA(random_state=seed)), \n                               (\"anova\", SelectFwe(f_classif, alpha=0.01))])\n            \nfor name, estimator in zip(models, estimators):\n    print(name)\n    model_factory[name] = {}\n    \n    # Pipeline feature selection (PCA) with model fitting\n    pipe = Pipeline(\n        [\n             ('feature_select', feature_select),\n             (name, estimator),\n        ]\n    )\n    model_params = {}\n    for hyperparam in params[name].keys():\n        model_params[f\"{name}__{hyperparam}\"] = params[name][hyperparam]\n    model_params[f\"feature_select__pca__n_components\"] = n_comps_range\n    pipe_grid_cv = GridSearchCV(pipe, model_params, scoring=[inner_scoring], \n                       refit=Razors.simplify(param='feature_select__pca__n_components', \n                                             scoring=inner_scoring, rule=\"se\", sigma=1), \n                       cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=seed), n_jobs=-1)\n    pipe_grid_cv.fit(X_train, y_train.values.ravel())\n    model_factory[name]['oos_score'] = cross_val_score(pipe_grid_cv, X_test, y_test.values.ravel(), \n                                                       scoring='accuracy', \n                                                       cv=StratifiedKFold(n_splits=10, \n                                                                          shuffle=True, \n                                                                          random_state=seed + 1))\n    model_factory[name]['best_params'] = pipe_grid_cv.best_params_\n    model_factory[name]['best_estimator'] = pipe_grid_cv.best_estimator_\n    \nbest_estimator = max(model_factory,\n                     key=lambda v: model_factory[v]['oos_score'])\n\nbest_estimator.fit(X_train_clean, train_targets)\n\nfinal_est = best_estimator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit best model to full training data","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nmeta_clf = GaussianNB()\n\n# base_models = [('GBC', model_factory['GBC']['best_estimator']['GBC']), ('SVC', model_factory['SVC']['best_estimator']['SVC']), ('EN', model_factory['EN']['best_estimator']['EN'])]\n\nbase_models = [('SVC', model_factory['SVC']['best_estimator']['SVC']), ('EN', model_factory['EN']['best_estimator']['EN'])]\n\nouter_stacked = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed + 1)\n\nec = ensemble.StackingClassifier(estimators=base_models, final_estimator=meta_clf, passthrough=False, cv=outer_stacked)\n\n# Pipeline feature selection (PCA) with model fitting\npipe = Pipeline(\n    [\n         ('feature_select', model_factory['SVC']['best_estimator']['feature_select']),\n         ('vc', ec),\n    ]\n)\n\npipe.fit(X_train_clean, train_targets)\n\nfinal_est = pipe","metadata":{"execution":{"iopub.status.busy":"2022-03-03T07:12:56.73446Z","iopub.execute_input":"2022-03-03T07:12:56.734723Z","iopub.status.idle":"2022-03-03T07:19:29.487732Z","shell.execute_reply.started":"2022-03-03T07:12:56.734698Z","shell.execute_reply":"2022-03-03T07:19:29.486793Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"y_test_pred = final_est.predict(X_test_clean)\ndf_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndf_submission[\"target\"] = y_test_pred\ndf_submission.to_csv('/kaggle/working/submission.csv',index=False)\ndf_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}