{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport spacy\nimport nltk\nimport random\nimport itertools\nimport unicodedata\nimport datasets, transformers\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk.corpus import stopwords, wordnet\nfrom joblib import dump\nimport scipy as sp\nfrom scipy import stats\nfrom itertools import groupby\nfrom joblib import parallel_backend\nfrom sklearn import linear_model, decomposition\nfrom collections import OrderedDict\nfrom operator import itemgetter\nfrom sklearn import metrics\nfrom joblib import Parallel, delayed\nfrom transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.ensemble import RandomForestRegressor, IsolationForest\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, train_test_split, KFold\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone, RegressorMixin\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS, ALPHA, ALPHA_LOWER, ALPHA_UPPER\nfrom spacy.util import compile_infix_regex\n# from scispacy.abbreviation import AbbreviationDetector\nfrom spacy.pipeline import EntityRecognizer\n\ntry:\n    from sklearn.utils._testing import ignore_warnings\nexcept:\n    from sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n%env TOKENIZERS_PARALLELISM=true\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"a942c950-ef86-481e-9bdd-4fc8ac222320","_cell_guid":"62ad6f3d-b227-4aaf-93bb-ca4860556fd2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:00:59.826890Z","iopub.execute_input":"2022-06-20T04:00:59.827154Z","iopub.status.idle":"2022-06-20T04:00:59.852190Z","shell.execute_reply.started":"2022-06-20T04:00:59.827127Z","shell.execute_reply":"2022-06-20T04:00:59.851333Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=true\n","output_type":"stream"}]},{"cell_type":"code","source":"INPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T04:01:02.204250Z","iopub.execute_input":"2022-06-20T04:01:02.204721Z","iopub.status.idle":"2022-06-20T04:01:02.209702Z","shell.execute_reply.started":"2022-06-20T04:01:02.204685Z","shell.execute_reply":"2022-06-20T04:01:02.208483Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')\n# nlp.add_pipe(\"abbreviation_detector\")\nre_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)","metadata":{"_uuid":"561bcb8c-31c6-4e52-b925-97112600781b","_cell_guid":"147479a7-c7c8-472f-bfdd-2c560020546b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:05.261731Z","iopub.execute_input":"2022-06-20T04:01:05.261992Z","iopub.status.idle":"2022-06-20T04:01:10.433722Z","shell.execute_reply.started":"2022-06-20T04:01:05.261963Z","shell.execute_reply":"2022-06-20T04:01:10.432957Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('display.precision', 4)\n# pd.set_option('display.max_rows', 500)\n# pd.set_option('display.max_columns', 500)\n# pd.set_option('display.width', 1000)\n\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 42\nCUSTOM_BATCH = 24\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Stopwords and infixes\nADDITIONAL_STOPWORDS = ['one or more', 'a', 'needn', 'a', 'not', 'able', 'never', 'about', 'needn’t', 'accordance', 'now', 'often', 'above', 'no', 'according', 'of', 'mentioned', 'others', 'after', 'nor', 'all', 'on', 'accordingly', 'otherwise', 'again', 'not', 'also', 'onto', 'across', 'overall', 'against', 'now', 'an', 'or', 'along', 'rather', 'ain', 'o', 'and', 'other', 'already', 'remarkably', 'all', 'of', 'another', 'particularly', 'alternatively', 'significantly', 'am', 'off', 'are', 'preferably', 'always', 'simply', 'an', 'on', 'as', 'preferred', 'among', 'sometimes', 'and', 'once', 'at', 'present', 'and/or', 'specifically', 'any', 'only', 'be', 'provide', 'anything', 'straight', 'are', 'or', 'because', 'provided', 'anywhere', 'forward', 'aren', 'other', 'been', 'provides', 'better', 'substantially', 'aren’t', 'our', 'being', 'relatively', 'disclosure', 'thereafter', 'as', 'ours', 'by', 'respectively', 'due', 'therebetween', 'at', 'ourselves', 'claim', 'said', 'easily', 'therefor', 'be', 'out', 'comprises', 'comprising', 'should', 'easy', 'therefrom', 'because', 'over', 'since', 'e.g', 'therein', 'been', 'own', 'could', 'some', 'either', 'thereinto', 'before', 're', 'described', 'such', 'elsewhere', 'thereon', 'being', 's', 'desired', 'suitable', 'enough', 'therethrough', 'below', 'same', 'do', 'than', 'especially', 'therewith', 'between', 'shan', 'does', 'that', 'essentially', 'together', 'both', 'shan’t', 'each', 'the', 'et', 'al', 'toward', 'but', 'she', 'embodiment', 'their', 'etc', 'towards', 'by', 'she’s', 'fig', 'then', 'eventually', 'typical', 'can', 'should', 'figs', 'there', 'excellent', 'upon', 'couldn', 'should’ve', 'for', 'thereby', 'finally', 'via', 'couldn’t', 'shouldn', 'from', 'therefore', 'furthermore', 'vice', 'versa', 'd', 'shouldn’t', 'further', 'thereof', 'good', 'whatever', 'did', 'so', 'generally', 'thereto', 'hence', 'whereas', 'didn', 'some', 'had', 'these', 'he/she', 'whereat', 'didn’t', 'such', 'has', 'they', 'him/her', 'wherever', 'do', 't', 'have', 'this', 'his/her', 'whether', 'does', 'than', 'having', 'those', 'ie', 'whose', 'doesn', 'that', 'herein', 'thus', 'ii', 'within', 'doesn’t', 'that’ll', 'however', 'to', 'iii', 'without', 'doing', 'the', 'if', 'use', 'instead', 'yet', 'don', 'their', 'in', 'various', 'later', 'don’t', 'theirs', 'into', 'was', 'like', 'down', 'them', 'invention', 'were', 'little', 'during', 'themselves', 'is', 'what', 'many', 'each', 'there', 'it', 'when', 'may', 'few', 'these', 'its', 'where', 'meanwhile', 'for', 'they', 'means', 'whereby', 'might', 'from', 'this', 'wherein', 'moreover', 'further', 'those', 'which', 'much', 'had', 'through', 'while', 'must', 'hadn', 'to', 'who', 'hadn’t', 'too', 'will', 'has', 'under', 'with', 'hasn', 'until', 'Would', 'hasn’t', 'up', 'have', 've', 'haven', 'very', 'haven’t', 'was', 'having', 'wasn', 'he', 'wasn’t', 'her', 'we', 'here', 'were', 'hers', 'weren', 'herself', 'weren’t', 'him', 'what', 'himself', 'when', 'his', 'where', 'how', 'which', 'i', 'while', 'if', 'who', 'in', 'whom', 'into', 'why', 'is', 'will', 'isn', 'with', 'isn’t', 'won', 'it', 'won’t', 'it’s', 'wouldn', 'its', 'wouldn’t', 'itself', 'y', 'just', 'you', 'll', 'you’d', 'm', 'you’ll', 'ma', 'you’re', 'me', 'you’ve', 'mightn', 'your', 'mightn’t', 'yours', 'more', 'yourself', 'most', 'yourselves', 'mustn', 'mustn’t', 'my', 'myself']\n\npuncts = ['\\u200d','?', '....','..','...','', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\',\n    '•', '~', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█',\n    '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓',\n    '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾',\n    'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n    '¹', '≤', '‡', '√', '!','🅰','🅱']\n\ninfixes = (\n    LIST_ELLIPSES\n    + LIST_ICONS\n    + [\n        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n        ),\n        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n        # ✅ Commented out regex that splits on hyphens between letters:\n        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n        r'''[-~]'''\n    ]\n)\n\nstopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n\nprefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\nsuffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\ninfix_re = compile_infix_regex(infixes)\n\ndef customize_tokenizer(nlp):\n    # Adds support to use `-` as the delimiter for tokenization\n    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n                     suffix_search=suffix_re.search,\n                     infix_finditer=infix_re.finditer,\n                     token_match=None\n                    )\n\nnlp.tokenizer = customize_tokenizer(nlp)","metadata":{"_uuid":"8238d081-b92b-434a-8b85-4329fbcb278f","_cell_guid":"ba02c35f-90b7-4f80-905b-0391f96f2347","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:10.435279Z","iopub.execute_input":"2022-06-20T04:01:10.435544Z","iopub.status.idle":"2022-06-20T04:01:10.471678Z","shell.execute_reply.started":"2022-06-20T04:01:10.435508Z","shell.execute_reply":"2022-06-20T04:01:10.470910Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"table = {\n'A': 'Human Necessities',\n'B': 'Operations and Transport',\n'C': 'Chemistry and Metallurgy',\n'D': 'Textiles',\n'E': 'Fixed Constructions',\n'F': 'Mechanical Engineering',\n'G': 'Physics',\n'H': 'Electricity',\n'Y': 'Emerging Cross-Sectional Technologies'\n}","metadata":{"_uuid":"102bd6d4-c302-4fde-995f-decacdbce996","_cell_guid":"d97adb40-4f30-487b-aa22-c14a50be0f17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:11.758114Z","iopub.execute_input":"2022-06-20T04:01:11.758410Z","iopub.status.idle":"2022-06-20T04:01:11.763514Z","shell.execute_reply.started":"2022-06-20T04:01:11.758379Z","shell.execute_reply":"2022-06-20T04:01:11.761839Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def remove_from_list(x, stuff_to_remove) -> list:\n    for item in stuff_to_remove:\n        # Making sure to iterate through the entire token\n        for i,token in enumerate(x):\n            if item == token:\n                del x[i]\n    return x\n\ndef Remove_Duplicates(text_in):\n    return re.sub(r\"\\b(\\w+)(?:\\W\\1\\b)+\", r\"\\1\", text_in, flags=re.IGNORECASE)\n\n\ndef remove_consecutive_nums(text):\n    # Remove any chunks of consecutive numbers\n    number_strings = re.findall(r'\\d+[ \\t]\\d+', text)\n    ind_num_strings = []\n    for j in number_strings:\n        x = [int(i) for i in j.split()]\n        ind_num_strings.append(x)\n\n    flat_num_list = [item for sublist in ind_num_strings for item in sublist]\n\n    for i in flat_num_list:\n        j=re.sub(r'\\d+','',str(i))\n        text = text.replace(str(i),j)\n    return text\n\n\ndef basic_clean(text_list, infixes, stopwords):\n    \"\"\"\n    A simple function to clean up the data. All the words that\n    are not designated as a stop word is then lemmatized after\n    encoding and basic regex parsing are performed.\n    \"\"\"\n\n    text_list_clean = []\n    for text in text_list:\n        text = re.sub(r'[\\)\\(\\.\\,\\;\\\\\\?\\&\\%\\!\\+\\-]', '', re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6§\\\\\\£\\Â*_<>\"\"⎫•{}Γ~]', ' ', str(' '.join(re.split('\\s*-\\s*', text)))))\n        if len(text.split(\"  \")) > 1000:\n            text = \" \".join([\"\".join(w.split(\" \")) if len(w.split(' '))>1 else w for w in text.split(\"  \")])\n        text_list_clean.append([i for i in remove_from_list(re.sub('\\s+', ' ', re.sub('\\s\\s+', ' ', re.sub('\\s+\\s+', ' ', Remove_Duplicates(re.sub(r\"\\b(?=[mdclxvii])m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})([ii]x|[ii]v|v?[ii]{0,3})\\b\\.?\", '', (unicodedata.normalize('NFKD', re.sub(' +', ' ', re.sub(r\"\\s+\\s+\",\" \", re.sub(r\"\\\\,\",\",\", re.sub(r\" \\,\",\",\", re.sub(r\"\\\\.\",\".\", re.sub(r\" \\.\",\".\", re.sub(r\"\\(\\s+\\)\",\"\", re.sub(r\"\\(\\)\",\"\", re.sub(r\" \\)\",\"\", re.sub(r\"\\( \",\"\", remove_consecutive_nums(re.sub(r\"\\s+\",\" \", re.sub(r\"([A-z])\\- ([A-z])\", r\"\\1\\2\", re.sub(r'\\s', ' ', text)).replace('\\'','').replace('. .', '.').replace('\\'',''))))))))))))).lower())\n        .encode('ascii', 'ignore')\n        .decode('utf-8', 'ignore')\n        .lower())))))).split(), puncts) if not i.isdigit() or i in stopwords])\n        del text\n\n    return '. '.join(x.strip().capitalize() for x in '. '.join(' '.join([word for word in sent]) for sent in text_list_clean).split('.')) + '.'\n\n\ndef get_cpc_texts():\n    \"\"\"\n    Function taken from Y Nakama's notebook:\n    https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\n    \"\"\"\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results","metadata":{"_uuid":"d75199e1-2d90-4925-8857-fc15aa97eee9","_cell_guid":"4047b6a7-5604-4be7-ab3c-b02d7ea18512","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:14.220251Z","iopub.execute_input":"2022-06-20T04:01:14.220696Z","iopub.status.idle":"2022-06-20T04:01:14.244125Z","shell.execute_reply.started":"2022-06-20T04:01:14.220659Z","shell.execute_reply":"2022-06-20T04:01:14.243438Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ntest = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')","metadata":{"_uuid":"f3ffefe0-201e-4750-b043-c76da4bab192","_cell_guid":"4ec8f13b-391a-48c7-a86c-c35526506b16","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:17.571894Z","iopub.execute_input":"2022-06-20T04:01:17.572177Z","iopub.status.idle":"2022-06-20T04:01:17.666633Z","shell.execute_reply.started":"2022-06-20T04:01:17.572146Z","shell.execute_reply":"2022-06-20T04:01:17.665863Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train['general_context'] = train['context'].apply(lambda x: table[x[0].upper()])\ntest['general_context'] = test['context'].apply(lambda x: table[x[0].upper()])\n\ntrain = pd.concat([train, pd.get_dummies(train['general_context'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['general_context'])], axis=1)\n\ncpc_texts = torch.load(f\"../input/cpc-texts/cpc_texts.pth\")\ntrain['context_text'] = train['context'].map(cpc_texts)\ntest['context_text'] = test['context'].map(cpc_texts)\n\ntrain['section'] = train['context'].astype(str).str[0]\ntrain['classes'] = train['context'].astype(str).str[1:]\ntest['section'] = test['context'].astype(str).str[0]\ntest['classes'] = test['context'].astype(str).str[1:]\n\ntrain['anchor_len'] = train['anchor'].str.split().str.len()\ntrain['target_len'] = train['target'].str.split().str.len()\n\ntest['anchor_len'] = test['anchor'].str.split().str.len()\ntest['target_len'] = test['target'].str.split().str.len()\n\ntrain['num_anchor_stops'] = test['anchor'].str.count('|'.join(stopwords))\ntest['num_anchor_stops'] = test['anchor'].str.count('|'.join(stopwords))\ntrain['num_target_stops'] = test['target'].str.count('|'.join(stopwords))\ntest['num_target_stops'] = test['target'].str.count('|'.join(stopwords))\n\ntrain['dataset'] = 'train'\ntest['dataset'] = 'test'","metadata":{"_uuid":"c05280f7-90a1-49ce-9868-0f03aaff67b9","_cell_guid":"cd06dd39-bba8-4aec-a547-23fe3ec4a32e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:21.971351Z","iopub.execute_input":"2022-06-20T04:01:21.971607Z","iopub.status.idle":"2022-06-20T04:01:22.175149Z","shell.execute_reply.started":"2022-06-20T04:01:21.971579Z","shell.execute_reply":"2022-06-20T04:01:22.174429Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train = train.loc[~train.index.duplicated(keep='first')]\ntest = test.loc[~test.index.duplicated(keep='first')]\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\ndf_all = train.append(test)","metadata":{"_uuid":"75838474-5f7f-4ee7-8362-9bc40a853e1b","_cell_guid":"ad17e21f-04a5-43a8-87f3-91a9694e6806","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:24.549264Z","iopub.execute_input":"2022-06-20T04:01:24.549845Z","iopub.status.idle":"2022-06-20T04:01:24.584856Z","shell.execute_reply.started":"2022-06-20T04:01:24.549807Z","shell.execute_reply":"2022-06-20T04:01:24.584132Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_all['anchor_parsed'] = df_all['anchor'].apply(\n    lambda text:\n        \" \".join(\n            token.lemma_ for token in nlp(text)\n                if token.lemma_.lower() not in stopwords and token.is_alpha\n        )\n)\n\ndf_all['target_parsed'] = df_all['target'].apply(\n    lambda text:\n        \" \".join(\n            token.lemma_ for token in nlp(text)\n                if token.lemma_.lower() not in stopwords and token.is_alpha\n        )\n)","metadata":{"_uuid":"f5aeb6b5-62a0-434d-a5d8-b09d05215ac0","_cell_guid":"a2ded0a7-ecc0-4625-9798-b1af9d525c51","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:01:28.140787Z","iopub.execute_input":"2022-06-20T04:01:28.141388Z","iopub.status.idle":"2022-06-20T04:07:22.111433Z","shell.execute_reply.started":"2022-06-20T04:01:28.141350Z","shell.execute_reply":"2022-06-20T04:07:22.110474Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df_all['anchor_nlp'] = df_all.anchor.apply(lambda series: nlp(series))\ndf_all['target_nlp'] = df_all.target.apply(lambda series: nlp(series))\n\ndf_all['anchor_VERB'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\ndf_all['target_VERB'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\n\ndf_all['anchor_NOUN'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\ndf_all['target_NOUN'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\n\ndf_all['anchor_DET'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\ndf_all['target_DET'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\n\ndf_all['anchor_ADJ'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\ndf_all['target_ADJ'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\n\ndf_all['anchor_ADV'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADV']))\ndf_all['target_ADV'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADV']))\n\ndf_all['anchor_in_target'] = df_all.apply(lambda x: x[\"anchor_parsed\"] in x[\"target\"], axis=1)\ndf_all['target_in_anchor'] = df_all.apply(lambda x: x[\"target_parsed\"] in x[\"anchor\"], axis=1)","metadata":{"_uuid":"eb33244d-de87-45ba-90b9-541980ce4093","_cell_guid":"4fb10eae-003e-4f2e-a769-dc9f85dc81a1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:07:22.112865Z","iopub.execute_input":"2022-06-20T04:07:22.113107Z","iopub.status.idle":"2022-06-20T04:13:19.642124Z","shell.execute_reply.started":"2022-06-20T04:07:22.113072Z","shell.execute_reply":"2022-06-20T04:13:19.641359Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sims = df_all[[\"anchor_parsed\", \"target_parsed\"]]\nsimilarityValue = []\nfor i in range(sims.count()[0]):\n    sentence_1 = nlp(sims.iloc[i][0])\n    sentence_2 = nlp(sims.iloc[i][1])\n    similarityValue.append(sentence_1.similarity(sentence_2))\n\ndf_all['anchor_target_cos_sim'] = similarityValue\n\ntrain = df_all.loc[df_all['dataset'] == 'train']\ntest = df_all.loc[df_all['dataset'] == 'test']","metadata":{"_uuid":"00148a52-36d5-48d5-8600-875693a2172c","_cell_guid":"ca883518-7c1f-4112-978b-5a0dac56b352","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T04:13:19.643468Z","iopub.execute_input":"2022-06-20T04:13:19.643731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]' + train['context_text']\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]' + test['context_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformers","metadata":{}},{"cell_type":"code","source":"class CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = ['../input/bert-for-patents-0003/',\n                  '../input/deberta-v3-5folds/', \n                  '../input/electrav1/',\n                  '../input/pppm-debertav3large-baseline/',\n                  '../input/pppm-deberta-v2-xlarge-5fold/',\n                  '../input/pppmrobertalarge/',\n                 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_test(unit):\n        return {\n        **tokenizer(unit['text'])\n    }\n\nfor i in range(len(CFG.model_path)):   \n    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}tokenizer/')\n    test_ds = datasets.Dataset.from_pandas(test.drop(columns=['id', 'anchor', 'target', 'context', 'score', 'general_context',\n       'Chemistry and Metallurgy', 'Electricity', 'Fixed Constructions',\n       'Human Necessities', 'Mechanical Engineering',\n       'Operations and Transport', 'Physics', 'Textiles', 'context_text',\n       'section', 'classes', 'anchor_len', 'target_len', 'dataset', 'anchor_parsed', 'target_parsed', 'num_anchor_stops', 'num_target_stops', 'anchor_in_target', \n                                                        'target_in_anchor', 'anchor_ADV', 'target_ADV', 'anchor_ADJ', 'target_ADJ', \n                                                        'anchor_DET', 'target_DET', 'anchor_NOUN', 'target_NOUN', 'anchor_VERB', 'target_VERB', \n                                                        'anchor_nlp', 'target_nlp']))\n    test_ds = test_ds.map(process_test)\n\n    folds = sorted([m for m in glob.glob(f'{CFG.model_path[i]}*.pth') if '_fold' in m])\n\n    predictions_test = []\n    for fold in folds:        \n        trainer = Trainer(\n                AutoModelForSequenceClassification.from_pretrained(fold),\n                tokenizer=tokenizer,\n            )\n        \n        predictions_test.append(trainer.predict(test_ds).predictions)\n        del trainer\n        gc.collect()\n        \n    test[f'predictions_{os.basename(CFG.model_path[i])}'] = np.average(predictions_test, axis=0)\n    \n    del tokenizer, test_ds\n    gc.collect()","metadata":{"_uuid":"e8d796bc-1c18-4d24-aa17-4023ca03cb20","_cell_guid":"8bcead30-5905-499e-acb9-db98aca2b561","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_valid(unit):\n        return {\n        **tokenizer(unit['text'])\n    }\n\nfor i in range(len(CFG.model_path)):   \n    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}tokenizer/')\n    valid_ds = datasets.Dataset.from_pandas(train.drop(columns=['id', 'anchor', 'target', 'context', 'score', 'general_context',\n       'Chemistry and Metallurgy', 'Electricity', 'Fixed Constructions',\n       'Human Necessities', 'Mechanical Engineering',\n       'Operations and Transport', 'Physics', 'Textiles', 'context_text',\n       'section', 'classes', 'anchor_len', 'target_len', 'dataset', 'anchor_parsed', 'target_parsed', 'num_anchor_stops', 'num_target_stops', 'anchor_in_target', \n                                                        'target_in_anchor', 'anchor_ADV', 'target_ADV', 'anchor_ADJ', 'target_ADJ', \n                                                        'anchor_DET', 'target_DET', 'anchor_NOUN', 'target_NOUN', 'anchor_VERB', 'target_VERB', \n                                                        'anchor_nlp', 'target_nlp']))\n    valid_ds = valid_ds.map(process_valid)\n\n    folds = sorted([m for m in glob.glob(f'{CFG.model_path[i]}*.pth') if '_fold' in m])\n\n    predictions_valid = []\n    for fold in folds:        \n        trainer = Trainer(\n                AutoModelForSequenceClassification.from_pretrained(fold),\n                tokenizer=tokenizer,\n            )\n        \n        predictions_valid.append(trainer.predict(valid_ds).predictions)\n        del trainer\n        gc.collect()\n    \n    valid[f'predictions_{os.basename(CFG.model_path[i])}'] = np.average(predictions_valid, axis=0)\n    \n    del tokenizer, valid_ds\n    gc.collect()","metadata":{"_uuid":"f7e15874-fc82-4243-9a91-34cb24ab04bc","_cell_guid":"aa4a21af-9cd7-4051-be66-6f9837acf680","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = valid.drop(columns=['id', 'anchor', 'target', 'context', 'score', 'general_context', 'context_text',\n       'section', 'classes', 'dataset', 'anchor_parsed', 'target_parsed', 'anchor_nlp', 'target_nlp']).astype('float64')\ny = valid['score']","metadata":{"_uuid":"c6e4839b-efc5-4086-a590-2f7b7b65e8de","_cell_guid":"46bdead9-8e05-4907-bc48-78c2be20cb9a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:43:41.398489Z","iopub.execute_input":"2022-06-19T21:43:41.399071Z","iopub.status.idle":"2022-06-19T21:43:41.409069Z","shell.execute_reply.started":"2022-06-19T21:43:41.399031Z","shell.execute_reply":"2022-06-19T21:43:41.408336Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2022-06-19T21:43:45.074100Z","iopub.execute_input":"2022-06-19T21:43:45.074659Z","iopub.status.idle":"2022-06-19T21:43:45.114805Z","shell.execute_reply.started":"2022-06-19T21:43:45.074619Z","shell.execute_reply":"2022-06-19T21:43:45.113978Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"       Chemistry and Metallurgy  Electricity  Fixed Constructions  \\\n33511                       0.0          0.0                  0.0   \n18670                       0.0          0.0                  0.0   \n18049                       0.0          1.0                  0.0   \n31660                       0.0          0.0                  0.0   \n15573                       0.0          0.0                  0.0   \n...                         ...          ...                  ...   \n19301                       0.0          0.0                  0.0   \n1264                        1.0          0.0                  0.0   \n3024                        1.0          0.0                  0.0   \n31820                       0.0          0.0                  0.0   \n21648                       0.0          0.0                  1.0   \n\n       Human Necessities  Mechanical Engineering  Operations and Transport  \\\n33511                0.0                     0.0                       0.0   \n18670                0.0                     0.0                       1.0   \n18049                0.0                     0.0                       0.0   \n31660                1.0                     0.0                       0.0   \n15573                0.0                     0.0                       1.0   \n...                  ...                     ...                       ...   \n19301                0.0                     0.0                       0.0   \n1264                 0.0                     0.0                       0.0   \n3024                 0.0                     0.0                       0.0   \n31820                0.0                     0.0                       1.0   \n21648                0.0                     0.0                       0.0   \n\n       Physics  Textiles  anchor_len  target_len  ...  anchor_ADJ  target_ADJ  \\\n33511      1.0       0.0         2.0         2.0  ...         0.0         0.0   \n18670      0.0       0.0         2.0         2.0  ...         0.0         0.0   \n18049      0.0       0.0         2.0         1.0  ...         1.0         0.0   \n31660      0.0       0.0         2.0         1.0  ...         0.0         0.0   \n15573      0.0       0.0         4.0         2.0  ...         2.0         1.0   \n...        ...       ...         ...         ...  ...         ...         ...   \n19301      1.0       0.0         3.0         2.0  ...         0.0         0.0   \n1264       0.0       0.0         2.0         2.0  ...         0.0         0.0   \n3024       0.0       0.0         2.0         1.0  ...         0.0         0.0   \n31820      0.0       0.0         2.0         3.0  ...         0.0         0.0   \n21648      0.0       0.0         2.0         2.0  ...         0.0         0.0   \n\n       anchor_DET  target_DET  anchor_NOUN  target_NOUN  anchor_VERB  \\\n33511         0.0         0.0          1.0          2.0          1.0   \n18670         0.0         0.0          1.0          1.0          1.0   \n18049         0.0         0.0          1.0          1.0          0.0   \n31660         0.0         0.0          2.0          1.0          0.0   \n15573         0.0         0.0          2.0          1.0          0.0   \n...           ...         ...          ...          ...          ...   \n19301         0.0         0.0          3.0          2.0          0.0   \n1264          0.0         0.0          2.0          2.0          0.0   \n3024          0.0         0.0          1.0          1.0          1.0   \n31820         0.0         0.0          2.0          2.0          0.0   \n21648         0.0         0.0          2.0          1.0          0.0   \n\n       target_VERB  anchor_in_target  target_in_anchor  \n33511          0.0               0.0               0.0  \n18670          1.0               0.0               0.0  \n18049          0.0               0.0               0.0  \n31660          0.0               0.0               1.0  \n15573          0.0               0.0               0.0  \n...            ...               ...               ...  \n19301          0.0               0.0               0.0  \n1264           0.0               0.0               0.0  \n3024           0.0               0.0               0.0  \n31820          1.0               1.0               0.0  \n21648          0.0               0.0               0.0  \n\n[18236 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chemistry and Metallurgy</th>\n      <th>Electricity</th>\n      <th>Fixed Constructions</th>\n      <th>Human Necessities</th>\n      <th>Mechanical Engineering</th>\n      <th>Operations and Transport</th>\n      <th>Physics</th>\n      <th>Textiles</th>\n      <th>anchor_len</th>\n      <th>target_len</th>\n      <th>...</th>\n      <th>anchor_ADJ</th>\n      <th>target_ADJ</th>\n      <th>anchor_DET</th>\n      <th>target_DET</th>\n      <th>anchor_NOUN</th>\n      <th>target_NOUN</th>\n      <th>anchor_VERB</th>\n      <th>target_VERB</th>\n      <th>anchor_in_target</th>\n      <th>target_in_anchor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33511</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18670</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18049</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31660</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15573</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19301</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1264</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3024</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31820</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21648</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>18236 rows × 29 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def slice_by_corr(X, r_min=0):\n    # Create correlation matrix\n    corr_matrix = X.corr().abs()\n\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find features with correlation greater than r_min\n    return X[[column for column in upper.columns if any(upper[column] > r_min)]]\n\ndef variance_inflation_factor(X, exog_idx):\n    clf = LinearRegression(fit_intercept=True)\n    sub_X = np.delete(np.nan_to_num(X), exog_idx, axis=1)\n    sub_y = X[:, exog_idx][np.newaxis].T\n    sub_clf = clf.fit(sub_X, sub_y)\n    return 1 / (1 - r2_score(sub_y, sub_clf.predict(sub_X)))\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n\n    def __init__(self, thresh=10.0, nthreads=4, r_min=0, obs=250):\n        self.thresh = thresh\n        self.nthreads = nthreads\n        self.r_min = r_min\n        self.obs = obs\n        \n    def fit(self, X):\n        self.X = X\n        return self\n\n    def transform(self, X):\n        return ReduceVIF.calculate_vif(X, self.thresh, \n                                       self.nthreads, \n                                       self.r_min, \n                                       self.obs)\n\n    @staticmethod\n    def calculate_vif(X, thresh=10.0, nthreads=16, r_min=0, obs=250):        \n        dropped = True\n        vif_cols = []\n        X_vif_candidates = slice_by_corr(X, r_min)\n        X_vif_candidates = X_vif_candidates.sample(n=obs)\n        while dropped:\n            variables = X_vif_candidates.columns\n            dropped = False\n            with Parallel(n_jobs=nthreads, backend='threading') as parallel:\n                vif = parallel(\n                    delayed(variance_inflation_factor)(\n                        np.asarray(X_vif_candidates[variables].values),\n                        X_vif_candidates.columns.get_loc(var)) for var in \n                    X_vif_candidates.columns)\n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X_vif_candidates.columns[maxloc]} with vif={max_vif}')\n                vif_cols.append(X_vif_candidates.columns.tolist()[maxloc])\n                X_vif_candidates = X_vif_candidates.drop(\n                    [X_vif_candidates.columns.tolist()[maxloc]], axis=1)\n                dropped = True\n        \n        if len(vif_cols) > 0:\n            return X.drop(columns=vif_cols), vif_cols\n        else:\n            return X, vif_cols\n\n    \ndef preprocess_x_y(X, nodrop_columns=[],\n                   var_thr=0.95, remove_multi=True,\n                   standardize=True, standardizer='mm',\n                   std_dev=3, vif_thr=15, missingness_thr=0.50,\n                   zero_thr=0.99, nthreads=4):\n    from colorama import Fore, Style\n\n    # Replace all near-zero with zeros\n    # Drop excessively sparse columns with >zero_thr zeros\n    if zero_thr > 0:\n        X = X.apply(lambda x: np.where(np.abs(x) < 0.000001, 0, x))\n        X_tmp = X.T.loc[(X == 0).sum() < (float(zero_thr)) * X.shape[0]].T\n\n        if len(nodrop_columns) > 0:\n            X = pd.concat([X_tmp, X[[i for i in X.columns if i in\n                                     nodrop_columns and i not in\n                                     X_tmp.columns]]], axis=1)\n        else:\n            X = X_tmp\n        del X_tmp\n\n        if X.empty or len(X.columns) < 5:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (Zero Columns): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    # Remove columns with excessive missing values\n    X = X.dropna(thresh=len(X) * (1 - missingness_thr), axis=1)\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (missingness): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n    # Apply a simple imputer (note that this assumes extreme cases of\n    # missingness have already been addressed). The SimpleImputer is better\n    # for smaller datasets, whereas the IterativeImputer performs best on\n    # larger sets.\n\n    # from sklearn.experimental import enable_iterative_imputer\n    # from sklearn.impute import IterativeImputer\n    # imp = IterativeImputer(random_state=0, sample_posterior=True)\n    # X = pd.DataFrame(imp.fit_transform(X, y), columns=X.columns)\n    imp1 = SimpleImputer()\n    X = pd.DataFrame(imp1.fit_transform(X.astype('float32')),\n                     columns=X.columns)\n        \n    # Remove low-variance columns\n    sel = VarianceThreshold(threshold=(var_thr*(1-var_thr)))\n    sel.fit(X)\n    if len(nodrop_columns) > 0:\n        good_var_cols = X.columns[np.concatenate(\n            [sel.get_support(indices=True), np.array([X.columns.get_loc(c)\n                                                      for c in\n                                                      nodrop_columns if\n                                                      c in X])])]\n    else:\n        good_var_cols = X.columns[sel.get_support(indices=True)]\n    low_var_cols = [i for i in X.columns if i not in list(good_var_cols)]\n    if len(low_var_cols) > 0:\n        print(f\"Dropping {low_var_cols} for low variance...\")\n    X = X[good_var_cols]\n\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (low-variance): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n\\# Standardize X\n    if standardize is True:\n        if standardizer == 'ss':\n            scaler = StandardScaler()\n        else:\n            scaler = MinMaxScaler()\n        X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n        \n    # Remove multicollinear columns\n    if remove_multi is True:\n        try:\n            rvif = ReduceVIF(thresh=vif_thr, nthreads=nthreads)\n            X = rvif.fit_transform(X)[0]\n            if X.empty or len(X.columns) < 5:\n                print(f\"\\n\\n{Fore.RED}Empty feature-space \"\n                      f\"(multicollinearity): \"\n                      f\"{X}{Style.RESET_ALL}\\n\\n\")\n                return X\n        except:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (multicollinearity): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    print(f\"\\nX: {X}\\n\")\n    print(f\"Features: {list(X.columns)}\\n\")\n    return X\n\n\nclass Razors(object):\n    \"\"\"\n    Razors is a callable refit option for `GridSearchCV` whose aim is to\n    balance model complexity and cross-validated score in the spirit of the\n    \"one standard error\" rule of Breiman et al. (1984), which showed that\n    the tuning hyperparameter associated with the best performing model may be\n    prone to overfit. To help mitigate this risk, we can instead instruct\n    gridsearch to refit the highest performing 'parsimonious' model, as defined\n    using simple statistical rules (e.g. standard error (`sigma`),\n    percentile (`eta`), or significance level (`alpha`)) to compare\n    distributions of model performance across folds. Importantly, this\n    strategy assumes that the grid of multiple cross-validated models\n    can be principly ordered from simplest to most complex with respect to some\n    target hyperparameter of interest. To use the razors suite, supply\n    the `simplify` function partial of the `Razors` class as a callable\n    directly to the `refit` argument of `GridSearchCV`.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`.\n    scoring : str\n        Refit scoring metric.\n    param : str\n        Parameter whose complexity will be optimized.\n    rule : str\n        Rule for balancing model complexity with performance.\n        Options are 'se', 'percentile', and 'ranksum'. Default is 'se'.\n    sigma : int\n        Number of standard errors tolerance in the case that a standard error\n        threshold is used to filter outlying scores across folds. Required if\n        `rule`=='se'. Default is 1.\n    eta : float\n        Percentile tolerance in the case that a percentile threshold\n        is used to filter outlier scores across folds. Required if\n        `rule`=='percentile'. Default is 0.68.\n    alpha : float\n        An alpha significance level in the case that wilcoxon rank sum\n        hypothesis testing is used to filter outlying scores across folds.\n        Required if `rule`=='ranksum'. Default is 0.05.\n\n    References\n    ----------\n    Breiman, Friedman, Olshen, and Stone. (1984) Classification and Regression\n    Trees. Wadsworth.\n\n    Notes\n    -----\n    Here, 'simplest' is defined by the complexity of the model as influenced by\n    some user-defined target parameter (e.g. number of components, number of\n    estimators, polynomial degree, cost, scale, number hidden units, weight\n    decay, number of nearest neighbors, L1/L2 penalty, etc.).\n\n    The callable API accordingly assumes that the `params` attribute of\n    `cv_results_` 1) contains the indicated hyperparameter (`param`) of\n    interest, and 2) contains a sequence of values (numeric, boolean, or\n    categorical) that are ordered from least to most complex.\n    \"\"\"\n    __slots__ = ('cv_results', 'param', 'param_complexity', 'scoring',\n                 'rule', 'greater_is_better',\n                 '_scoring_funcs', '_scoring_dict',\n                 '_n_folds', '_splits', '_score_grid',\n                 '_cv_means', '_sigma', '_eta', '_alpha')\n\n    def __init__(\n            self,\n            cv_results_,\n            param,\n            scoring,\n            rule,\n            sigma=1,\n            eta=0.95,\n            alpha=0.01,\n    ):\n        import sklearn.metrics\n\n        self.cv_results = cv_results_\n        self.param = param\n        self.scoring = scoring\n        self.rule = rule\n        self._scoring_funcs = [\n            met\n            for met in sklearn.metrics.__all__\n            if (met.endswith(\"_score\")) or (met.endswith(\"_error\"))\n        ]\n        # Set _score metrics to True and _error metrics to False\n        self._scoring_dict = dict(\n            zip(\n                self._scoring_funcs,\n                [met.endswith(\"_score\") for met in self._scoring_funcs],\n            )\n        )\n        self.greater_is_better = self._check_scorer()\n        self._n_folds = len(list(set([i.split('_')[0] for i in\n                                     list(self.cv_results.keys()) if\n                                     i.startswith('split')])))\n        # Extract subgrid corresponding to the scoring metric of interest\n        self._splits = [i for i in list(self.cv_results.keys()) if\n                        i.endswith(f\"test_{self.scoring}\") and\n                        i.startswith('split')]\n        self._score_grid = np.vstack([self.cv_results[cv] for cv in\n                                      self._splits]).T\n        self._cv_means = np.array(np.nanmean(self._score_grid, axis=1))\n        self._sigma = sigma\n        self._eta = eta\n        self._alpha = alpha\n\n    def _check_scorer(self):\n        \"\"\"\n        Check whether the target refit scorer is negated. If so, adjust\n        greater_is_better accordingly.\n        \"\"\"\n\n        if (\n                self.scoring not in self._scoring_dict.keys()\n                and f\"{self.scoring}_score\" not in self._scoring_dict.keys()\n        ):\n            if self.scoring.startswith(\"neg_\"):\n                self.greater_is_better = True\n            else:\n                raise NotImplementedError(f\"Scoring metric {self.scoring} not \"\n                                          f\"recognized.\")\n        else:\n            self.greater_is_better = [\n                value for key, value in self._scoring_dict.items() if\n                self.scoring in key][0]\n        return self.greater_is_better\n\n    def _best_low_complexity(self):\n        \"\"\"\n        Balance model complexity with cross-validated score.\n\n        Return\n        ------\n        int\n            Index of a model that has the lowest complexity but its test score\n            is the highest on average across folds as compared to other models\n            that are equally likely to occur.\n        \"\"\"\n\n        # Check parameter(s) whose complexity we seek to restrict\n        if not any(self.param in x for x in\n                   self.cv_results[\"params\"][0].keys()):\n            raise KeyError(f\"Parameter {self.param} not found in cv grid.\")\n        else:\n            hyperparam = [\n                i for i in self.cv_results[\"params\"][0].keys() if\n                i.endswith(self.param)][0]\n\n        # Select low complexity threshold based on specified evaluation rule\n        if self.rule == \"se\":\n            if not self._sigma:\n                raise ValueError(\n                    \"For `se` rule, the tolerance \"\n                    \"(i.e. `_sigma`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_standard_error()\n        elif self.rule == \"percentile\":\n            if not self._eta:\n                raise ValueError(\n                    \"For `percentile` rule, the tolerance \"\n                    \"(i.e. `_eta`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_percentile()\n        elif self.rule == \"ranksum\":\n            if not self._alpha:\n                raise ValueError(\n                    \"For `ranksum` rule, the alpha-level \"\n                    \"(i.e. `_alpha`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_rank_sum_test()\n        else:\n            raise NotImplementedError(f\"{self.rule} is not a valid \"\n                                      f\"rule of RazorCV.\")\n\n        self.cv_results[f\"param_{hyperparam}\"].mask = np.where(\n            (self._cv_means >= float(l_cutoff)) &\n            (self._cv_means <= float(h_cutoff)),\n            True, False)\n\n        if np.sum(self.cv_results[f\"param_{hyperparam}\"].mask) == 0:\n            print(f\"\\nLow: {l_cutoff}\")\n            print(f\"High: {h_cutoff}\")\n            print(f\"{self._cv_means}\")\n            print(f\"hyperparam: {hyperparam}\\n\")\n            raise ValueError(\"No valid grid columns remain within the \"\n                             \"boundaries of the specified razor\")\n\n        highest_surviving_rank = np.nanmin(\n            self.cv_results[f\"rank_test_{self.scoring}\"][\n                self.cv_results[f\"param_{hyperparam}\"].mask])\n\n        # print(f\"Highest surviving rank: {highest_surviving_rank}\\n\")\n\n        return np.flatnonzero(np.isin(\n            self.cv_results[f\"rank_test_{self.scoring}\"],\n            highest_surviving_rank))[0]\n\n    def call_standard_error(self):\n        \"\"\"\n        Returns the simplest model whose performance is within `sigma`\n        standard errors of the average highest performing model.\n        \"\"\"\n\n        # Estimate the standard error across folds for each column of the grid\n        cv_se = np.array(np.nanstd(self._score_grid, axis=1) /\n                         np.sqrt(self._n_folds))\n\n        # Determine confidence interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    def call_rank_sum_test(self):\n        \"\"\"\n        Returns the simplest model whose paired performance across folds is\n        insignificantly different from the average highest performing,\n        at a predefined `alpha` level of significance.\n        \"\"\"\n\n        from scipy.stats import wilcoxon\n        import itertools\n\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n\n        # Perform signed Wilcoxon rank sum test for each pair combination of\n        # columns against the best average score column\n        tests = [pair for pair in list(itertools.combinations(range(\n            self._score_grid.shape[0]), 2)) if best_score_idx in pair]\n\n        p_dict = {}\n        for i, test in enumerate(tests):\n            p_dict[i] = wilcoxon(self._score_grid[test[0], :],\n                                 self._score_grid[test[1], :])[1]\n\n        # Sort and prune away significant tests\n        p_dict = {k: v for k, v in sorted(p_dict.items(),\n                                          key=lambda item: item[1]) if\n                  v > self._alpha}\n\n        # Flatten list of tuples, remove best score index, and take the\n        # lowest and highest remaining bounds\n        tests = [j for j in list(set(list(sum([tests[i] for i in\n                                               list(p_dict.keys())],\n                                              ())))) if j != best_score_idx]\n        if self.greater_is_better:\n            h_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n        else:\n            h_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n\n        return l_cutoff, h_cutoff\n\n\n    def call_percentile(self):\n        \"\"\"\n        Returns the simplest model whose performance is within the `eta`\n        percentile of the average highest performing model.\n        \"\"\"\n\n        # Estimate the indicated percentile, and its inverse, across folds for\n        # each column of the grid\n        perc_cutoff = np.nanpercentile(self._score_grid,\n                                       [100 * self._eta,\n                                        100 - 100 * self._eta], axis=1)\n\n        # Determine bounds of the percentile interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    @staticmethod\n    def simplify(param, scoring, rule='se', sigma=1, eta=0.68, alpha=0.01):\n        \"\"\"\n        Callable to be run as `refit` argument of `GridsearchCV`.\n\n        Parameters\n        ----------\n        param : str\n            Parameter with the largest influence on model complexity.\n        scoring : str\n            Refit scoring metric.\n        sigma : int\n            Number of standard errors tolerance in the case that a standard\n            error threshold is used to filter outlying scores across folds.\n            Only applicable if `rule`=='se'. Default is 1.\n        eta : float\n            Acceptable percent tolerance in the case that a percentile\n            threshold is used. Only applicable if `rule`=='percentile'.\n            Default is 0.68.\n        alpha : float\n            Alpha-level to use for signed wilcoxon rank sum testing.\n            Only applicable if `rule`=='ranksum'. Default is 0.01.\n        \"\"\"\n        from functools import partial\n\n        def razor_pass(\n                cv_results_, param, scoring, rule, sigma, alpha, eta\n        ):\n            rcv = Razors(cv_results_, param, scoring, rule=rule,\n                         sigma=sigma, alpha=alpha, eta=eta)\n            return rcv._best_low_complexity()\n\n        return partial(\n            razor_pass,\n            param=param,\n            scoring=scoring,\n            rule=rule,\n            sigma=sigma,\n            alpha=alpha,\n            eta=eta,\n        )\n\ndef divide_df(df_all,train_len):\n    return df_all.loc[:train_len-1], df_all.loc[train_len:].drop('target',axis=1)\n\ndef concat_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)","metadata":{"_uuid":"07381d69-4197-43f3-9ab8-44ec5d3e32cc","_cell_guid":"cb66bafc-95ec-44f7-871c-b4d825158399","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:50:15.015281Z","iopub.execute_input":"2022-06-19T21:50:15.015580Z","iopub.status.idle":"2022-06-19T21:50:15.086385Z","shell.execute_reply.started":"2022-06-19T21:50:15.015549Z","shell.execute_reply":"2022-06-19T21:50:15.085622Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"preprocess = FunctionTransformer(preprocess_x_y)\nX_clean = preprocess.fit_transform(X=X)\nsurviving_features = list(X_clean.columns)","metadata":{"_uuid":"babc5fa2-709b-49bd-9ad2-f9753ac17822","_cell_guid":"2712eda4-f76c-499e-acae-26b8f5b934c8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:50:16.896382Z","iopub.execute_input":"2022-06-19T21:50:16.897135Z","iopub.status.idle":"2022-06-19T21:50:18.057032Z","shell.execute_reply.started":"2022-06-19T21:50:16.897081Z","shell.execute_reply":"2022-06-19T21:50:18.056396Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Dropping ['Fixed Constructions', 'Textiles', 'anchor_len', 'target_len', 'predictions_deberta_v3', 'predictions_bert_4_patents', 'anchor_target_cos_sim', 'anchor_ADV', 'target_ADV', 'target_ADJ', 'target_DET', 'anchor_NOUN', 'target_NOUN'] for low variance...\n\n\n\u001b[31mEmpty feature-space (multicollinearity):        Chemistry and Metallurgy  Electricity  Human Necessities  \\\n0                           0.0          0.0                0.0   \n1                           0.0          0.0                0.0   \n2                           0.0          1.0                0.0   \n3                           0.0          0.0                1.0   \n4                           0.0          0.0                0.0   \n...                         ...          ...                ...   \n18231                       0.0          0.0                0.0   \n18232                       1.0          0.0                0.0   \n18233                       1.0          0.0                0.0   \n18234                       0.0          0.0                0.0   \n18235                       0.0          0.0                0.0   \n\n       Mechanical Engineering  Operations and Transport  Physics  \\\n0                         0.0                       0.0      1.0   \n1                         0.0                       1.0      0.0   \n2                         0.0                       0.0      0.0   \n3                         0.0                       0.0      0.0   \n4                         0.0                       1.0      0.0   \n...                       ...                       ...      ...   \n18231                     0.0                       0.0      1.0   \n18232                     0.0                       0.0      0.0   \n18233                     0.0                       0.0      0.0   \n18234                     0.0                       1.0      0.0   \n18235                     0.0                       0.0      0.0   \n\n       anchor_in_target  anchor_in_target  target_in_anchor  target_in_anchor  \\\n0                   0.0               0.0               0.0               0.0   \n1                   0.0               0.0               0.0               0.0   \n2                   0.0               0.0               0.0               0.0   \n3                   0.0               0.0               1.0               1.0   \n4                   0.0               0.0               0.0               0.0   \n...                 ...               ...               ...               ...   \n18231               0.0               0.0               0.0               0.0   \n18232               0.0               0.0               0.0               0.0   \n18233               0.0               0.0               0.0               0.0   \n18234               1.0               1.0               0.0               0.0   \n18235               0.0               0.0               0.0               0.0   \n\n       anchor_ADJ  anchor_VERB  target_VERB  anchor_in_target  \\\n0             0.0          0.5          0.0               0.0   \n1             0.0          0.5          0.5               0.0   \n2             0.5          0.0          0.0               0.0   \n3             0.0          0.0          0.0               0.0   \n4             1.0          0.0          0.0               0.0   \n...           ...          ...          ...               ...   \n18231         0.0          0.0          0.0               0.0   \n18232         0.0          0.0          0.0               0.0   \n18233         0.0          0.5          0.0               0.0   \n18234         0.0          0.0          0.5               1.0   \n18235         0.0          0.0          0.0               0.0   \n\n       anchor_in_target  target_in_anchor  target_in_anchor  \n0                   0.0               0.0               0.0  \n1                   0.0               0.0               0.0  \n2                   0.0               0.0               0.0  \n3                   0.0               1.0               1.0  \n4                   0.0               0.0               0.0  \n...                 ...               ...               ...  \n18231               0.0               0.0               0.0  \n18232               0.0               0.0               0.0  \n18233               0.0               0.0               0.0  \n18234               1.0               0.0               0.0  \n18235               0.0               0.0               0.0  \n\n[18236 rows x 17 columns]\u001b[0m\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"seed=42\nX_train, X_test, y_train, y_test = train_test_split(X_clean, y, random_state=seed)\n\nX_train = X_train.reset_index(drop=True)\ny_train = pd.DataFrame(y_train).reset_index(drop=True)\n\nX_train = X_train.head(10000)\ny_train = y_train.head(10000)\n\nX_test = X_test.reset_index(drop=True)\ny_test = pd.DataFrame(y_test).reset_index(drop=True)\n\nX_test = X_test.head(2000)\ny_test = y_test.head(2000)","metadata":{"_uuid":"2013efce-e219-44ea-9b4f-3fcaf00e7a1a","_cell_guid":"caa0b740-b39a-4181-b506-39e5829c1997","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:21:48.468921Z","iopub.execute_input":"2022-06-19T21:21:48.469185Z","iopub.status.idle":"2022-06-19T21:21:48.481074Z","shell.execute_reply.started":"2022-06-19T21:21:48.469136Z","shell.execute_reply":"2022-06-19T21:21:48.480296Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"models = [\n            'rf'\n         ]\n\nestimators = [\n        RandomForestRegressor(random_state=42, min_samples_leaf=7, min_samples_split=3)\n]","metadata":{"_uuid":"c795674c-3eba-46df-9ba0-107ffcc738fc","_cell_guid":"1cae2f7f-40b3-42cc-9d3c-2b5b2dca76bd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:21:48.482778Z","iopub.execute_input":"2022-06-19T21:21:48.483153Z","iopub.status.idle":"2022-06-19T21:21:48.489125Z","shell.execute_reply.started":"2022-06-19T21:21:48.483115Z","shell.execute_reply":"2022-06-19T21:21:48.488360Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"params={models[0]: {'max_depth': [3, 4, 5],\n                    'n_estimators': [50, 60, 70],}\n       }","metadata":{"_uuid":"4c5a010d-7329-46a0-b904-4af8032697d6","_cell_guid":"3cb8358e-126b-4b91-ae6f-ba4d70386c2a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:21:48.490756Z","iopub.execute_input":"2022-06-19T21:21:48.491014Z","iopub.status.idle":"2022-06-19T21:21:48.497851Z","shell.execute_reply.started":"2022-06-19T21:21:48.490981Z","shell.execute_reply":"2022-06-19T21:21:48.496957Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_factory = {}\n\ninner_scoring = \"neg_mean_absolute_error\"\n\nfor name, estimator in zip(models, estimators):\n    print(name)\n    model_factory[name] = {}\n    \n    pipe = Pipeline([\n        (name, TransformedTargetRegressor(regressor=estimator, transformer=MinMaxScaler()))\n    ])\n    model_params = {}\n    for hyperparam in params[name].keys():\n        model_params[f\"{name}__regressor__{hyperparam}\"] = params[name][hyperparam]\n    pipe_grid_cv = GridSearchCV(pipe, model_params, scoring=[inner_scoring], \n                       refit=Razors.simplify(param=f'{name}__regressor__n_estimators', \n                                             scoring=inner_scoring, rule=\"se\", sigma=1), \n                       cv=KFold(n_splits=5, shuffle=True, random_state=seed), n_jobs=-1)\n    pipe_grid_cv.fit(X_train, y_train.values.ravel())\n    model_factory[name]['oos_score'] = cross_val_score(pipe_grid_cv, X_test, y_test.values.ravel(), \n                                                       scoring='r2', \n                                                       cv=KFold(n_splits=5, shuffle=True, \n                                                                random_state=seed + 1))\n    model_factory[name]['best_params'] = pipe_grid_cv.best_params_\n    model_factory[name]['best_estimator'] = pipe_grid_cv.best_estimator_\n\nleaderboard = {}\nfor mod in model_factory.keys():\n    leaderboard[mod] = np.mean(model_factory[mod]['oos_score'])\n\nbest_estimator_name = max(leaderboard, key=leaderboard.get)\n\nbest_estimator = model_factory[best_estimator_name]['best_estimator']\n\nmodel_factory","metadata":{"_uuid":"8036d58b-7c10-49bf-a571-6a0567094cff","_cell_guid":"47c28da5-8a5f-4096-a8f2-38710f345686","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:21:48.498897Z","iopub.execute_input":"2022-06-19T21:21:48.499804Z","iopub.status.idle":"2022-06-19T21:22:34.860662Z","shell.execute_reply.started":"2022-06-19T21:21:48.499767Z","shell.execute_reply":"2022-06-19T21:22:34.859900Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"rf\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'rf': {'oos_score': array([0.91208248, 0.91473054, 0.93139849, 0.92554432, 0.91192949]),\n  'best_params': {'rf__regressor__max_depth': 4,\n   'rf__regressor__n_estimators': 60},\n  'best_estimator': Pipeline(steps=[('rf',\n                   TransformedTargetRegressor(regressor=RandomForestRegressor(max_depth=4,\n                                                                              min_samples_leaf=7,\n                                                                              min_samples_split=3,\n                                                                              n_estimators=60,\n                                                                              random_state=42),\n                                              transformer=MinMaxScaler()))])}}"},"metadata":{}}]},{"cell_type":"code","source":"model_factory[name]['best_params']","metadata":{"_uuid":"97d22c7f-b8a9-4e0f-8184-635733fd552c","_cell_guid":"424275f0-4842-4f8c-9144-623804168b6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:34.862245Z","iopub.execute_input":"2022-06-19T21:22:34.862890Z","iopub.status.idle":"2022-06-19T21:22:34.869925Z","shell.execute_reply.started":"2022-06-19T21:22:34.862847Z","shell.execute_reply":"2022-06-19T21:22:34.869240Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'rf__regressor__max_depth': 4, 'rf__regressor__n_estimators': 60}"},"metadata":{}}]},{"cell_type":"code","source":"outer_best = KFold(n_splits=5, shuffle=True, random_state=seed)\n\nbest_estimator.fit(X_clean, y)\n\nscores = cross_val_score(best_estimator, X_clean, y, scoring='r2', cv=outer_best, n_jobs=-1, error_score='raise')\nscores","metadata":{"_uuid":"b1d45736-646e-4036-98ef-126d90fde156","_cell_guid":"759be8eb-545f-42c1-aa8b-7b465db135e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:34.871245Z","iopub.execute_input":"2022-06-19T21:22:34.872085Z","iopub.status.idle":"2022-06-19T21:22:39.086048Z","shell.execute_reply.started":"2022-06-19T21:22:34.872043Z","shell.execute_reply":"2022-06-19T21:22:39.085326Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"array([0.92587911, 0.93063741, 0.93061506, 0.92976181, 0.92367142])"},"metadata":{}}]},{"cell_type":"code","source":"best_estimator.named_steps['rf'].regressor_.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2022-06-19T21:27:09.468313Z","iopub.execute_input":"2022-06-19T21:27:09.468897Z","iopub.status.idle":"2022-06-19T21:27:09.481230Z","shell.execute_reply.started":"2022-06-19T21:27:09.468855Z","shell.execute_reply":"2022-06-19T21:27:09.480547Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"array([0.00000000e+00, 1.09784250e-05, 9.98848236e-01, 2.56479738e-05,\n       1.10868433e-03, 3.30096124e-06, 3.15230326e-06])"},"metadata":{}}]},{"cell_type":"code","source":"model_path = (\n    f\"/kaggle/working/rf_model.joblib\"\n)\ndump(best_estimator, model_path)","metadata":{"_uuid":"aad5fc5b-78b8-4bd7-b3ca-b4097950aad0","_cell_guid":"b4965bcb-bb94-422b-99b6-4a6f29e5ad94","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:39.088839Z","iopub.execute_input":"2022-06-19T21:22:39.089433Z","iopub.status.idle":"2022-06-19T21:22:39.123266Z","shell.execute_reply.started":"2022-06-19T21:22:39.089404Z","shell.execute_reply":"2022-06-19T21:22:39.122485Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/rf_model.joblib']"},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')","metadata":{"_uuid":"6d5556c8-1701-499c-a956-3d8b3152f27e","_cell_guid":"26c18b6f-bbd2-44d7-be32-04ab3ddcf43e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:39.124765Z","iopub.execute_input":"2022-06-19T21:22:39.125293Z","iopub.status.idle":"2022-06-19T21:22:39.137005Z","shell.execute_reply.started":"2022-06-19T21:22:39.125255Z","shell.execute_reply":"2022-06-19T21:22:39.136229Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ny_pred = best_estimator.predict(pd.DataFrame(scaler.fit_transform(test[surviving_features]), columns=surviving_features))","metadata":{"_uuid":"872795a1-5df7-47e3-b646-8457869d79db","_cell_guid":"5798fccc-7530-4166-a31d-82295021445b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:39.138293Z","iopub.execute_input":"2022-06-19T21:22:39.139085Z","iopub.status.idle":"2022-06-19T21:22:39.157172Z","shell.execute_reply.started":"2022-06-19T21:22:39.139046Z","shell.execute_reply":"2022-06-19T21:22:39.156440Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"submission['id'] = test['id']\nsubmission['score'] = y_pred","metadata":{"_uuid":"e87dd27c-b526-4099-80a7-680fd95e7d09","_cell_guid":"9822b0f9-7585-4c97-992d-b0b18cbcc8b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:39.161969Z","iopub.execute_input":"2022-06-19T21:22:39.162464Z","iopub.status.idle":"2022-06-19T21:22:39.167815Z","shell.execute_reply.started":"2022-06-19T21:22:39.162435Z","shell.execute_reply":"2022-06-19T21:22:39.167099Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"804eee29-db36-4b3a-9229-b78f2f05bf59","_cell_guid":"35399c7a-1c57-40db-aeac-4f6b8e9b0ece","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-19T21:22:39.169028Z","iopub.execute_input":"2022-06-19T21:22:39.169688Z","iopub.status.idle":"2022-06-19T21:22:39.178325Z","shell.execute_reply.started":"2022-06-19T21:22:39.169648Z","shell.execute_reply":"2022-06-19T21:22:39.177541Z"},"trusted":true},"execution_count":30,"outputs":[]}]}