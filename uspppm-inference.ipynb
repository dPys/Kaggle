{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport spacy\nimport nltk\nimport random\nimport itertools\nimport torch\nimport unicodedata\nimport datasets, transformers\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk.corpus import stopwords, wordnet\nfrom joblib import dump\nimport scipy as sp\nfrom scipy import stats\nfrom itertools import groupby\nfrom joblib import parallel_backend\nfrom sklearn import linear_model, decomposition\nfrom collections import OrderedDict\nfrom operator import itemgetter\nfrom sklearn import metrics\nfrom joblib import Parallel, delayed\nfrom transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.ensemble import RandomForestRegressor, IsolationForest\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, train_test_split, KFold\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone, RegressorMixin\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS, ALPHA, ALPHA_LOWER, ALPHA_UPPER\nfrom spacy.util import compile_infix_regex\n# from scispacy.abbreviation import AbbreviationDetector\nfrom spacy.pipeline import EntityRecognizer\n\ntry:\n    from sklearn.utils._testing import ignore_warnings\nexcept:\n    from sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n%env TOKENIZERS_PARALLELISM=true\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"a942c950-ef86-481e-9bdd-4fc8ac222320","_cell_guid":"62ad6f3d-b227-4aaf-93bb-ca4860556fd2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:29.589919Z","iopub.execute_input":"2022-06-20T05:49:29.590384Z","iopub.status.idle":"2022-06-20T05:49:29.613579Z","shell.execute_reply.started":"2022-06-20T05:49:29.590331Z","shell.execute_reply":"2022-06-20T05:49:29.612513Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=true\n","output_type":"stream"}]},{"cell_type":"code","source":"INPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T05:49:29.615880Z","iopub.execute_input":"2022-06-20T05:49:29.616462Z","iopub.status.idle":"2022-06-20T05:49:29.632134Z","shell.execute_reply.started":"2022-06-20T05:49:29.616408Z","shell.execute_reply":"2022-06-20T05:49:29.630939Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')\n# nlp.add_pipe(\"abbreviation_detector\")\nre_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)","metadata":{"_uuid":"561bcb8c-31c6-4e52-b925-97112600781b","_cell_guid":"147479a7-c7c8-472f-bfdd-2c560020546b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:29.635091Z","iopub.execute_input":"2022-06-20T05:49:29.636015Z","iopub.status.idle":"2022-06-20T05:49:32.192830Z","shell.execute_reply.started":"2022-06-20T05:49:29.635970Z","shell.execute_reply":"2022-06-20T05:49:32.191760Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('display.precision', 4)\n# pd.set_option('display.max_rows', 500)\n# pd.set_option('display.max_columns', 500)\n# pd.set_option('display.width', 1000)\n\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 42\nCUSTOM_BATCH = 24\n\n# Stopwords and infixes\nADDITIONAL_STOPWORDS = ['one or more', 'a', 'needn', 'a', 'not', 'able', 'never', 'about', 'neednâ€™t', 'accordance', 'now', 'often', 'above', 'no', 'according', 'of', 'mentioned', 'others', 'after', 'nor', 'all', 'on', 'accordingly', 'otherwise', 'again', 'not', 'also', 'onto', 'across', 'overall', 'against', 'now', 'an', 'or', 'along', 'rather', 'ain', 'o', 'and', 'other', 'already', 'remarkably', 'all', 'of', 'another', 'particularly', 'alternatively', 'significantly', 'am', 'off', 'are', 'preferably', 'always', 'simply', 'an', 'on', 'as', 'preferred', 'among', 'sometimes', 'and', 'once', 'at', 'present', 'and/or', 'specifically', 'any', 'only', 'be', 'provide', 'anything', 'straight', 'are', 'or', 'because', 'provided', 'anywhere', 'forward', 'aren', 'other', 'been', 'provides', 'better', 'substantially', 'arenâ€™t', 'our', 'being', 'relatively', 'disclosure', 'thereafter', 'as', 'ours', 'by', 'respectively', 'due', 'therebetween', 'at', 'ourselves', 'claim', 'said', 'easily', 'therefor', 'be', 'out', 'comprises', 'comprising', 'should', 'easy', 'therefrom', 'because', 'over', 'since', 'e.g', 'therein', 'been', 'own', 'could', 'some', 'either', 'thereinto', 'before', 're', 'described', 'such', 'elsewhere', 'thereon', 'being', 's', 'desired', 'suitable', 'enough', 'therethrough', 'below', 'same', 'do', 'than', 'especially', 'therewith', 'between', 'shan', 'does', 'that', 'essentially', 'together', 'both', 'shanâ€™t', 'each', 'the', 'et', 'al', 'toward', 'but', 'she', 'embodiment', 'their', 'etc', 'towards', 'by', 'sheâ€™s', 'fig', 'then', 'eventually', 'typical', 'can', 'should', 'figs', 'there', 'excellent', 'upon', 'couldn', 'shouldâ€™ve', 'for', 'thereby', 'finally', 'via', 'couldnâ€™t', 'shouldn', 'from', 'therefore', 'furthermore', 'vice', 'versa', 'd', 'shouldnâ€™t', 'further', 'thereof', 'good', 'whatever', 'did', 'so', 'generally', 'thereto', 'hence', 'whereas', 'didn', 'some', 'had', 'these', 'he/she', 'whereat', 'didnâ€™t', 'such', 'has', 'they', 'him/her', 'wherever', 'do', 't', 'have', 'this', 'his/her', 'whether', 'does', 'than', 'having', 'those', 'ie', 'whose', 'doesn', 'that', 'herein', 'thus', 'ii', 'within', 'doesnâ€™t', 'thatâ€™ll', 'however', 'to', 'iii', 'without', 'doing', 'the', 'if', 'use', 'instead', 'yet', 'don', 'their', 'in', 'various', 'later', 'donâ€™t', 'theirs', 'into', 'was', 'like', 'down', 'them', 'invention', 'were', 'little', 'during', 'themselves', 'is', 'what', 'many', 'each', 'there', 'it', 'when', 'may', 'few', 'these', 'its', 'where', 'meanwhile', 'for', 'they', 'means', 'whereby', 'might', 'from', 'this', 'wherein', 'moreover', 'further', 'those', 'which', 'much', 'had', 'through', 'while', 'must', 'hadn', 'to', 'who', 'hadnâ€™t', 'too', 'will', 'has', 'under', 'with', 'hasn', 'until', 'Would', 'hasnâ€™t', 'up', 'have', 've', 'haven', 'very', 'havenâ€™t', 'was', 'having', 'wasn', 'he', 'wasnâ€™t', 'her', 'we', 'here', 'were', 'hers', 'weren', 'herself', 'werenâ€™t', 'him', 'what', 'himself', 'when', 'his', 'where', 'how', 'which', 'i', 'while', 'if', 'who', 'in', 'whom', 'into', 'why', 'is', 'will', 'isn', 'with', 'isnâ€™t', 'won', 'it', 'wonâ€™t', 'itâ€™s', 'wouldn', 'its', 'wouldnâ€™t', 'itself', 'y', 'just', 'you', 'll', 'youâ€™d', 'm', 'youâ€™ll', 'ma', 'youâ€™re', 'me', 'youâ€™ve', 'mightn', 'your', 'mightnâ€™t', 'yours', 'more', 'yourself', 'most', 'yourselves', 'mustn', 'mustnâ€™t', 'my', 'myself']\n\npuncts = ['\\u200d','?', '....','..','...','', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\',\n    'â€¢', '~', 'Â£', 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ',\n    'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“',\n    'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾',\n    'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜',\n    'Â¹', 'â‰¤', 'â€¡', 'âˆš', '!','ðŸ…°','ðŸ…±']\n\ninfixes = (\n    LIST_ELLIPSES\n    + LIST_ICONS\n    + [\n        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n        ),\n        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n        # âœ… Commented out regex that splits on hyphens between letters:\n        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n        r'''[-~]'''\n    ]\n)\n\nstopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n\nprefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\nsuffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\ninfix_re = compile_infix_regex(infixes)\n\ndef customize_tokenizer(nlp):\n    # Adds support to use `-` as the delimiter for tokenization\n    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n                     suffix_search=suffix_re.search,\n                     infix_finditer=infix_re.finditer,\n                     token_match=None\n                    )\n\nnlp.tokenizer = customize_tokenizer(nlp)","metadata":{"_uuid":"8238d081-b92b-434a-8b85-4329fbcb278f","_cell_guid":"ba02c35f-90b7-4f80-905b-0391f96f2347","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:32.196263Z","iopub.execute_input":"2022-06-20T05:49:32.196644Z","iopub.status.idle":"2022-06-20T05:49:32.237849Z","shell.execute_reply.started":"2022-06-20T05:49:32.196585Z","shell.execute_reply":"2022-06-20T05:49:32.236830Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"table = {\n'A': 'Human Necessities',\n'B': 'Operations and Transport',\n'C': 'Chemistry and Metallurgy',\n'D': 'Textiles',\n'E': 'Fixed Constructions',\n'F': 'Mechanical Engineering',\n'G': 'Physics',\n'H': 'Electricity',\n'Y': 'Emerging Cross-Sectional Technologies'\n}","metadata":{"_uuid":"102bd6d4-c302-4fde-995f-decacdbce996","_cell_guid":"d97adb40-4f30-487b-aa22-c14a50be0f17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:32.239571Z","iopub.execute_input":"2022-06-20T05:49:32.240283Z","iopub.status.idle":"2022-06-20T05:49:32.246980Z","shell.execute_reply.started":"2022-06-20T05:49:32.240229Z","shell.execute_reply":"2022-06-20T05:49:32.245899Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def remove_from_list(x, stuff_to_remove) -> list:\n    for item in stuff_to_remove:\n        # Making sure to iterate through the entire token\n        for i,token in enumerate(x):\n            if item == token:\n                del x[i]\n    return x\n\ndef Remove_Duplicates(text_in):\n    return re.sub(r\"\\b(\\w+)(?:\\W\\1\\b)+\", r\"\\1\", text_in, flags=re.IGNORECASE)\n\n\ndef remove_consecutive_nums(text):\n    # Remove any chunks of consecutive numbers\n    number_strings = re.findall(r'\\d+[ \\t]\\d+', text)\n    ind_num_strings = []\n    for j in number_strings:\n        x = [int(i) for i in j.split()]\n        ind_num_strings.append(x)\n\n    flat_num_list = [item for sublist in ind_num_strings for item in sublist]\n\n    for i in flat_num_list:\n        j=re.sub(r'\\d+','',str(i))\n        text = text.replace(str(i),j)\n    return text\n\n\ndef basic_clean(text_list, infixes, stopwords):\n    \"\"\"\n    A simple function to clean up the data. All the words that\n    are not designated as a stop word is then lemmatized after\n    encoding and basic regex parsing are performed.\n    \"\"\"\n\n    text_list_clean = []\n    for text in text_list:\n        text = re.sub(r'[\\)\\(\\.\\,\\;\\\\\\?\\&\\%\\!\\+\\-]', '', re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6Â§\\\\\\Â£\\Ã‚*_<>\"\"âŽ«â€¢{}Î“~]', ' ', str(' '.join(re.split('\\s*-\\s*', text)))))\n        if len(text.split(\"  \")) > 1000:\n            text = \" \".join([\"\".join(w.split(\" \")) if len(w.split(' '))>1 else w for w in text.split(\"  \")])\n        text_list_clean.append([i for i in remove_from_list(re.sub('\\s+', ' ', re.sub('\\s\\s+', ' ', re.sub('\\s+\\s+', ' ', Remove_Duplicates(re.sub(r\"\\b(?=[mdclxvii])m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})([ii]x|[ii]v|v?[ii]{0,3})\\b\\.?\", '', (unicodedata.normalize('NFKD', re.sub(' +', ' ', re.sub(r\"\\s+\\s+\",\" \", re.sub(r\"\\\\,\",\",\", re.sub(r\" \\,\",\",\", re.sub(r\"\\\\.\",\".\", re.sub(r\" \\.\",\".\", re.sub(r\"\\(\\s+\\)\",\"\", re.sub(r\"\\(\\)\",\"\", re.sub(r\" \\)\",\"\", re.sub(r\"\\( \",\"\", remove_consecutive_nums(re.sub(r\"\\s+\",\" \", re.sub(r\"([A-z])\\- ([A-z])\", r\"\\1\\2\", re.sub(r'\\s', ' ', text)).replace('\\'','').replace('. .', '.').replace('\\'',''))))))))))))).lower())\n        .encode('ascii', 'ignore')\n        .decode('utf-8', 'ignore')\n        .lower())))))).split(), puncts) if not i.isdigit() or i in stopwords])\n        del text\n\n    return '. '.join(x.strip().capitalize() for x in '. '.join(' '.join([word for word in sent]) for sent in text_list_clean).split('.')) + '.'\n\n\ndef get_cpc_texts():\n    \"\"\"\n    Function taken from Y Nakama's notebook:\n    https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train\n    \"\"\"\n    contexts = []\n    pattern = '[A-Z]\\d+'\n    for file_name in os.listdir('cpc-data/CPCSchemeXML202105'):\n        result = re.findall(pattern, file_name)\n        if result:\n            contexts.append(result)\n    contexts = sorted(set(sum(contexts, [])))\n    results = {}\n    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n        with open(f'cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n            s = f.read()\n        pattern = f'{cpc}\\t\\t.+'\n        result = re.findall(pattern, s)\n        cpc_result = result[0].lstrip(pattern)\n        for context in [c for c in contexts if c[0] == cpc]:\n            pattern = f'{context}\\t\\t.+'\n            result = re.findall(pattern, s)\n            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n    return results","metadata":{"_uuid":"d75199e1-2d90-4925-8857-fc15aa97eee9","_cell_guid":"4047b6a7-5604-4be7-ab3c-b02d7ea18512","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:32.248918Z","iopub.execute_input":"2022-06-20T05:49:32.249428Z","iopub.status.idle":"2022-06-20T05:49:32.276897Z","shell.execute_reply.started":"2022-06-20T05:49:32.249383Z","shell.execute_reply":"2022-06-20T05:49:32.275845Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train.csv')\ntest = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')","metadata":{"_uuid":"f3ffefe0-201e-4750-b043-c76da4bab192","_cell_guid":"4ec8f13b-391a-48c7-a86c-c35526506b16","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:32.278815Z","iopub.execute_input":"2022-06-20T05:49:32.279185Z","iopub.status.idle":"2022-06-20T05:49:32.348639Z","shell.execute_reply.started":"2022-06-20T05:49:32.279140Z","shell.execute_reply":"2022-06-20T05:49:32.347715Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"train['general_context'] = train['context'].apply(lambda x: table[x[0].upper()])\ntest['general_context'] = test['context'].apply(lambda x: table[x[0].upper()])\n\ntrain = pd.concat([train, pd.get_dummies(train['general_context'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['general_context'])], axis=1)\n\ncpc_texts = torch.load(f\"../input/cpc-texts/cpc_texts.pth\")\ntrain['context_text'] = train['context'].map(cpc_texts)\ntest['context_text'] = test['context'].map(cpc_texts)\n\ntrain['section'] = train['context'].astype(str).str[0]\ntrain['classes'] = train['context'].astype(str).str[1:]\ntest['section'] = test['context'].astype(str).str[0]\ntest['classes'] = test['context'].astype(str).str[1:]\n\ntrain['anchor_len'] = train['anchor'].str.split().str.len()\ntrain['target_len'] = train['target'].str.split().str.len()\n\ntest['anchor_len'] = test['anchor'].str.split().str.len()\ntest['target_len'] = test['target'].str.split().str.len()\n\ntrain['num_anchor_stops'] = test['anchor'].str.count('|'.join(stopwords))\ntest['num_anchor_stops'] = test['anchor'].str.count('|'.join(stopwords))\ntrain['num_target_stops'] = test['target'].str.count('|'.join(stopwords))\ntest['num_target_stops'] = test['target'].str.count('|'.join(stopwords))\n\ntrain['dataset'] = 'train'\ntest['dataset'] = 'test'","metadata":{"_uuid":"c05280f7-90a1-49ce-9868-0f03aaff67b9","_cell_guid":"cd06dd39-bba8-4aec-a547-23fe3ec4a32e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:32.350071Z","iopub.execute_input":"2022-06-20T05:49:32.350593Z","iopub.status.idle":"2022-06-20T05:49:32.985333Z","shell.execute_reply.started":"2022-06-20T05:49:32.350548Z","shell.execute_reply":"2022-06-20T05:49:32.984365Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train = train.loc[~train.index.duplicated(keep='first')]\ntest = test.loc[~test.index.duplicated(keep='first')]\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\ndf_all = train.append(test)","metadata":{"_uuid":"75838474-5f7f-4ee7-8362-9bc40a853e1b","_cell_guid":"ad17e21f-04a5-43a8-87f3-91a9694e6806","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:32.989443Z","iopub.execute_input":"2022-06-20T05:49:32.989756Z","iopub.status.idle":"2022-06-20T05:49:33.026035Z","shell.execute_reply.started":"2022-06-20T05:49:32.989718Z","shell.execute_reply":"2022-06-20T05:49:33.025111Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df_all['anchor_parsed'] = df_all['anchor'].apply(\n    lambda text:\n        \" \".join(\n            token.lemma_ for token in nlp(text)\n                if token.lemma_.lower() not in stopwords and token.is_alpha\n        )\n)\n\ndf_all['target_parsed'] = df_all['target'].apply(\n    lambda text:\n        \" \".join(\n            token.lemma_ for token in nlp(text)\n                if token.lemma_.lower() not in stopwords and token.is_alpha\n        )\n)","metadata":{"_uuid":"f5aeb6b5-62a0-434d-a5d8-b09d05215ac0","_cell_guid":"a2ded0a7-ecc0-4625-9798-b1af9d525c51","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T05:49:33.027508Z","iopub.execute_input":"2022-06-20T05:49:33.027944Z","iopub.status.idle":"2022-06-20T05:57:37.613393Z","shell.execute_reply.started":"2022-06-20T05:49:33.027903Z","shell.execute_reply":"2022-06-20T05:57:37.612512Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df_all['anchor_nlp'] = df_all.anchor.apply(lambda series: nlp(series))\ndf_all['target_nlp'] = df_all.target.apply(lambda series: nlp(series))\n\ndf_all['anchor_VERB'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\ndf_all['target_VERB'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'VERB']))\n\ndf_all['anchor_NOUN'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\ndf_all['target_NOUN'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'NOUN']))\n\ndf_all['anchor_DET'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\ndf_all['target_DET'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'DET']))\n\ndf_all['anchor_ADJ'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\ndf_all['target_ADJ'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADJ']))\n\ndf_all['anchor_ADV'] = df_all.anchor_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADV']))\ndf_all['target_ADV'] = df_all.target_nlp.apply(lambda series: len([token for token in series if token.pos_ == 'ADV']))\n\ndf_all['anchor_in_target'] = df_all.apply(lambda x: x[\"anchor_parsed\"] in x[\"target\"], axis=1)\ndf_all['target_in_anchor'] = df_all.apply(lambda x: x[\"target_parsed\"] in x[\"anchor\"], axis=1)","metadata":{"_uuid":"eb33244d-de87-45ba-90b9-541980ce4093","_cell_guid":"4fb10eae-003e-4f2e-a769-dc9f85dc81a1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T07:05:47.767325Z","iopub.execute_input":"2022-06-20T07:05:47.767961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sims = df_all[[\"anchor_parsed\", \"target_parsed\"]]\nsimilarityValue = []\nfor i in range(sims.count()[0]):\n    sentence_1 = nlp(sims.iloc[i][0])\n    sentence_2 = nlp(sims.iloc[i][1])\n    similarityValue.append(sentence_1.similarity(sentence_2))\n\ndf_all['anchor_target_cos_sim'] = similarityValue\n\ntrain = df_all.loc[df_all['dataset'] == 'train']\ntest = df_all.loc[df_all['dataset'] == 'test']","metadata":{"_uuid":"00148a52-36d5-48d5-8600-875693a2172c","_cell_guid":"ca883518-7c1f-4112-978b-5a0dac56b352","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T06:54:12.549347Z","iopub.execute_input":"2022-06-20T06:54:12.550147Z","iopub.status.idle":"2022-06-20T07:02:58.519380Z","shell.execute_reply.started":"2022-06-20T06:54:12.550109Z","shell.execute_reply":"2022-06-20T07:02:58.518397Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]' + train['context_text']\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]' + test['context_text']","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:03:32.577969Z","iopub.execute_input":"2022-06-20T07:03:32.578283Z","iopub.status.idle":"2022-06-20T07:03:32.615247Z","shell.execute_reply.started":"2022-06-20T07:03:32.578249Z","shell.execute_reply":"2022-06-20T07:03:32.614328Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"# Transformers","metadata":{}},{"cell_type":"code","source":"class CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = [\n                  '../input/deberta-large-v1/',\n                   '../input/deberta-v3-5folds/',\n                  '../input/xlm-roberta-large-5folds/',\n                  '../input/electra-upppm/electra_upppm/',\n                  '../input/bert4patents-upppm/bert4patents_upppm/',\n                 ]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:26:38.047637Z","iopub.execute_input":"2022-06-20T06:26:38.047924Z","iopub.status.idle":"2022-06-20T06:26:38.053224Z","shell.execute_reply.started":"2022-06-20T06:26:38.047894Z","shell.execute_reply":"2022-06-20T06:26:38.052121Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def process_test(unit):\n        return {\n        **tokenizer(unit['text'])\n    }\n\nfor i in range(len(CFG.model_path)):   \n    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}fold0', local_files_only=True)\n    test_ds = datasets.Dataset.from_pandas(test[['text']])\n    test_ds = test_ds.map(process_test)\n\n    folds = sorted([m for m in glob.glob(f'{CFG.model_path[i]}') if 'fold' in m])\n\n    predictions_test = []\n    for fold in range(len(folds)):        \n        trainer = Trainer(\n                AutoModelForSequenceClassification.from_pretrained(f'{CFG.model_path[i]}fold{fold}', local_files_only=True),\n                tokenizer=tokenizer,\n            )\n        \n        predictions_test.append(trainer.predict(test_ds).predictions)\n        del trainer\n        gc.collect()\n        \n    test[f'predictions_{os.path.basename(CFG.model_path[i])}'] = np.average(predictions_test, axis=0)\n    \n    del tokenizer, test_ds\n    gc.collect()","metadata":{"_uuid":"e8d796bc-1c18-4d24-aa17-4023ca03cb20","_cell_guid":"8bcead30-5905-499e-acb9-db98aca2b561","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T06:26:43.400859Z","iopub.execute_input":"2022-06-20T06:26:43.401137Z","iopub.status.idle":"2022-06-20T06:27:02.192538Z","shell.execute_reply.started":"2022-06-20T06:26:43.401108Z","shell.execute_reply":"2022-06-20T06:27:02.191554Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"Didn't find file ../input/deberta-large-v1/fold0/added_tokens.json. We won't load it.\nloading file ../input/deberta-large-v1/fold0/vocab.json\nloading file ../input/deberta-large-v1/fold0/merges.txt\nloading file ../input/deberta-large-v1/fold0/tokenizer.json\nloading file None\nloading file ../input/deberta-large-v1/fold0/special_tokens_map.json\nloading file ../input/deberta-large-v1/fold0/tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3746f27bb3a948f9a135fd7f21f403e2"}},"metadata":{}},{"name":"stderr","text":"Didn't find file ../input/deberta-v3-5folds/fold0/tokenizer.json. We won't load it.\nloading file ../input/deberta-v3-5folds/fold0/spm.model\nloading file ../input/deberta-v3-5folds/fold0/added_tokens.json\nloading file ../input/deberta-v3-5folds/fold0/special_tokens_map.json\nloading file ../input/deberta-v3-5folds/fold0/tokenizer_config.json\nloading file None\nAdding [MASK] to the vocabulary\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd2213b332b412aa7f392712b26854d"}},"metadata":{}},{"name":"stderr","text":"loading configuration file ../input/deberta-v3-5folds/fold0/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/deberta-v3-5folds/fold0\",\n  \"architectures\": [\n    \"DebertaV2ForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ../input/deberta-v3-5folds/fold0/pytorch_model.bin\nAll model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n\nAll the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ../input/deberta-v3-5folds/fold0.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\nNo `TrainingArguments` passed, using `output_dir=tmp_trainer`.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the test set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text.\n***** Running Prediction *****\n  Num examples = 36\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Didn't find file ../input/xlm-roberta-large-5folds/fold0/added_tokens.json. We won't load it.\nloading file ../input/xlm-roberta-large-5folds/fold0/sentencepiece.bpe.model\nloading file ../input/xlm-roberta-large-5folds/fold0/tokenizer.json\nloading file None\nloading file ../input/xlm-roberta-large-5folds/fold0/special_tokens_map.json\nloading file ../input/xlm-roberta-large-5folds/fold0/tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3543526fc764b799f6a035e17752a7f"}},"metadata":{}},{"name":"stderr","text":"loading configuration file ../input/xlm-roberta-large-5folds/fold0/config.json\nModel config XLMRobertaConfig {\n  \"_name_or_path\": \"../input/xlm-roberta-large-5folds/fold0\",\n  \"architectures\": [\n    \"XLMRobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"regression\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}\n\nloading weights file ../input/xlm-roberta-large-5folds/fold0/pytorch_model.bin\nAll model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n\nAll the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at ../input/xlm-roberta-large-5folds/fold0.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\nNo `TrainingArguments` passed, using `output_dir=tmp_trainer`.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the test set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text.\n***** Running Prediction *****\n  Num examples = 36\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Didn't find file ../input/electra-upppm/electra_upppm/fold0/added_tokens.json. We won't load it.\nloading file ../input/electra-upppm/electra_upppm/fold0/vocab.txt\nloading file ../input/electra-upppm/electra_upppm/fold0/tokenizer.json\nloading file None\nloading file ../input/electra-upppm/electra_upppm/fold0/special_tokens_map.json\nloading file ../input/electra-upppm/electra_upppm/fold0/tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cd4eec759144c39fc410bde6430ecb"}},"metadata":{}},{"name":"stderr","text":"Didn't find file ../input/bert4patents-upppm/bert4patents_upppm/fold0/added_tokens.json. We won't load it.\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/vocab.txt\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/tokenizer.json\nloading file None\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/special_tokens_map.json\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2478f2b60a53490c930dd73c147ef669"}},"metadata":{}}]},{"cell_type":"code","source":"def process_valid(unit):\n        return {\n        **tokenizer(unit['text'])\n    }\n\nfor i in range(len(CFG.model_path))[4:]:   \n    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}fold0')\n    valid_ds = datasets.Dataset.from_pandas(train[['text']])\n    valid_ds = valid_ds.map(process_valid)\n\n    folds = sorted([m for m in glob.glob(f'{CFG.model_path[i]}') if 'fold' in m])\n\n    predictions_valid = []\n    for fold in range(len(folds)):        \n        trainer = Trainer(\n                AutoModelForSequenceClassification.from_pretrained(f'{CFG.model_path[i]}fold{fold}'),\n                tokenizer=tokenizer,\n            )\n        \n        predictions_valid.append(trainer.predict(valid_ds).predictions)\n        del trainer\n        gc.collect()\n        \n    train[f'predictions_{os.path.basename(CFG.model_path[i])}'] = np.average(predictions_valid, axis=0)\n    \n    del tokenizer, valid_ds\n    gc.collect()","metadata":{"_uuid":"f7e15874-fc82-4243-9a91-34cb24ab04bc","_cell_guid":"aa4a21af-9cd7-4051-be66-6f9837acf680","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T07:04:51.969401Z","iopub.execute_input":"2022-06-20T07:04:51.970319Z","iopub.status.idle":"2022-06-20T07:05:06.084408Z","shell.execute_reply.started":"2022-06-20T07:04:51.970281Z","shell.execute_reply":"2022-06-20T07:05:06.083407Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stderr","text":"Didn't find file ../input/bert4patents-upppm/bert4patents_upppm/fold0/added_tokens.json. We won't load it.\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/vocab.txt\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/tokenizer.json\nloading file None\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/special_tokens_map.json\nloading file ../input/bert4patents-upppm/bert4patents_upppm/fold0/tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565ab47e2ed9411aaac0e2c91880f914"}},"metadata":{}}]},{"cell_type":"code","source":"X = train.drop(columns=['id', 'anchor', 'target', 'context', 'score', 'general_context', 'context_text',\n       'section', 'classes', 'dataset', 'anchor_parsed', 'target_parsed', 'anchor_nlp', 'target_nlp']).astype('float64')\ny = train['score']","metadata":{"_uuid":"c6e4839b-efc5-4086-a590-2f7b7b65e8de","_cell_guid":"46bdead9-8e05-4907-bc48-78c2be20cb9a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-20T07:05:26.416386Z","iopub.execute_input":"2022-06-20T07:05:26.417245Z","iopub.status.idle":"2022-06-20T07:05:26.467648Z","shell.execute_reply.started":"2022-06-20T07:05:26.417193Z","shell.execute_reply":"2022-06-20T07:05:26.466384Z"},"trusted":true},"execution_count":75,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/1024215573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m X = train.drop(columns=['id', 'anchor', 'target', 'context', 'score', 'general_context', 'context_text',\n\u001b[0;32m----> 2\u001b[0;31m        'section', 'classes', 'dataset', 'anchor_parsed', 'target_parsed', 'anchor_nlp', 'target_nlp']).astype('float64')\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4911\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4912\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4913\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4914\u001b[0m         )\n\u001b[1;32m   4915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['anchor_nlp' 'target_nlp'] not found in axis\""],"ename":"KeyError","evalue":"\"['anchor_nlp' 'target_nlp'] not found in axis\"","output_type":"error"}]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def slice_by_corr(X, r_min=0):\n    # Create correlation matrix\n    corr_matrix = X.corr().abs()\n\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find features with correlation greater than r_min\n    return X[[column for column in upper.columns if any(upper[column] > r_min)]]\n\ndef variance_inflation_factor(X, exog_idx):\n    clf = LinearRegression(fit_intercept=True)\n    sub_X = np.delete(np.nan_to_num(X), exog_idx, axis=1)\n    sub_y = X[:, exog_idx][np.newaxis].T\n    sub_clf = clf.fit(sub_X, sub_y)\n    return 1 / (1 - r2_score(sub_y, sub_clf.predict(sub_X)))\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n\n    def __init__(self, thresh=10.0, nthreads=4, r_min=0, obs=250):\n        self.thresh = thresh\n        self.nthreads = nthreads\n        self.r_min = r_min\n        self.obs = obs\n        \n    def fit(self, X):\n        self.X = X\n        return self\n\n    def transform(self, X):\n        return ReduceVIF.calculate_vif(X, self.thresh, \n                                       self.nthreads, \n                                       self.r_min, \n                                       self.obs)\n\n    @staticmethod\n    def calculate_vif(X, thresh=10.0, nthreads=16, r_min=0, obs=250):        \n        dropped = True\n        vif_cols = []\n        X_vif_candidates = slice_by_corr(X, r_min)\n        X_vif_candidates = X_vif_candidates.sample(n=obs)\n        while dropped:\n            variables = X_vif_candidates.columns\n            dropped = False\n            with Parallel(n_jobs=nthreads, backend='threading') as parallel:\n                vif = parallel(\n                    delayed(variance_inflation_factor)(\n                        np.asarray(X_vif_candidates[variables].values),\n                        X_vif_candidates.columns.get_loc(var)) for var in \n                    X_vif_candidates.columns)\n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X_vif_candidates.columns[maxloc]} with vif={max_vif}')\n                vif_cols.append(X_vif_candidates.columns.tolist()[maxloc])\n                X_vif_candidates = X_vif_candidates.drop(\n                    [X_vif_candidates.columns.tolist()[maxloc]], axis=1)\n                dropped = True\n        \n        if len(vif_cols) > 0:\n            return X.drop(columns=vif_cols), vif_cols\n        else:\n            return X, vif_cols\n\n    \ndef preprocess_x_y(X, nodrop_columns=[],\n                   var_thr=0.95, remove_multi=True,\n                   standardize=True, standardizer='mm',\n                   std_dev=3, vif_thr=5, missingness_thr=0.50,\n                   zero_thr=0.99, nthreads=4):\n    from colorama import Fore, Style\n\n    # Replace all near-zero with zeros\n    # Drop excessively sparse columns with >zero_thr zeros\n    if zero_thr > 0:\n        X = X.apply(lambda x: np.where(np.abs(x) < 0.000001, 0, x))\n        X_tmp = X.T.loc[(X == 0).sum() < (float(zero_thr)) * X.shape[0]].T\n\n        if len(nodrop_columns) > 0:\n            X = pd.concat([X_tmp, X[[i for i in X.columns if i in\n                                     nodrop_columns and i not in\n                                     X_tmp.columns]]], axis=1)\n        else:\n            X = X_tmp\n        del X_tmp\n\n        if X.empty or len(X.columns) < 5:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (Zero Columns): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    # Remove columns with excessive missing values\n    X = X.dropna(thresh=len(X) * (1 - missingness_thr), axis=1)\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (missingness): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n\n    # Apply a simple imputer (note that this assumes extreme cases of\n    # missingness have already been addressed). The SimpleImputer is better\n    # for smaller datasets, whereas the IterativeImputer performs best on\n    # larger sets.\n\n    # from sklearn.experimental import enable_iterative_imputer\n    # from sklearn.impute import IterativeImputer\n    # imp = IterativeImputer(random_state=0, sample_posterior=True)\n    # X = pd.DataFrame(imp.fit_transform(X, y), columns=X.columns)\n    imp1 = SimpleImputer()\n    X = pd.DataFrame(imp1.fit_transform(X.astype('float32')),\n                     columns=X.columns)\n\n    # Standardize X\n    if standardize is True:\n        if standardizer == 'ss':\n            scaler = StandardScaler()\n        else:\n            scaler = MinMaxScaler()\n        X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n    # Remove low-variance columns\n    sel = VarianceThreshold(threshold=(var_thr*(1-var_thr)))\n    sel.fit(X)\n    if len(nodrop_columns) > 0:\n        good_var_cols = X.columns[np.concatenate(\n            [sel.get_support(indices=True), np.array([X.columns.get_loc(c)\n                                                      for c in\n                                                      nodrop_columns if\n                                                      c in X])])]\n    else:\n        good_var_cols = X.columns[sel.get_support(indices=True)]\n    low_var_cols = [i for i in X.columns if i not in list(good_var_cols)]\n    if len(low_var_cols) > 0:\n        print(f\"Dropping {low_var_cols} for low variance...\")\n    X = X[good_var_cols]\n\n    if X.empty:\n        print(f\"\\n\\n{Fore.RED}Empty feature-space (low-variance): \"\n              f\"{X}{Style.RESET_ALL}\\n\\n\")\n        return X\n        \n    # Remove multicollinear columns\n    if remove_multi is True:\n        try:\n            rvif = ReduceVIF(thresh=vif_thr, nthreads=nthreads)\n            X = rvif.fit_transform(X)[0]\n            if X.empty or len(X.columns) < 5:\n                print(f\"\\n\\n{Fore.RED}Empty feature-space \"\n                      f\"(multicollinearity): \"\n                      f\"{X}{Style.RESET_ALL}\\n\\n\")\n                return X\n        except:\n            print(f\"\\n\\n{Fore.RED}Empty feature-space (multicollinearity): \"\n                  f\"{X}{Style.RESET_ALL}\\n\\n\")\n            return X\n\n    print(f\"\\nX: {X}\\n\")\n    print(f\"Features: {list(X.columns)}\\n\")\n    return X\n\n\nclass Razors(object):\n    \"\"\"\n    Razors is a callable refit option for `GridSearchCV` whose aim is to\n    balance model complexity and cross-validated score in the spirit of the\n    \"one standard error\" rule of Breiman et al. (1984), which showed that\n    the tuning hyperparameter associated with the best performing model may be\n    prone to overfit. To help mitigate this risk, we can instead instruct\n    gridsearch to refit the highest performing 'parsimonious' model, as defined\n    using simple statistical rules (e.g. standard error (`sigma`),\n    percentile (`eta`), or significance level (`alpha`)) to compare\n    distributions of model performance across folds. Importantly, this\n    strategy assumes that the grid of multiple cross-validated models\n    can be principly ordered from simplest to most complex with respect to some\n    target hyperparameter of interest. To use the razors suite, supply\n    the `simplify` function partial of the `Razors` class as a callable\n    directly to the `refit` argument of `GridSearchCV`.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`.\n    scoring : str\n        Refit scoring metric.\n    param : str\n        Parameter whose complexity will be optimized.\n    rule : str\n        Rule for balancing model complexity with performance.\n        Options are 'se', 'percentile', and 'ranksum'. Default is 'se'.\n    sigma : int\n        Number of standard errors tolerance in the case that a standard error\n        threshold is used to filter outlying scores across folds. Required if\n        `rule`=='se'. Default is 1.\n    eta : float\n        Percentile tolerance in the case that a percentile threshold\n        is used to filter outlier scores across folds. Required if\n        `rule`=='percentile'. Default is 0.68.\n    alpha : float\n        An alpha significance level in the case that wilcoxon rank sum\n        hypothesis testing is used to filter outlying scores across folds.\n        Required if `rule`=='ranksum'. Default is 0.05.\n\n    References\n    ----------\n    Breiman, Friedman, Olshen, and Stone. (1984) Classification and Regression\n    Trees. Wadsworth.\n\n    Notes\n    -----\n    Here, 'simplest' is defined by the complexity of the model as influenced by\n    some user-defined target parameter (e.g. number of components, number of\n    estimators, polynomial degree, cost, scale, number hidden units, weight\n    decay, number of nearest neighbors, L1/L2 penalty, etc.).\n\n    The callable API accordingly assumes that the `params` attribute of\n    `cv_results_` 1) contains the indicated hyperparameter (`param`) of\n    interest, and 2) contains a sequence of values (numeric, boolean, or\n    categorical) that are ordered from least to most complex.\n    \"\"\"\n    __slots__ = ('cv_results', 'param', 'param_complexity', 'scoring',\n                 'rule', 'greater_is_better',\n                 '_scoring_funcs', '_scoring_dict',\n                 '_n_folds', '_splits', '_score_grid',\n                 '_cv_means', '_sigma', '_eta', '_alpha')\n\n    def __init__(\n            self,\n            cv_results_,\n            param,\n            scoring,\n            rule,\n            sigma=1,\n            eta=0.95,\n            alpha=0.01,\n    ):\n        import sklearn.metrics\n\n        self.cv_results = cv_results_\n        self.param = param\n        self.scoring = scoring\n        self.rule = rule\n        self._scoring_funcs = [\n            met\n            for met in sklearn.metrics.__all__\n            if (met.endswith(\"_score\")) or (met.endswith(\"_error\"))\n        ]\n        # Set _score metrics to True and _error metrics to False\n        self._scoring_dict = dict(\n            zip(\n                self._scoring_funcs,\n                [met.endswith(\"_score\") for met in self._scoring_funcs],\n            )\n        )\n        self.greater_is_better = self._check_scorer()\n        self._n_folds = len(list(set([i.split('_')[0] for i in\n                                     list(self.cv_results.keys()) if\n                                     i.startswith('split')])))\n        # Extract subgrid corresponding to the scoring metric of interest\n        self._splits = [i for i in list(self.cv_results.keys()) if\n                        i.endswith(f\"test_{self.scoring}\") and\n                        i.startswith('split')]\n        self._score_grid = np.vstack([self.cv_results[cv] for cv in\n                                      self._splits]).T\n        self._cv_means = np.array(np.nanmean(self._score_grid, axis=1))\n        self._sigma = sigma\n        self._eta = eta\n        self._alpha = alpha\n\n    def _check_scorer(self):\n        \"\"\"\n        Check whether the target refit scorer is negated. If so, adjust\n        greater_is_better accordingly.\n        \"\"\"\n\n        if (\n                self.scoring not in self._scoring_dict.keys()\n                and f\"{self.scoring}_score\" not in self._scoring_dict.keys()\n        ):\n            if self.scoring.startswith(\"neg_\"):\n                self.greater_is_better = True\n            else:\n                raise NotImplementedError(f\"Scoring metric {self.scoring} not \"\n                                          f\"recognized.\")\n        else:\n            self.greater_is_better = [\n                value for key, value in self._scoring_dict.items() if\n                self.scoring in key][0]\n        return self.greater_is_better\n\n    def _best_low_complexity(self):\n        \"\"\"\n        Balance model complexity with cross-validated score.\n\n        Return\n        ------\n        int\n            Index of a model that has the lowest complexity but its test score\n            is the highest on average across folds as compared to other models\n            that are equally likely to occur.\n        \"\"\"\n\n        # Check parameter(s) whose complexity we seek to restrict\n        if not any(self.param in x for x in\n                   self.cv_results[\"params\"][0].keys()):\n            raise KeyError(f\"Parameter {self.param} not found in cv grid.\")\n        else:\n            hyperparam = [\n                i for i in self.cv_results[\"params\"][0].keys() if\n                i.endswith(self.param)][0]\n\n        # Select low complexity threshold based on specified evaluation rule\n        if self.rule == \"se\":\n            if not self._sigma:\n                raise ValueError(\n                    \"For `se` rule, the tolerance \"\n                    \"(i.e. `_sigma`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_standard_error()\n        elif self.rule == \"percentile\":\n            if not self._eta:\n                raise ValueError(\n                    \"For `percentile` rule, the tolerance \"\n                    \"(i.e. `_eta`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_percentile()\n        elif self.rule == \"ranksum\":\n            if not self._alpha:\n                raise ValueError(\n                    \"For `ranksum` rule, the alpha-level \"\n                    \"(i.e. `_alpha`) parameter cannot be null.\"\n                )\n            l_cutoff, h_cutoff = self.call_rank_sum_test()\n        else:\n            raise NotImplementedError(f\"{self.rule} is not a valid \"\n                                      f\"rule of RazorCV.\")\n\n        self.cv_results[f\"param_{hyperparam}\"].mask = np.where(\n            (self._cv_means >= float(l_cutoff)) &\n            (self._cv_means <= float(h_cutoff)),\n            True, False)\n\n        if np.sum(self.cv_results[f\"param_{hyperparam}\"].mask) == 0:\n            print(f\"\\nLow: {l_cutoff}\")\n            print(f\"High: {h_cutoff}\")\n            print(f\"{self._cv_means}\")\n            print(f\"hyperparam: {hyperparam}\\n\")\n            raise ValueError(\"No valid grid columns remain within the \"\n                             \"boundaries of the specified razor\")\n\n        highest_surviving_rank = np.nanmin(\n            self.cv_results[f\"rank_test_{self.scoring}\"][\n                self.cv_results[f\"param_{hyperparam}\"].mask])\n\n        # print(f\"Highest surviving rank: {highest_surviving_rank}\\n\")\n\n        return np.flatnonzero(np.isin(\n            self.cv_results[f\"rank_test_{self.scoring}\"],\n            highest_surviving_rank))[0]\n\n    def call_standard_error(self):\n        \"\"\"\n        Returns the simplest model whose performance is within `sigma`\n        standard errors of the average highest performing model.\n        \"\"\"\n\n        # Estimate the standard error across folds for each column of the grid\n        cv_se = np.array(np.nanstd(self._score_grid, axis=1) /\n                         np.sqrt(self._n_folds))\n\n        # Determine confidence interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = self._cv_means[best_score_idx] - cv_se[best_score_idx]\n            l_cutoff = self._cv_means[best_score_idx] + cv_se[best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    def call_rank_sum_test(self):\n        \"\"\"\n        Returns the simplest model whose paired performance across folds is\n        insignificantly different from the average highest performing,\n        at a predefined `alpha` level of significance.\n        \"\"\"\n\n        from scipy.stats import wilcoxon\n        import itertools\n\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n\n        # Perform signed Wilcoxon rank sum test for each pair combination of\n        # columns against the best average score column\n        tests = [pair for pair in list(itertools.combinations(range(\n            self._score_grid.shape[0]), 2)) if best_score_idx in pair]\n\n        p_dict = {}\n        for i, test in enumerate(tests):\n            p_dict[i] = wilcoxon(self._score_grid[test[0], :],\n                                 self._score_grid[test[1], :])[1]\n\n        # Sort and prune away significant tests\n        p_dict = {k: v for k, v in sorted(p_dict.items(),\n                                          key=lambda item: item[1]) if\n                  v > self._alpha}\n\n        # Flatten list of tuples, remove best score index, and take the\n        # lowest and highest remaining bounds\n        tests = [j for j in list(set(list(sum([tests[i] for i in\n                                               list(p_dict.keys())],\n                                              ())))) if j != best_score_idx]\n        if self.greater_is_better:\n            h_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n        else:\n            h_cutoff = self._cv_means[\n                np.nanargmax(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n            l_cutoff = self._cv_means[\n                np.nanargmin(self.cv_results[\n                                 f\"rank_test_{self.scoring}\"][tests])]\n\n        return l_cutoff, h_cutoff\n\n\n    def call_percentile(self):\n        \"\"\"\n        Returns the simplest model whose performance is within the `eta`\n        percentile of the average highest performing model.\n        \"\"\"\n\n        # Estimate the indicated percentile, and its inverse, across folds for\n        # each column of the grid\n        perc_cutoff = np.nanpercentile(self._score_grid,\n                                       [100 * self._eta,\n                                        100 - 100 * self._eta], axis=1)\n\n        # Determine bounds of the percentile interval\n        if self.greater_is_better:\n            best_score_idx = np.nanargmax(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n        else:\n            best_score_idx = np.nanargmin(self._cv_means)\n            h_cutoff = perc_cutoff[0, best_score_idx]\n            l_cutoff = perc_cutoff[1, best_score_idx]\n\n        return l_cutoff, h_cutoff\n\n    @staticmethod\n    def simplify(param, scoring, rule='se', sigma=1, eta=0.68, alpha=0.01):\n        \"\"\"\n        Callable to be run as `refit` argument of `GridsearchCV`.\n\n        Parameters\n        ----------\n        param : str\n            Parameter with the largest influence on model complexity.\n        scoring : str\n            Refit scoring metric.\n        sigma : int\n            Number of standard errors tolerance in the case that a standard\n            error threshold is used to filter outlying scores across folds.\n            Only applicable if `rule`=='se'. Default is 1.\n        eta : float\n            Acceptable percent tolerance in the case that a percentile\n            threshold is used. Only applicable if `rule`=='percentile'.\n            Default is 0.68.\n        alpha : float\n            Alpha-level to use for signed wilcoxon rank sum testing.\n            Only applicable if `rule`=='ranksum'. Default is 0.01.\n        \"\"\"\n        from functools import partial\n\n        def razor_pass(\n                cv_results_, param, scoring, rule, sigma, alpha, eta\n        ):\n            rcv = Razors(cv_results_, param, scoring, rule=rule,\n                         sigma=sigma, alpha=alpha, eta=eta)\n            return rcv._best_low_complexity()\n\n        return partial(\n            razor_pass,\n            param=param,\n            scoring=scoring,\n            rule=rule,\n            sigma=sigma,\n            alpha=alpha,\n            eta=eta,\n        )\n\ndef divide_df(df_all,train_len):\n    return df_all.loc[:train_len-1], df_all.loc[train_len:].drop('target',axis=1)\n\ndef concat_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)","metadata":{"_uuid":"07381d69-4197-43f3-9ab8-44ec5d3e32cc","_cell_guid":"cb66bafc-95ec-44f7-871c-b4d825158399","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess = FunctionTransformer(preprocess_x_y)\nX_clean = preprocess.fit_transform(X=X)\nsurviving_features = list(X_clean.columns)","metadata":{"_uuid":"babc5fa2-709b-49bd-9ad2-f9753ac17822","_cell_guid":"2712eda4-f76c-499e-acae-26b8f5b934c8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed=42\nX_train, X_test, y_train, y_test = train_test_split(X_clean, y, random_state=seed)\n\nX_train = X_train.reset_index(drop=True)\ny_train = pd.DataFrame(y_train).reset_index(drop=True)\n\nX_train = X_train.head(10000)\ny_train = y_train.head(10000)\n\nX_test = X_test.reset_index(drop=True)\ny_test = pd.DataFrame(y_test).reset_index(drop=True)\n\nX_test = X_test.head(2000)\ny_test = y_test.head(2000)","metadata":{"_uuid":"2013efce-e219-44ea-9b4f-3fcaf00e7a1a","_cell_guid":"caa0b740-b39a-4181-b506-39e5829c1997","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [\n            'rf'\n         ]\n\nestimators = [\n        RandomForestRegressor(random_state=42, min_samples_leaf=7, min_samples_split=3)\n]","metadata":{"_uuid":"c795674c-3eba-46df-9ba0-107ffcc738fc","_cell_guid":"1cae2f7f-40b3-42cc-9d3c-2b5b2dca76bd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params={models[0]: {'max_depth': [3, 4, 5],\n                    'n_estimators': [50, 60, 70],}\n       }","metadata":{"_uuid":"4c5a010d-7329-46a0-b904-4af8032697d6","_cell_guid":"3cb8358e-126b-4b91-ae6f-ba4d70386c2a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_factory = {}\n\ninner_scoring = \"neg_mean_absolute_error\"\n\nfor name, estimator in zip(models, estimators):\n    print(name)\n    model_factory[name] = {}\n    \n    pipe = Pipeline([\n        (name, TransformedTargetRegressor(regressor=estimator, transformer=MinMaxScaler()))\n    ])\n    model_params = {}\n    for hyperparam in params[name].keys():\n        model_params[f\"{name}__regressor__{hyperparam}\"] = params[name][hyperparam]\n    pipe_grid_cv = GridSearchCV(pipe, model_params, scoring=[inner_scoring], \n                       refit=Razors.simplify(param=f'{name}__regressor__n_estimators', \n                                             scoring=inner_scoring, rule=\"se\", sigma=1), \n                       cv=KFold(n_splits=5, shuffle=True, random_state=seed), n_jobs=-1)\n    pipe_grid_cv.fit(X_train, y_train.values.ravel())\n    model_factory[name]['oos_score'] = cross_val_score(pipe_grid_cv, X_test, y_test.values.ravel(), \n                                                       scoring='r2', \n                                                       cv=KFold(n_splits=5, shuffle=True, \n                                                                random_state=seed + 1))\n    model_factory[name]['best_params'] = pipe_grid_cv.best_params_\n    model_factory[name]['best_estimator'] = pipe_grid_cv.best_estimator_\n\nleaderboard = {}\nfor mod in model_factory.keys():\n    leaderboard[mod] = np.mean(model_factory[mod]['oos_score'])\n\nbest_estimator_name = max(leaderboard, key=leaderboard.get)\n\nbest_estimator = model_factory[best_estimator_name]['best_estimator']\n\nmodel_factory","metadata":{"_uuid":"8036d58b-7c10-49bf-a571-6a0567094cff","_cell_guid":"47c28da5-8a5f-4096-a8f2-38710f345686","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_factory[name]['best_params']","metadata":{"_uuid":"97d22c7f-b8a9-4e0f-8184-635733fd552c","_cell_guid":"424275f0-4842-4f8c-9144-623804168b6a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outer_best = KFold(n_splits=5, shuffle=True, random_state=seed)\n\nbest_estimator.fit(X_clean, y)\n\nscores = cross_val_score(best_estimator, X_clean, y, scoring='r2', cv=outer_best, n_jobs=-1, error_score='raise')\nscores","metadata":{"_uuid":"b1d45736-646e-4036-98ef-126d90fde156","_cell_guid":"759be8eb-545f-42c1-aa8b-7b465db135e7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator.named_steps['rf'].regressor_.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = (\n    f\"/kaggle/working/rf_model.joblib\"\n)\ndump(best_estimator, model_path)","metadata":{"_uuid":"aad5fc5b-78b8-4bd7-b3ca-b4097950aad0","_cell_guid":"b4965bcb-bb94-422b-99b6-4a6f29e5ad94","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')","metadata":{"_uuid":"6d5556c8-1701-499c-a956-3d8b3152f27e","_cell_guid":"26c18b6f-bbd2-44d7-be32-04ab3ddcf43e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ny_pred = best_estimator.predict(pd.DataFrame(scaler.fit_transform(test[surviving_features]), columns=surviving_features))","metadata":{"_uuid":"872795a1-5df7-47e3-b646-8457869d79db","_cell_guid":"5798fccc-7530-4166-a31d-82295021445b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['id'] = test['id']\nsubmission['score'] = y_pred","metadata":{"_uuid":"e87dd27c-b526-4099-80a7-680fd95e7d09","_cell_guid":"9822b0f9-7585-4c97-992d-b0b18cbcc8b0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"804eee29-db36-4b3a-9229-b78f2f05bf59","_cell_guid":"35399c7a-1c57-40db-aeac-4f6b8e9b0ece","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}